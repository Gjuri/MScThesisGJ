{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35e6ac7b-b185-421a-ad37-11339f5a1dcf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f23e5e7-cb86-4849-9dbf-97fd189d48e9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import absl.logging\n",
    "\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)# Avoid print error\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np \n",
    "from numpy.random import seed\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.options.mode.chained_assignment = None # avoid error \n",
    "pd.options.display.float_format = '{:.5f}'.format # No scientific annotation when print dataframe\n",
    "from functools import reduce\n",
    "  \n",
    "# ADA and Modeling imports\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler,MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split,KFold,GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error, mean_squared_log_error, accuracy_score\n",
    "from IPython.display import clear_output\n",
    "import statistics\n",
    "from scipy.stats import stats\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#from tsmoothie.smoother import *\n",
    "\n",
    "# To plot pretty figures\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Build function\n",
    "#from HelperFunctions import *\n",
    "#from Implementation import *\n",
    "\n",
    "# To generate an stable output across runs\n",
    "rnd_seed = 42\n",
    "rnd_gen = np.random.default_rng(rnd_seed)\n",
    "\n",
    "# MAchine learing packages \n",
    "import tensorflow \n",
    "from keras.layers import SimpleRNN\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import metrics \n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD, Adamax, Nadam\n",
    "from tensorflow.keras.activations import selu,elu,relu,tanh\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args\n",
    "import tensorflow_probability as tfp\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from keras import backend as K\n",
    "from keras import Input #\n",
    "from keras.models import Sequential, load_model\n",
    "import keras.backend as K \n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Dropout, LSTM, GRU, InputLayer, Flatten, Conv1D, Activation, LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "\n",
    "n_cpu = os.cpu_count()\n",
    "print(\"Number of CPUs in the system:\", n_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0c58c-399a-4ca0-b79b-d81b0d13324e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d539b0-caa4-4ea0-be42-9c0ca5d05e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This code required the right version of packages\n",
    "     like numpy and tensor '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe02f1dc-ea80-421b-a045-5e2ea6a54d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data normlization and reshaping'''\n",
    "df = pd.read_csv('d_f_RNN.csv')\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(df.iloc[:,:-1])\n",
    "X_df =pd.DataFrame(X_scaled)\n",
    "\n",
    "y_scaled = scaler.fit_transform((df.iloc[:,-1].to_numpy()).reshape(-1,1))#.reshape(-1,1)\n",
    "y_df =pd.DataFrame(y_scaled)\n",
    "\n",
    "# Selecting features and label for training \n",
    "\n",
    "features = X_df.astype('float32') # Training data\n",
    "features[2] = features[2].astype(int) # Strat as integer\n",
    "label = y_df.astype('float32')#.to_numpy().reshape(-1,1) # 'Labels'\n",
    "# Reshaping data into n_samples x timestep to be feed into the GRU netwok\n",
    "def Reshape_df(X, y, time_steps):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "X, y = Reshape_df(features, label, 100)\n",
    "\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "\n",
    "'''Data Splitting'''\n",
    "\n",
    "s = 0.8309\n",
    "split = int((s)*X.shape[0])\n",
    "X_train = X[:split,:,:]\n",
    "y_train = y[:split,:]\n",
    "X_test =  X[split:,:,:]\n",
    "y_test =  y[split:,:]\n",
    "\n",
    "sets = [X, X_test, y, y_test]\n",
    "sets_n = ['X_train', 'X_test', 'y_train', 'y_test']\n",
    "\n",
    "for i,im in enumerate(sets):\n",
    "    print(f'{sets_n[i]}:',im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a99da1-f68b-4ddf-b4d9-d097b4a26694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.callbacks import DeltaYStopper\n",
    "search_space = [Categorical([12,34,60,120], name='batch_size'),\n",
    "                Categorical([10,30], name='epochs'),\n",
    "                Real(1e-8, 1e-3,'log-uniform',name='learning_rate'),\n",
    "                Categorical(['relu', 'tanh','selu','elu'], name='activation'),\n",
    "                Real(0.0, 0.5, name='dropout_rate'),\n",
    "                Categorical(['Adam', 'SGD','rmsprop'], name='optimizer'),\n",
    "                Integer(10, 200, name='num_units'),\n",
    "                Integer(1, 3, name='num_layers'),\n",
    "                Real(0.1,0.9,name='lambda1'), \n",
    "                Real(0.1,0.9,name='lambda2')]\n",
    "\n",
    "# Define the objective function to be minimized\n",
    "@use_named_args(search_space)\n",
    "def Hybrid_model(**params):\n",
    "\n",
    "\n",
    "    batch_size = params['batch_size']\n",
    "    epochs = params['epochs']\n",
    "    learning_rate = params['learning_rate']\n",
    "    activation = params['activation']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    optimizer = params['optimizer']\n",
    "    num_units = params['num_units']\n",
    "    num_layers = params['num_layers']\n",
    "    l1 = params['lambda1']\n",
    "    l2 = params['lambda2']\n",
    "\n",
    "    if optimizer == 'Adam':\n",
    "        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'SGD':\n",
    "        opt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    # Build the model\n",
    "    Hybrid = Sequential()\n",
    "    Hybrid.add(Input(shape=(X_train_kfold.shape[1],X_train_kfold.shape[2]), name='input_layer'))\n",
    "                     \n",
    "    for i in range(num_layers):\n",
    "        if i == num_layers - 1:\n",
    "            Hybrid.add(GRU(units=num_units,\n",
    "                           activation=activation,\n",
    "                           dropout=dropout_rate,\n",
    "                           return_sequences=False,\n",
    "                           name=f'Hidden_layer_{i+1}'))\n",
    "        else:\n",
    "            # Not the last GRU layer\n",
    "            Hybrid.add(GRU(units=num_units,\n",
    "                           activation=activation,\n",
    "                           dropout=dropout_rate,\n",
    "                           return_sequences=True,\n",
    "                           name=f'Hidden_layer_{i+1}'))\n",
    "    Hybrid.add(Dense(1))\n",
    "    Hybrid.compile(loss=rmse, optimizer=opt, metrics=[r_squared])\n",
    "\n",
    "    # Train the model on the training data for this fold\n",
    "    \n",
    "    # Return the validation loss as the objective value to be minimized\n",
    "\n",
    "    # Evaluate the model on the validation data for this fold\n",
    "    Hybrid.fit(X_train_kfold,y_train_kfold,\n",
    "                   batch_size=batch_size,\n",
    "                   epochs=epochs,\n",
    "                   verbose=1)\n",
    "    val_loss, val_r_squared = Hybrid.evaluate(X_val_kfold, y_val_kfold, verbose=1)\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad513f53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define per-fold score containers\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "from skopt.callbacks import DeltaYStopper\n",
    "r2_per_fold = []\n",
    "loss_per_fold = []\n",
    "rmse_per_fold = []\n",
    "gru_best_score = []\n",
    "\n",
    "#l1 = 0.5\n",
    "#l2 = 0.5\n",
    "kfold = KFold(n_splits=6)\n",
    "fold_no =1\n",
    "parameters_fold = [None]*6\n",
    "#for train_index, val_index in kfold.split(X_train, y_train):\n",
    "for i, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):       \n",
    "    print(i,fold_no)\n",
    "    X_train_kfold, X_val_kfold = X_train[train_index], X_train[val_index]\n",
    "    y_train_kfold, y_val_kfold = y_train[train_index], y_train[val_index]\n",
    "        # Best model for th k-fold\n",
    "    loss = True\n",
    "    delta_stopper = DeltaYStopper(delta=0.1)\n",
    "    result = gp_minimize(func=Hybrid_model,\n",
    "                     dimensions=search_space,\n",
    "                     n_calls=10,\n",
    "                    random_state=42,\n",
    "                    callback=[delta_stopper],\n",
    "                    verbose=True)\n",
    "    clear_output(wait = True)\n",
    "    print('Saving',fold_no)\n",
    "    params0 = {param.name: value for param, value in zip(search_space, result.x)}\n",
    "    parameters_fold[i] = {'fold':fold_no,'parameters':params0}\n",
    "    clear_output(wait = True)\n",
    "    fold_no +=1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1d031c-e2d6-4727-836a-826a8ad1e6d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GRU_foldcv(params0):\n",
    "    batch_size = params0['batch_size']\n",
    "    epochs = params0['epochs']\n",
    "    learning_rate = params0['learning_rate']\n",
    "    activation = params0['activation']\n",
    "    dropout_rate = params0['dropout_rate']\n",
    "    optimizer = params0['optimizer']\n",
    "    num_units = params0['num_units']\n",
    "    num_layers = params0['num_layers']\n",
    "    l1 = params0['lambda1']\n",
    "    l2 = params0['lambda2']\n",
    "\n",
    "    if optimizer == 'Adam':\n",
    "        #opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        opt = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'SGD':\n",
    "         opt =optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate)\n",
    "         #opt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = tf.keras.optimizers.legacy.RMSprop(learning_rate=learning_rate)\n",
    "        #opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "  \n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):       \n",
    "            X_train_kfold, X_val_kfold = X_train[train_index], X_train[val_index]\n",
    "            y_train_kfold, y_val_kfold = y_train[train_index], y_train[val_index]\n",
    "            mchpt = tf.keras.callbacks.ModelCheckpoint(filepath=f'GRU/gru_new0_fold{i+1}_model{p}.ckpt',\n",
    "                                                       monitor = 'val_loss' ,\n",
    "                                                       save_best_only=True,\n",
    "                                                       mode ='min', \n",
    "                                                       verbose =0)#,save_weights_only=True) restore_best_weights = True\n",
    "            early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', \n",
    "                                                              patience=5, mode ='min'\n",
    "                                                              ,verbose =0) # try patience of 3 or moin\n",
    "\n",
    "            print(f'fitting{i+1}')\n",
    "                        \n",
    "            # Build the model\n",
    "            Hybrid = Sequential()\n",
    "            Hybrid.add(Input(shape=(X_train_kfold.shape[1],X_train_kfold.shape[2]), name='input_layer'))\n",
    "\n",
    "            for i in range(num_layers):\n",
    "                if i == num_layers - 1:\n",
    "                    Hybrid.add(GRU(units=num_units,\n",
    "                                   activation=activation,\n",
    "                                   dropout=dropout_rate,\n",
    "                                   return_sequences=False,\n",
    "                                   name=f'Hidden_layer_{i+1}'))\n",
    "                else:\n",
    "                    # Not the last GRU layer\n",
    "                    Hybrid.add(GRU(units=num_units,\n",
    "                                   activation=activation,\n",
    "                                   dropout=dropout_rate,\n",
    "                                   return_sequences=True,\n",
    "                                   name=f'Hidden_layer_{i+1}'))\n",
    "            Hybrid.add(Dense(1))\n",
    "            Hybrid.compile(loss=rmse, optimizer=opt, metrics=[r_squared])\n",
    "            #Train the model on the entire training dataset\n",
    "            Hybrid.fit(X_train_kfold, y_train_kfold,\n",
    "                                   epochs = params0['batch_size'],\n",
    "                                   batch_size = params0['batch_size'],\n",
    "                                   shuffle = False,\n",
    "                                   callbacks = [early_stopping,mchpt],\n",
    "                                   validation_data=(X_val_kfold, y_val_kfold))\n",
    "    return Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5babde-6230-4fed-9d99-09c603224bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = K.sum(K.square(y_true - y_pred))\n",
    "    ss_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return 1 - ss_res / (ss_tot + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67694646-b6bf-46be-9160-ec5887e0b694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GRU_foldcv(params0):\n",
    "    batch_size = params0['batch_size']\n",
    "    epochs = params0['epochs']\n",
    "    learning_rate = params0['learning_rate']\n",
    "    activation = params0['activation']\n",
    "    dropout_rate = params0['dropout_rate']\n",
    "    optimizer = params0['optimizer']\n",
    "    num_units = params0['num_units']\n",
    "    num_layers = params0['num_layers']\n",
    "    l1 = params0['lambda1']\n",
    "    l2 = params0['lambda2']\n",
    "\n",
    "    if optimizer == 'Adam':\n",
    "        #opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        opt = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'SGD':\n",
    "         opt =optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate)\n",
    "         #opt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = tf.keras.optimizers.legacy.RMSprop(learning_rate=learning_rate)\n",
    "        #opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "  \n",
    "    \n",
    "    # for i, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):       \n",
    "    #         X_train_kfold, X_val_kfold = X_train[train_index], X_train[val_index]\n",
    "    #         y_train_kfold, y_val_kfold = y_train[train_index], y_train[val_index]\n",
    "            # mchpt = tf.keras.callbacks.ModelCheckpoint(filepath=f'GRU/gru_new0_fold{i+1}_model{p}.ckpt',\n",
    "            #                                            monitor = 'val_loss' ,\n",
    "            #                                            save_best_only=True,\n",
    "            #                                            mode ='min', \n",
    "            #                                            verbose =0)#,save_weights_only=True) restore_best_weights = True\n",
    "            # early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', \n",
    "            #                                                   patience=5, mode ='min'\n",
    "            #                                                   ,verbose =0) # try patience of 3 or moin\n",
    "                        \n",
    "            # Build the model\n",
    "    Hybrid = Sequential()\n",
    "    Hybrid.add(Input(shape=(X_train_kfold.shape[1],X_train_kfold.shape[2]), name='input_layer'))\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        if i == num_layers - 1:\n",
    "            Hybrid.add(GRU(units=num_units,\n",
    "                           activation=activation,\n",
    "                           dropout=dropout_rate,\n",
    "                           return_sequences=False,\n",
    "                           name=f'Hidden_layer_{i+1}'))\n",
    "        else:\n",
    "            # Not the last GRU layer\n",
    "            Hybrid.add(GRU(units=num_units,\n",
    "                           activation=activation,\n",
    "                           dropout=dropout_rate,\n",
    "                           return_sequences=True,\n",
    "                           name=f'Hidden_layer_{i+1}'))\n",
    "    Hybrid.add(Dense(1))\n",
    "    Hybrid.compile(loss=rmse, optimizer=opt, metrics=[rmse])\n",
    "    #Train the model on the entire training dataset\n",
    "    Hybrid.fit(X_train_kfold, y_train_kfold,\n",
    "                           epochs = params0['batch_size'],\n",
    "                           batch_size = params0['batch_size'],\n",
    "                           shuffle = False,\n",
    "                           # callbacks = [early_stopping,mchpt],\n",
    "                           validation_data=(X_val_kfold, y_val_kfold))\n",
    "    return Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287bac2a-0af1-4faf-a924-4481464fb7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "params0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24139ae-bd92-4d45-805c-dc8e8e6cc088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 60\n",
    "epochs = 20\n",
    "learning_rate = 1e-10\n",
    "activation = selu\n",
    "dropout_rate = 0.2\n",
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "num_units = 20\n",
    "l1 = 0.45\n",
    "l2 = 0.26\n",
    "# Build the model\n",
    "Hybrid = Sequential()\n",
    "Hybrid.add(Input(shape=(X_train_kfold.shape[1],X_train_kfold.shape[2]), name='input_layer'))\n",
    "\n",
    "Hybrid.add(GRU(units=num_units,\n",
    "               activation=activation,\n",
    "               dropout=dropout_rate,\n",
    "               return_sequences=False,\n",
    "               name=f'Hidden_layer_{i+1}'))\n",
    "Hybrid.add(Dense(1))\n",
    "Hybrid.compile(loss=rmse, optimizer=opt, metrics=[rmse])\n",
    "#Train the model on the entire training dataset\n",
    "Hybrid.fit(X_train, y_train,\n",
    "                       epochs = params0['batch_size'],\n",
    "                       batch_size = params0['batch_size'],\n",
    "                       shuffle = False,\n",
    "                       # callbacks = [early_stopping,mchpt],\n",
    "                       validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a8c553-d672-4e9d-9c5f-ee14c88aef6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GRUparams = pd.read_csv('GRUparams1.csv', index_col = 'fold')\n",
    "GRUparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f46ff-bbf5-4958-9c16-9541272b6c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    X_train_kfold,X_val_kfold = [X_train]*2\n",
    "    y_train_kfold,y_val_kfold= [y_train]*2\n",
    "    params0 =pd.read_csv('GRUparams1.csv', index_col = 'fold').iloc[i,:].to_dict()\n",
    "    exec(f'G0{i+1} = GRU_foldcv(params0)')\n",
    "    clear_output(wait = True)\n",
    "    pred = eval(f'G0{i+1}.predict(X_test)')\n",
    "    r2_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab67d07-b1ca-472a-88e6-78b8882332ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skopt.callbacks import DeltaYStopper\n",
    "r2_per_fold = []\n",
    "loss_per_fold = []\n",
    "rmse_per_fold = []\n",
    "gru_best_score = []\n",
    "\n",
    "kfold = KFold(n_splits=6)\n",
    "fold_no = 1\n",
    "parameters_fold = [None]*6\n",
    "for i, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    print(i,fold_no)\n",
    "    X_train_kfold, X_val_kfold = X_train[train_index], X_train[val_index]\n",
    "    y_train_kfold, y_val_kfold = y_train[train_index], y_train[val_index]\n",
    "    print(X_train_kfold.shape)\n",
    "    # Best model for th k-fold\n",
    "    params0 =pd.read_csv('GRUparams1.csv', index_col = 'fold').iloc[i,:].to_dict()\n",
    "    exec(f'G{i+1} = GRU_foldcv(params0)')\n",
    "    clear_output(wait = True)\n",
    "    fold_no +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672d5820-b85f-474a-b97a-9b4d0314316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_range = np.arange(0.05, 1.1, 0.18)\n",
    "y_true = y_test\n",
    "def concatenate_data(X_train, y_train, X_test, y_test):\n",
    "    X = np.concatenate([X_train, X_test], axis=0)\n",
    "    y = np.concatenate([y_train, y_test], axis=0)\n",
    "    return X, y\n",
    "def Unreshape_df(Xs, ys=None):\n",
    "    X = np.concatenate(Xs, axis=0)\n",
    "    if ys is not None:\n",
    "        y = np.concatenate(ys, axis=0)\n",
    "        return X, y\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "        return np.sqrt(np.mean(np.square(y_pred - y_true)))  \n",
    "def calculate_weighted_average(predictions, weights):\n",
    "    return sum([weight*preds for preds, weight in zip(predictions, weights)])\n",
    "\n",
    "def evaluate_ensemble(predictions, weights, y_true):\n",
    "    pred = calculate_weighted_average(predictions, weights)\n",
    "    loss = rmse(y_true,pred)\n",
    "    return r2_score(y_true, pred),loss\n",
    "\n",
    "#for i in range(2):\n",
    " #(f'GRU{i+1}')#\n",
    "best_models = [G01,G02]\n",
    "predictions = []\n",
    "losses = []\n",
    "for model in best_models:\n",
    "    y_pred = model.predict(X_test)\n",
    "    loss = rmse(y_true,y_pred)\n",
    "    losses.append(loss)\n",
    "    predictions.append(y_pred)\n",
    "evaluate_ensemble(predictions, [0.6,0.4], y_true)\n",
    "\n",
    "def Ensemble(X_topredict,best_weights,best_models):\n",
    "    model_predictions = []\n",
    "    for model in best_models:\n",
    "        y_pred = model.predict(X_topredict).T[0]\n",
    "        model_predictions.append(y_pred)\n",
    "\n",
    "    # Combine the model predictions using the optimized weights to obtain the final prediction\n",
    "    final_prediction = calculate_weighted_average(model_predictions, best_weights)\n",
    "    return final_prediction\n",
    "RNNe = Ensemble(X,[0.6,0.4],[G01,G02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb564bb7-23aa-4c35-8b38-7b63de7df29c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v = scaler.inverse_transform(RNNe.reshape(-1, 1)).T[0]\n",
    "RNNdf = pd.DataFrame().assign(RNNe = v,Time = plgm.iloc[100:-1,-3].values)\n",
    "RNNdf.to_csv('RNNpred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e672a8-81e4-4edb-9def-3e8ec8449767",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [list(parameters_fold[0].keys())[0]] + list(parameters_fold[0][list(parameters_fold[0].keys())[1]].keys()) \n",
    "GRUparams = pd.DataFrame(columns = columns)\n",
    "GRUparams.set_index('fold',inplace = True)\n",
    "for ix in range(6):\n",
    "    GRUparams.loc[ix] = ix+1\n",
    "    for i in list(parameters_fold[ix][list(parameters_fold[0].keys())[1]].keys()):\n",
    "        GRUparams.loc[ix,i] = parameters_fold[ix][list(parameters_fold[0])[1]][i]\n",
    "GRUparams#.to_csv('GRUparams1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103cce27-0bf6-4618-b1cb-788a6fc93d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def rmse(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "    \n",
    "r2 = []\n",
    "loss = []\n",
    "dic ={\"r_squared\": r_squared,\"rmse\":rmse}\n",
    "\n",
    "for p in GRUparams.index:\n",
    "    if p == 0 or p == 1:\n",
    "        params0 = GRUparams.iloc[p,:].to_dict()\n",
    "        G = GRU_foldcv(params0)\n",
    "        clear_output(wait = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aac84f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import load_model\n",
    "\n",
    "r2 = []\n",
    "loss = []\n",
    "dic ={\"r_squared\": r_squared,\"rmse\":rmse}\n",
    "best_models = []\n",
    "for p in GRUparams.index:\n",
    "    for i in list(range(0,7))[1:]:\n",
    "        if p == 0 or p == 1:\n",
    "            params = GRUparams.iloc[p,:].to_dict()\n",
    "            exec(f'model{p}_{i} = load_model(f\"GRU/gru_new0_fold{i}_model{p}.ckpt\",dic)')\n",
    "            exec(f'model{p}_{i}.compile(loss = rmse,\\\n",
    "                                     optimizer=keras.optimizers.Adam(learning_rate= params[\"learning_rate\"]),\\\n",
    "                                    metrics = [r_squared])')\n",
    "            print(f'fold{p}_{i}')\n",
    "            exec(f'pred = model{p}_{i}.predict(X_test)')\n",
    "            exec(f'best_models.append(model{p}_{i})')\n",
    "            r = r2_score(pred,y_test);r2.append(r)\n",
    "            loss.append(rmse(pred,y_test))\n",
    "    clear_output()\n",
    "for p in GRUparams.index:\n",
    "    for i in list(range(0,7))[1:]:\n",
    "        if p == 0 or p == 1:\n",
    "            print(f'Fold:{i}, r2:{r2[i]}, rmse: {loss[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f1fd76-1c85-468a-9667-4353308b56c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_input_gradients(model, x):\n",
    "    # Convert the input to a tensor\n",
    "    x = np.reshape(x, (x.shape[0], 100, 9))\n",
    "    x_tensor = tf.convert_to_tensor(x)\n",
    "    \n",
    "    # Compute the gradients of the output with respect to the input\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x_tensor)\n",
    "        output = model(x_tensor)\n",
    "        \n",
    "    gradients = tape.gradient(output, x_tensor)\n",
    "    \n",
    "    # Convert the gradients to a numpy array\n",
    "    gradients = tf.keras.backend.get_value(gradients)\n",
    "    \n",
    "    # Compute the absolute sum of the gradients for each feature\n",
    "    feature_importances = np.sum(np.abs(gradients), axis=0)\n",
    "    \n",
    "    return feature_importances\n",
    "\n",
    "# Compute the feature importances for a sample input\n",
    "x_sample = X_train[0:1, :, :]\n",
    "feature_importances = get_input_gradients(G1, x_sample)\n",
    "\n",
    "# Print the feature importances\n",
    "#print(\"Feature importances:\", feature_importances)\n",
    "\n",
    "# Compute the average feature importances across all timesteps\n",
    "average_importances = np.mean(feature_importances, axis=0)\n",
    "\n",
    "# Rank the features based on their importance\n",
    "sorted_indices = np.argsort(average_importances)[::-1]\n",
    "\n",
    "# Print the feature rankings\n",
    "names = list(df.columns)[:-1]\n",
    "for i in range(len(sorted_indices)):\n",
    "    print(f\"{i+1}. {names[sorted_indices[i]]}: {average_importances[sorted_indices[i]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82b1434-7c37-459f-a17c-8212c08cb816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_gradients(model, x):\n",
    "    # Convert the input to a tensor\n",
    "    x = np.reshape(x, (x.shape[0], 100, 9))\n",
    "    x_tensor = tf.convert_to_tensor(x)\n",
    "    \n",
    "    # Compute the gradients of the output with respect to the input\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x_tensor)\n",
    "        output = model(x_tensor)\n",
    "        \n",
    "    gradients = tape.gradient(output, x_tensor)\n",
    "    \n",
    "    # Convert the gradients to a numpy array\n",
    "    gradients = tf.keras.backend.get_value(gradients)\n",
    "    \n",
    "    # Compute the absolute sum of the gradients for each feature\n",
    "    feature_importances = np.sum(np.abs(gradients), axis=0)\n",
    "    \n",
    "    return feature_importances\n",
    "\n",
    "# Compute the feature importances for a sample input\n",
    "x_sample = X_train[0:1, :, :]\n",
    "feature_importances = get_input_gradients(G2, x_sample)\n",
    "\n",
    "# Print the feature importances\n",
    "#print(\"Feature importances:\", feature_importances)\n",
    "\n",
    "# Compute the average feature importances across all timesteps\n",
    "average_importances = np.mean(feature_importances, axis=0)\n",
    "\n",
    "# Rank the features based on their importance\n",
    "sorted_indices = np.argsort(average_importances)[::-1]\n",
    "\n",
    "# Print the feature rankings\n",
    "names = list(df.columns)[:-1]\n",
    "for i in range(len(sorted_indices)):\n",
    "    print(f\"{i+1}. {names[sorted_indices[i]]}: {average_importances[sorted_indices[i]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792dccc7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights_range = np.arange(0.05, 1.1, 0.18)\n",
    "y_true = y_test\n",
    "def concatenate_data(X_train, y_train, X_test, y_test):\n",
    "    X = np.concatenate([X_train, X_test], axis=0)\n",
    "    y = np.concatenate([y_train, y_test], axis=0)\n",
    "    return X, y\n",
    "def Unreshape_df(Xs, ys=None):\n",
    "    X = np.concatenate(Xs, axis=0)\n",
    "    if ys is not None:\n",
    "        y = np.concatenate(ys, axis=0)\n",
    "        return X, y\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "        return np.sqrt(np.mean(np.square(y_pred - y_true)))  \n",
    "def calculate_weighted_average(predictions, weights):\n",
    "    return sum([weight*preds for preds, weight in zip(predictions, weights)])\n",
    "\n",
    "def evaluate_ensemble(predictions, weights, y_true):\n",
    "    pred = calculate_weighted_average(predictions, weights)\n",
    "    loss = rmse(y_true,pred)\n",
    "    return r2_score(y_true, pred),loss\n",
    "\n",
    "#for i in range(2):\n",
    " #(f'GRU{i+1}')#\n",
    "best_models = best_modelss\n",
    "predictions = []\n",
    "losses = []\n",
    "for model in best_models:\n",
    "    y_pred = model.predict(X_test)\n",
    "    loss = rmse(y_true,y_pred)\n",
    "    losses.append(loss)\n",
    "    predictions.append(y_pred)\n",
    "iterations = len(weights_range) ** len(best_models)\n",
    "best_weights = None\n",
    "best_score = None\n",
    "best_loss = None\n",
    "track= {}\n",
    "si = 0\n",
    "bi = 0\n",
    "found = lambda si,bi,best_score: si> 0 and bi> 0 and si > bi*10000#or (best_score is not None and best_score > 0.91)\n",
    "for wx0, w1 in enumerate(weights_range):\n",
    "    if found(si,bi,best_score):\n",
    "        wx0 = weights_range[len(weights_range)-1]\n",
    "        break\n",
    "    for wx1, w2 in enumerate(weights_range):\n",
    "        if found(si,bi,best_score):\n",
    "            wx1 = weights_range[len(weights_range)-1]\n",
    "            break\n",
    "        for wx2, w3 in enumerate(weights_range):\n",
    "            if found(si,bi,best_score):\n",
    "                wx2 = weights_range[len(weights_range)-1]\n",
    "                break\n",
    "            for wx3, w4 in enumerate(weights_range):\n",
    "                if found(si,bi,best_score):\n",
    "                    wx3 = weights_range[len(weights_range)-1]\n",
    "                    break\n",
    "                for wx5, w6 in enumerate(weights_range):\n",
    "                    if found(si,bi,best_score):\n",
    "                        wx5 = weights_range[len(weights_range)-1]\n",
    "                        break\n",
    "                    for wx6, w7 in enumerate(weights_range):\n",
    "                        if found(si,bi,best_score):\n",
    "                            wx6 = weights_range[len(weights_range)-1]\n",
    "                            break\n",
    "                        for wx7, w8 in enumerate(weights_range):\n",
    "                            if found(si,bi,best_score):\n",
    "                                wx7 = weights_range[len(weights_range)-1]\n",
    "                                break\n",
    "                            for wx8, w9 in enumerate(weights_range):\n",
    "                                if found(si,bi,best_score):\n",
    "                                    wx8 = weights_range[len(weights_range)-1]\n",
    "                                    break\n",
    "                                for wx9, w10 in enumerate(weights_range):\n",
    "                                    if found(si,bi,best_score):\n",
    "                                        wx9 = weights_range[len(weights_range)-1]\n",
    "                                        break\n",
    "                                    for wx10, w11 in enumerate(weights_range):\n",
    "                                        if found(si,bi,best_score):\n",
    "                                            wx10= weights_range[len(weights_range)-1]\n",
    "                                            break\n",
    "                                        for wx12, w13 in enumerate(weights_range):\n",
    "                                            if found(si,bi,best_score):\n",
    "                                                wx12 = weights_range[len(weights_range)-1]\n",
    "                                                break\n",
    "                                            if found(si,bi,best_score):\n",
    "                                                wx6 = weights_range[len(weights_range)-1]\n",
    "                                                clear_output(wait = True)\n",
    "                                                print(f'{best_score:.2%}')\n",
    "                                                print(f'{found(si,bi,best_score)}')\n",
    "                                                print(f'{si},{bi}')\n",
    "                                                break\n",
    "                                            else:\n",
    "                                                w = [random.choice(weights_range) for _ in range(12)]\n",
    "                                                weights = [nu/sum(w) for nu in w]\n",
    "                                                score = evaluate_ensemble(predictions, weights, y_true)[0]\n",
    "                                                s_loss = evaluate_ensemble(predictions, weights, y_true)[1]\n",
    "                                                si += 1\n",
    "                                                cond = (best_score is None or best_loss is None) or\\\n",
    "                                                       (np.round(score,4) > np.round(best_score,4) and np.round(s_loss, 2) < 0.12)\n",
    "                                                if cond:\n",
    "                                                    bi += 1\n",
    "                                                    best_score = score       \n",
    "                                                    best_weights = weights\n",
    "                                                    best_loss = s_loss\n",
    "                                                    track[bi] = {'Titer':si,'score':best_score , 'loss':best_loss}\n",
    "                                                    clear_output(wait = True)\n",
    "                                                    print(f'{best_loss}')\n",
    "                                                    print(f'{best_score:.2%}')\n",
    "                                                    print(f'{best_weights}')\n",
    "                                                    print(f'{found(si,bi,best_score)}')\n",
    "                                                    print(f'recent')\n",
    "\n",
    "                                                if si> 0 and bi> 0:\n",
    "                                                    clear_output(wait = True)\n",
    "                                                    print(f'{score:.2%}')\n",
    "                                                    print(f'{loss}')\n",
    "                                                    print(f'{weights}')\n",
    "                                                    print(f'{found(si,bi,best_score)}')\n",
    "                                                    print(f'{si},{bi}')\n",
    "                                                    print(f'#### Best ####')\n",
    "                                                    print(f'{best_score:.2%}')\n",
    "                                                    print(f'{best_loss}')\n",
    "                                                    print(f'{best_weights}')\n",
    "\n",
    "\n",
    "                                            if found(si,bi,best_score):\n",
    "                                                break\n",
    "                                        if found(si,bi,best_score):\n",
    "                                            break\n",
    "                                    if found(si,bi,best_score):\n",
    "                                        break\n",
    "                                if found(si,bi,best_score):\n",
    "                                    break\n",
    "                            if found(si,bi,best_score):\n",
    "                                break\n",
    "                        if found(si,bi,best_score):\n",
    "                            break\n",
    "                    if found(si,bi,best_score):\n",
    "                        break\n",
    "                if found(si,bi,best_score):\n",
    "                    break\n",
    "            if found(si,bi,best_score):\n",
    "                break\n",
    "        if found(si,bi,best_score):        \n",
    "            track[bi] = {'Titer':si,'score':best_score , 'loss':best_loss}\n",
    "            print(best_weights, best_score)\n",
    "            break\n",
    "                #if i == 0:\n",
    "                #    best_weights1 = best_weights\n",
    "                #else:\n",
    "                #    best_weights2 = best_weights\n",
    "                #best_w = [best_weights1,best_weights2]         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d53b4a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Ensemble(X_topredict,best_weights,best_models):\n",
    "    model_predictions = []\n",
    "    for model in best_models:\n",
    "        y_pred = model.predict(X_topredict).T[0]\n",
    "        model_predictions.append(y_pred)\n",
    "\n",
    "    # Combine the model predictions using the optimized weights to obtain the final prediction\n",
    "    final_prediction = calculate_weighted_average(model_predictions, best_weights)\n",
    "    return final_prediction\n",
    "\n",
    "pred = list(Ensemble(X_test,best_weights,best_modelss))\n",
    "clear_output()\n",
    "print(r2_score(y_true, pred),rmse(y_true,pred))\n",
    "#for i in range(2):\n",
    "#    print(f'GRU{i+1}')\n",
    "#    best_models = best_modelss[i]\n",
    "#    best_weights = best_w[i]\n",
    "#    exec(f'Ens_pred{i} = list(Ensemble(X_test,best_weights,best_models))')\n",
    "#    print(r2_score(y_true, eval(f'Ens_pred{i}')),rmse(y_true,eval(f'Ens_pred{i}')))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffee14cd-86ad-45f1-86c9-e2db9986f310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_input_gradients(model, x):\n",
    "    # Convert the input to a tensor\n",
    "    x = np.reshape(x, (x.shape[0], 100, 9))\n",
    "    x_tensor = tf.convert_to_tensor(x)\n",
    "    \n",
    "    # Compute the gradients of the output with respect to the input\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x_tensor)\n",
    "        output = model(x_tensor)\n",
    "        \n",
    "    gradients = tape.gradient(output, x_tensor)\n",
    "    \n",
    "    # Convert the gradients to a numpy array\n",
    "    gradients = tf.keras.backend.get_value(gradients)\n",
    "    \n",
    "    # Compute the absolute sum of the gradients for each feature\n",
    "    feature_importances = np.sum(np.abs(gradients), axis=0)\n",
    "    \n",
    "    return feature_importances\n",
    "\n",
    "# Compute the feature importances for a sample input\n",
    "x_sample = X_train[0:1, :, :]\n",
    "feature_importances = get_input_gradients(model0_1, x_sample)\n",
    "\n",
    "# Print the feature importances\n",
    "#print(\"Feature importances:\", feature_importances)\n",
    "\n",
    "# Compute the average feature importances across all timesteps\n",
    "average_importances = np.mean(feature_importances, axis=0)\n",
    "\n",
    "# Rank the features based on their importance\n",
    "sorted_indices = np.argsort(average_importances)[::-1]\n",
    "\n",
    "# Print the feature rankings\n",
    "names = list(df.columns)[:-1]\n",
    "for i in range(len(sorted_indices)):\n",
    "    print(f\"{i+1}. {names[sorted_indices[i]]}: {average_importances[sorted_indices[i]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043a679e-4904-49be-b090-255fce153d73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_feature_importances(models, weights, X, y):\n",
    "    \"\"\"\n",
    "    Computes the feature importances using the representation erasure approach for an ensemble of models.\n",
    "    \n",
    "    Args:\n",
    "        models (list): A list of TensorFlow or Keras models.\n",
    "        weights (list): A list of floating point numbers representing the weights of each model in the ensemble.\n",
    "        X (numpy.ndarray): An array of shape (n_samples, n_features) containing the input data.\n",
    "        y (numpy.ndarray): An array of shape (n_samples,) containing the target labels.\n",
    "    \n",
    "    Returns:\n",
    "        A numpy.ndarray of shape (n_features,) containing the feature importances.\n",
    "    \"\"\"\n",
    "    baseline_score = 0\n",
    "    for i, model in enumerate(models):\n",
    "        pred = model.predict(X)\n",
    "        score = rmse(y,pred)\n",
    "        baseline_score += score * weights[i]\n",
    "    baseline_score /= np.sum(weights)\n",
    "\n",
    "    feature_importances = np.zeros(X.shape[1])\n",
    "    clear_output()\n",
    "    for i in range(X.shape[1]):\n",
    "        X_test = np.copy(X)\n",
    "        X_test[:, i] = 0\n",
    "        score = 0\n",
    "        for j, model in enumerate(models):\n",
    "            pred = model.predict(X)\n",
    "            score +=  rmse(y,pred) * weights[j]\n",
    "        feature_importances[i] = np.abs(baseline_score - score)\n",
    "\n",
    "    # Normalize the feature importances\n",
    "    feature_importances = feature_importances / np.sum(feature_importances)\n",
    "    \n",
    "    return feature_importances\n",
    "best_weights = [0.015151515151515152, 0.06969696969696969, 0.06969696969696969, 0.015151515151515152, \n",
    "                0.015151515151515152, 0.015151515151515152, 0.015151515151515152, 0.015151515151515152, \n",
    "                0.2333333333333333, 0.1787878787878788, 0.2333333333333333, 0.12424242424242422]\n",
    "fi = compute_feature_importances(best_models, best_weights, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8246c6c9-ffb0-4215-b63b-b274dca17b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights = [0.6,0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f451d3a-f928-489f-b0b3-d054ce448055",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importances = np.zeros(X.shape[2])\n",
    "clear_output()\n",
    "best_weights = [0.6,0.4]\n",
    "best_models = [G01,G02]\n",
    "\n",
    "baseline_score = 0\n",
    "for i, model in enumerate(best_models):\n",
    "    pred = model.predict(X)\n",
    "    score = np.array(rmse(y,pred))\n",
    "    baseline_score += score * best_weights[i]\n",
    "baseline_score /= np.sum(best_weights)\n",
    "for i in range(X.shape[2]):\n",
    "    X_test = np.copy(X)\n",
    "    X_test[:,:,i] = 0\n",
    "    score = 0\n",
    "    for j, model in enumerate(best_models):\n",
    "        pred = model.predict(X_test)\n",
    "        score +=  rmse(y,pred) * best_weights[j]\n",
    "    feature_importances[i] = np.abs(baseline_score - score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ec4d79-2759-4333-a8ce-d91ccd63587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "names = list(df.columns)[:-1]\n",
    "nm = {}\n",
    "for i in range(len(sorted_indices)):\n",
    "    nm[names[sorted_indices[i]]] = {'value':feature_importances[sorted_indices[i]],\n",
    "                                    'N°':int(i+1)}\n",
    "    print(f\"{i+1}. {names[sorted_indices[i]]} : {feature_importances[sorted_indices[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f054a54d-f9d5-4d53-8428-34c9f4389788",
   "metadata": {},
   "outputs": [],
   "source": [
    "FI = pd.DataFrame(nm).T\n",
    "FI['value'] = np.round(FI['value'] / np.sum(FI['value']),3)\n",
    "[print(x) for x in FI.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db9c385-d3c7-4e93-87ed-f17514432c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dic_importance = {}\n",
    "for ix, model in enumerate(best_models):\n",
    "    feature_importances = get_input_gradients(model, x_sample)\n",
    "    average_importances = np.mean(feature_importances, axis=0)\n",
    "    sorted_indices = np.argsort(average_importances)[::-1]\n",
    "    feature_rankings = {names[sorted_indices[i]]: average_importances[sorted_indices[i]] for i in range(len(sorted_indices))}\n",
    "    dic_importance[ix] = {'importance':feature_rankings}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d66a26-cb59-43bf-9014-e4fd578182b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "for _, sub_dict in dic_importance.items():\n",
    "    for key, value in subdict['importance'].items():\n",
    "        if key not in result:\n",
    "            result[key] = []\n",
    "        result[key].append(value)\n",
    "\n",
    "for key in result:\n",
    "    result[key] =sum(w * v for w, v in zip(best_weights, result[key])) / sum(best_weights)\n",
    "result = dict(sorted(result.items(), key=lambda item: item[1],reverse=True))\n",
    "result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e41777",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = scaler.inverse_transform(Ensemble(X_test,best_weights,best_modelss).reshape(-1, 1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6474908f-56a5-4a87-8a9a-d2e7c97aa0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ad8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = scaler.inverse_transform(Ensemble(X,best_weights,best_models).reshape(-1, 1)).T\n",
    "with open('RNNpred.pkl', 'wb') as f:\n",
    "    pickle.dump(pred[0], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14cc5c7-e4f1-48ab-a49e-aecc26b31155",
   "metadata": {},
   "outputs": [],
   "source": [
    "p =Ensemble(X,best_weights,best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49941b8b-bd1c-4cfc-8a0f-ff6bf73e1dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06725d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [Ens_pred0]\n",
    "predictions.append(Ens_pred1)\n",
    "#losses = [rmse(y_true,Ens_pred0),rmse(y_true,Ens_pred1)]\n",
    "iterations = len(weights_range) ** 2\n",
    "best_weights = None\n",
    "best_score = None\n",
    "best_loss = None\n",
    "track= {}\n",
    "si = 0\n",
    "bi = 0\n",
    "found = lambda si,bi,best_score: si> 0 and bi> 0 and si > bi*10000 or best_score is not None and best_score > 0.91\n",
    "    \n",
    "for wx0, w1 in enumerate(weights_range):    \n",
    "    if found(si,bi,best_score):\n",
    "        wx0 = weights_range[len(weights_range)-1]\n",
    "        break\n",
    "    for wx1, w2 in enumerate(weights_range):\n",
    "        if found(si,bi,best_score):\n",
    "            wx1 = weights_range[len(weights_range)-1]\n",
    "            clear_output(wait = True)\n",
    "            print(f'{best_score:.2%}')\n",
    "            print(f'{found(si,bi,best_score)}')\n",
    "            print(f'{si},{bi}')\n",
    "            break\n",
    "        else:\n",
    "            w = [random.choice(weights_range) for _ in range(2)]\n",
    "            weights = [nu/sum(w) for nu in w]\n",
    "            score = evaluate_ensemble(predictions, weights, y_true)[0]\n",
    "            s_loss = evaluate_ensemble(predictions, weights, y_true)[1]\n",
    "            si += 1\n",
    "            cond = (best_score is None or best_loss is None) or (np.round(score,4) > np.round(best_score,4) and np.round(s_loss, 2) < 0.12)\n",
    "            if cond:\n",
    "                bi += 1\n",
    "                best_score = score       \n",
    "                best_weights = weights\n",
    "                if best_loss is None:\n",
    "                    best_loss = s_loss\n",
    "                track[bi] = {'Titer':si,'score':best_score , 'loss':best_loss}\n",
    "                if (np.round(s_loss,2) < np.round(best_loss, 2)):\n",
    "                    if not cond:\n",
    "                        bi += 1\n",
    "                    best_score = score \n",
    "                    best_loss = s_loss\n",
    "                    best_weights = weights        \n",
    "                    track[bi] = {'Titer':si,'score':best_score , 'loss':best_loss}\n",
    "                clear_output(wait = True)\n",
    "                print(f'{best_loss}')\n",
    "                print(f'{best_score:.2%}')\n",
    "                print(f'{best_weights}')\n",
    "                print(f'{found(si,bi,best_score)}')\n",
    "                print(f'recent')\n",
    "\n",
    "            if si> 0 and bi> 0:\n",
    "                clear_output(wait = True)\n",
    "                print(f'{score:.2%}')\n",
    "                print(f'{loss}')\n",
    "                print(f'{weights}')\n",
    "                print(f'{found(si,bi,best_score)}')\n",
    "                print(f'{si},{bi}')\n",
    "                print(f'#### Best ####')\n",
    "                print(f'{best_score:.2%}')\n",
    "                print(f'{best_loss}')\n",
    "                print(f'{best_weights}')\n",
    "\n",
    "            if found(si,bi,best_score):\n",
    "                wx1 = weights_range[len(weights_range)-1]\n",
    "                break\n",
    "    if found(si,bi,best_score):\n",
    "        track[bi] = {'Titer':si,'score':best_score , 'loss':best_loss}\n",
    "        print(best_weights, best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac17aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = []\n",
    "loss = []\n",
    "for i in list(range(0,7))[1:]:\n",
    "    exec(f'model{i} = load_model(\"GRU/gru_{i}_fold.ckpt\",dic)')\n",
    "    print(f'fold{i}')\n",
    "    exec(f'pred{i} = model{i}.predict(X_test)')\n",
    "    r = eval(f'r2_score(pred{i},y_test)');r2.append(r)\n",
    "    eval(f'loss.append(rmse(pred{i},y_test))')\n",
    "clear_output()\n",
    "for i in list(range(0,6)):\n",
    "    print(i)\n",
    "    print(f'Fold:{i+1}, r2:{r2[i]}, rmse: {loss[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4216df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {'r_squared':r_squared,'rmse':rmse}\n",
    "\n",
    "r2 = []\n",
    "loss = []\n",
    "for i in list(range(0,7))[1:]:\n",
    "    exec(f'model{i} = load_model(\"GRU/gru_{i}_fold.ckpt\",dic)')\n",
    "    print(f'fold{i}')\n",
    "    exec(f'pred{i} = model{i}.predict(X_test)')\n",
    "    r = eval(f'r2_score(pred{i},y_test)');r2.append(r)\n",
    "    eval(f'loss.append(rmse(pred{i},y_test))')\n",
    "clear_output()\n",
    "for i in list(range(0,6)):\n",
    "    print(i)\n",
    "    print(f'Fold:{i+1}, r2:{r2[i]}, rmse: {loss[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06787b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r2_score(y,Ens_pred),rmse(y,Ens_pred)\n",
    "r2_score(y_true,Ens_pred),rmse(y_true,Ens_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea8a2bc-8ba6-4f99-888c-d1b94c7ec696",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa3264f-65ad-4b51-839f-bce457318499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.callbacks import DeltaYStopper\n",
    "n_splits = 6\n",
    "kfold = KFold(n_splits=n_splits, shuffle=False)\n",
    "\n",
    "# Define the search space\n",
    "search_space = [Categorical([10,20,40], name='batch_size'),\n",
    "                Categorical([10], name='epochs'),\n",
    "                Real(1e-6, 1e-3 ,'log-uniform',name='learning_rate'),\n",
    "                Categorical(['relu', 'tanh','selu','elu'], name='activation'),\n",
    "                Real(0.0, 0.5, name='dropout_rate'),\n",
    "                Categorical(['Adam', 'SGD','rmsprop'], name='optimizer'),\n",
    "                Integer(50, 200, name='num_units'),\n",
    "                Integer(1, 3, name='num_layers'),\n",
    "                Real(0.1,0.9,name='lambda1'), \n",
    "                Real(0.1,0.9,name='lambda2')]\n",
    "\n",
    "@use_named_args(search_space)\n",
    "def objective(**params):\n",
    "    # Unpack the hyperparameters\n",
    "    batch_size = params['batch_size']\n",
    "    epochs = params['epochs']\n",
    "    learning_rate = params['learning_rate']\n",
    "    activation = params['activation']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    optimizer = params['optimizer']\n",
    "    num_units = params['num_units']\n",
    "    num_layers = params['num_layers']\n",
    "    \n",
    "    if optimizer == 'Adam':\n",
    "        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'SGD':\n",
    "        opt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    \n",
    "    GRUmodel = Sequential()\n",
    "    GRUmodel.add(Input(shape=(X_train.shape[1],X_train.shape[2]), name='input_layer'))\n",
    "    for i in range(num_layers):\n",
    "        if i == num_layers - 1:\n",
    "            GRUmodel.add(GRU(units=num_units,\n",
    "                           activation=activation,\n",
    "                           dropout=dropout_rate,\n",
    "                           return_sequences=False,\n",
    "                           name=f'Hidden_layer_{i+1}'))\n",
    "        else:\n",
    "            # Not the last GRU layer\n",
    "            GRUmodel.add(GRU(units=num_units,\n",
    "                           activation=activation,\n",
    "                           dropout=dropout_rate,\n",
    "                           return_sequences=True,\n",
    "                           name=f'Hidden_layer_{i+1}'))\n",
    "    GRUmodel.add(Dense(1))\n",
    "    GRUmodel.compile(loss=rmse, optimizer= opt, metrics=[r_squared])\n",
    "    \n",
    "    print(f)\n",
    "    # Train the model on the training data for this fold\n",
    "    history = GRUmodel.fit(X_train_kfold, y_train_kfold,\n",
    "                           batch_size=batch_size,\n",
    "                           epochs=epochs)\n",
    "    \n",
    "    # Evaluate the model on the validation data for this fold\n",
    "    val_loss, val_r_squared = GRUmodel.evaluate(X_val_kfold, y_val_kfold)\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "delta_stopper = DeltaYStopper(delta=0.1)\n",
    "for train_index, val_index in kfold.split(X_train, y_train):\n",
    "    X_train_kfold, X_val_kfold = X_train[train_index], X_train[val_index]\n",
    "    y_train_kfold, y_val_kfold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    # Perform Bayesian optimization on the training data for this fold\n",
    "    res_gp = gp_minimize(objective,\n",
    "                         search_space,\n",
    "                         n_calls=15,\n",
    "                         callback=[delta_stopper],\n",
    "                         random_state=42)\n",
    "    \n",
    "    # Print the best score and best parameters for this fold\n",
    "    print(f\"Best score for fold {i}: {res_gp.fun} using {res_gp.x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9882efd4-c99e-42a7-92d8-b51326fca56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "r2_score(y_test,pred),rmse(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727dff0e-4a5b-4b12-8882-b775cbc181a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Unreshape_df(Xs, ys=None):\n",
    "    X = np.concatenate(Xs, axis=0)\n",
    "    if ys is not None:\n",
    "        y = np.concatenate(ys, axis=0)\n",
    "        return X, y\n",
    "    else:\n",
    "        return X\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd441c5-5ddc-4c74-8baa-bac4e77785f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(unpred.reshape(-1, 1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a54ee7-59bb-47ed-bfce-e16967d87581",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpred = Unreshape_df(Xs = pred)\n",
    "predicted = scaler.inverse_transform(unpred.reshape(-1, 1)).T[0]\n",
    "observed =  scaler.inverse_transform(Unreshape_df(Xs = y_test).reshape(-1, 1)).T[0]\n",
    "r2_score(predicted,observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70b8ab-b610-462e-9e5a-81cc45bf60ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "optimizers = {'Adam':Adam, 'SGD':SGD, 'RMSprop':RMSprop, 'Adamax':Adamax, 'Nadam':Nadam}\n",
    "activation_functions = ['selu','elu',LeakyReLU,'relu'] #()\n",
    "\n",
    "# Define the hyperparameter search space:\n",
    "pbounds = {'dropout_rate': (0.1, 0.5),\n",
    "           'num_neurons': (8,256),\n",
    "           'learning_rate': (-7, -2),\n",
    "           'activation_function': (0, len(activation_functions)-1),\n",
    "           'optimizer': (0, len(optimizers)-1),\n",
    "           'num_hidden_layers': (1, 3),\n",
    "           'epochs': (5,100),\n",
    "           'batch_size': (8, 128)\n",
    "          }\n",
    "\n",
    "# Define a function to train and evaluate the RNN model for a given set of hyperparameters:\n",
    "def rnn_evaluate(epochs,batch_size,dropout_rate, num_neurons, learning_rate, activation_function, optimizer, num_hidden_layers):\n",
    "    \n",
    "    # Convert integer indices back to hyperparameters:\n",
    "    activation_function = activation_functions[int(activation_function)]\n",
    "    optimizer = list(optimizers.values())[int(optimizer)](learning_rate=learning_rate)\n",
    "    \n",
    "    # Perform 6-fold cross validation:\n",
    "    num_folds = 6\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "    acc_list = []\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "    for train_idx, test_idx in kf.split(X_train,y_train):\n",
    "        X_Train, X_val = X[train_idx], X[test_idx]\n",
    "        y_Train, y_val = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Define and train the RNN model:\n",
    "        model = Sequential()\n",
    "        model.add(SimpleRNN(units=int(num_neurons), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        for i in range(int(num_hidden_layers)-1):\n",
    "            model.add(Dense(num_neurons, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss=rmse, optimizer=optimizer, metrics=[r_squared])\n",
    "        #model.fit(X_Train, y_Train, epochs=10, batch_size=32,validation_data=(X_val, y_val), verbose=0)\n",
    "        #model.fit(X_Train, y_Train, epochs=int(epochs), batch_size= int(batch_size),validation_data=(X_val, y_val),verbose=0)\n",
    "        history = model.fit(X_Train, y_Train, epochs=int(epochs), batch_size=int(batch_size),\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=0, callbacks=[es])   \n",
    "        \n",
    "        y_pred = scaler.inverse_transform(model.predict(X_test))\n",
    "        mse_v = mean_squared_error(scaler.inverse_transform(y_test), y_pred)\n",
    "        rmse_v = np.sqrt(mse_v)\n",
    "        acc_list.append(rmse_v)\n",
    "    \n",
    "    # Return the mean accuracy over all folds as the performance metric of interest:\n",
    "    return sum(acc_list) / len(acc_list)\n",
    "\n",
    "# Perform Bayesian optimization to find the optimal hyperparameters:\n",
    "optimizer = BayesianOptimization(f=rnn_evaluate, pbounds=pbounds, random_state=42)\n",
    "optimizer.maximize(init_points=5, n_iter=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a04b8-25ef-4cf3-8835-4bfc5ca228cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimizer.max)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c152de-42f5-479b-b0b4-44b6196aa474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, LSTM\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the optimal hyperparameters found by the optimizer\n",
    "\n",
    "def RNN_PLGM(X,param_dict,s = 0.8309):\n",
    "    # Convert integer indices back to hyperparameters:\n",
    "    optimizers = {'Adam':Adam, 'SGD':SGD, 'RMSprop':RMSprop, 'Adamax':Adamax, 'Nadam':Nadam}\n",
    "    activation_functions = ['selu','elu',LeakyReLU,'relu'] \n",
    "    \n",
    "    my_dict= param_dict    \n",
    "    batch_size = int(my_dict['batch_size'])\n",
    "    dropout_rate = my_dict['dropout_rate']\n",
    "    epochs = int(my_dict['epochs'])\n",
    "    learning_rate = my_dict['learning_rate']\n",
    "    num_hidden_layers = my_dict['num_hidden_layers']\n",
    "    num_neurons = my_dict['num_neurons']\n",
    "    activation_function = activation_functions[int(my_dict['activation_function'])]\n",
    "    optimizer = list(optimizers.values())[int(my_dict['optimizer'])](learning_rate=learning_rate)\n",
    "    \n",
    "    split = int((s)*X.shape[0])\n",
    "    X_train = X[:split,:,:]\n",
    "    y_train = y[:split,:]\n",
    "    X_test =  X[split:,:,:]\n",
    "    y_test =  y[split:,:]\n",
    "    \n",
    "    # Define and train the RNN model:\n",
    "    RNN = Sequential()\n",
    "    RNN.add(SimpleRNN(units=int(num_neurons), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    for i in range(int(num_hidden_layers)-1):\n",
    "        RNN.add(Dense(num_neurons, activation=activation_function))\n",
    "        RNN.add(Dropout(dropout_rate))\n",
    "    RNN.add(Dense(1, activation='sigmoid'))\n",
    "    RNN.compile(loss=rmse, optimizer=optimizer, metrics=[r_squared])\n",
    "    # Train the model with your data\n",
    "    RNN.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "    # Make predictions on new data\n",
    "    y_pred = RNN.predict(X_test)\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78466c47-e8be-4fab-94bb-cc52e6bcbbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimal hyperparameters obtained from optimization\n",
    "# Define and compile the RNN model with optimal hyperparameters\n",
    "param_dict = my_dict \n",
    "batch_size = int(my_dict['batch_size'])\n",
    "dropout_rate = my_dict['dropout_rate']\n",
    "epochs = int(my_dict['epochs'])\n",
    "learning_rate = my_dict['learning_rate']\n",
    "num_hidden_layers = my_dict['num_hidden_layers']\n",
    "num_neurons = my_dict['num_neurons']\n",
    "activation_function = activation_functions[int(my_dict['activation_function'])]\n",
    "optimizer = list(optimizers.values())[int(my_dict['optimizer'])](learning_rate=learning_rate)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=int(num_neurons), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "for i in range(int(num_hidden_layers)-1):\n",
    "    model.add(Dense(num_neurons, activation=activation_function))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss=rmse, optimizer=optimizer, metrics=[r_squared])\n",
    "\n",
    "# Train the model on the entire training set with optimal hyperparameters\n",
    "history = model.fit(X_train, y_train, epochs=int(epochs), batch_size=int(batch_size),\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = scaler.inverse_transform(model.predict(X_test))\n",
    "mse_v = mean_squared_error(scaler.inverse_transform((y_test), y_pred)\n",
    "rmse_v = np.sqrt(mse_v)\n",
    "r2 = r2_score(scaler.inverse_transform((y_test), y_pred)\n",
    "print('RMSE:', rmse_v,'R2',r2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b106e299-33db-4e8a-9a2c-5cedff83a729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = scaler.inverse_transform(model.predict(X_test))\n",
    "#X_train: (13006, 100, 15)\n",
    "#X_test: (2200, 100, 15)\n",
    "#y_train: (13006, 1)\n",
    "#y_test: (2200, 1)\n",
    "p =y_test.reshape(2200, 1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.inverse_transform(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
