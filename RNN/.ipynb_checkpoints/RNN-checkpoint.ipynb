{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35e6ac7b-b185-421a-ad37-11339f5a1dcf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f23e5e7-cb86-4849-9dbf-97fd189d48e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs in the system: 16\n"
     ]
    }
   ],
   "source": [
    "# Common imports\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import absl.logging\n",
    "\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)# Avoid print error\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np \n",
    "from numpy.random import seed\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.options.mode.chained_assignment = None # avoid error \n",
    "pd.options.display.float_format = '{:.5f}'.format # No scientific annotation when print dataframe\n",
    "from functools import reduce\n",
    "  \n",
    "# ADA and Modeling imports\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler,MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split,KFold,GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error, mean_squared_log_error, accuracy_score\n",
    "from IPython.display import clear_output\n",
    "import statistics\n",
    "from scipy.stats import stats\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#from tsmoothie.smoother import *\n",
    "\n",
    "# To plot pretty figures\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Build function\n",
    "#from HelperFunctions import *\n",
    "#from Implementation import *\n",
    "\n",
    "# To generate an stable output across runs\n",
    "rnd_seed = 42\n",
    "rnd_gen = np.random.default_rng(rnd_seed)\n",
    "\n",
    "# MAchine learing packages \n",
    "import tensorflow \n",
    "from keras.layers import SimpleRNN\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import metrics \n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD, Adamax, Nadam\n",
    "from tensorflow.keras.activations import selu,elu,relu,tanh\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args\n",
    "import tensorflow_probability as tfp\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from keras import backend as K\n",
    "from keras import Input #\n",
    "from keras.models import Sequential, load_model\n",
    "import keras.backend as K \n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Dropout, LSTM, GRU, InputLayer, Flatten, Conv1D, Activation, LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "\n",
    "n_cpu = os.cpu_count()\n",
    "print(\"Number of CPUs in the system:\", n_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0c58c-399a-4ca0-b79b-d81b0d13324e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe02f1dc-ea80-421b-a045-5e2ea6a54d0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'plgm.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m'''Importing data'''\u001b[39;00m\n\u001b[0;32m      2\u001b[0m cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msol[W/m2]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHypTemp\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQout\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpiV\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHypV\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mu2*[m/s]\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpiTemp\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPEPItrue\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTherm\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHvd\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntrP\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPLoad\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m#,EpiPpred','HypPpred','EpiV','HypV',\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m plgm \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mplgm.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minterpolate()\n\u001b[0;32m      7\u001b[0m meteo \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeteo.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m ThermML \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThermML.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[0;32m      9\u001b[0m                 \u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m:,:][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_ensemble\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrat\u001b[39m\u001b[38;5;124m'\u001b[39m]]\\\n\u001b[0;32m     10\u001b[0m                 \u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'plgm.csv'"
     ]
    }
   ],
   "source": [
    "'''Importing data'''\n",
    "cols = ['sol[W/m2]','St', 'HypTemp','Qout',\n",
    "        'EpiV','HypV',\n",
    "        'u2*[m/s]',\n",
    "        'EpiTemp','PEPItrue','Therm','Hvd','EntrP','PLoad'] #,EpiPpred','HypPpred','EpiV','HypV',\n",
    "plgm = pd.read_csv('plgm.csv').interpolate()\n",
    "meteo = pd.read_csv('meteo.csv')\n",
    "ThermML = pd.read_csv('ThermML.csv')\\\n",
    "                .iloc[1:,:][['Time','y_ensemble','Strat']]\\\n",
    "                .set_index('Time')\n",
    "df= pd.read_csv('DF.csv', sep = ',')\\\n",
    "                .iloc[1:13107,:]\\\n",
    "                .assign(Time = pd.to_datetime(pd.read_csv('DF.csv', sep = ',').iloc[1:,:]['Time']),\n",
    "                        Therm = [0 if str(x)=='nan' else int(x) for x in ThermML['y_ensemble']][1:],\n",
    "                        Strat = ThermML['Strat'][1:],\n",
    "                        EpiV  = plgm['EpiV[L]'][1:],\n",
    "                        HypV  = plgm['HypV[L]'][1:],\n",
    "                        Hvd = plgm.Hvd[1:],\n",
    "                        EntrP = plgm['EntrP[ug]'],\n",
    "                        PEPItrue = plgm['PEPItrue[ug]'][1:],\n",
    "                        Qout = meteo['Qout[l/d]'] )\n",
    "df['PLoad'] = df['PLoad[g]']*10*6  \n",
    "df = df.loc[:,cols]\n",
    "df = df.interpolate()\n",
    "df = df[['sol[W/m2]', 'St',\n",
    "    'PLoad', 'u2*[m/s]',\n",
    "    'Qout','EpiTemp',\n",
    "    'HypTemp','HypV',\n",
    "    'EpiV','PEPItrue']]#\n",
    "\n",
    "\n",
    "'''Data normlization and reshaping'''\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(df.iloc[:,:-1])\n",
    "X_df =pd.DataFrame(X_scaled)\n",
    "\n",
    "y_scaled = scaler.fit_transform((df.iloc[:,-1].to_numpy()).reshape(-1,1))#.reshape(-1,1)\n",
    "y_df =pd.DataFrame(y_scaled)\n",
    "\n",
    "# Selecting features and label for training \n",
    "\n",
    "features = X_df.astype('float32') # Training data\n",
    "features[2] = features[2].astype(int) # Strat as integer\n",
    "label = y_df.astype('float32')#.to_numpy().reshape(-1,1) # 'Labels'\n",
    "# Reshaping data into n_samples x timestep to be feed into the GRU netwok\n",
    "def Reshape_df(X, y, time_steps):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "X, y = Reshape_df(features, label, 100)\n",
    "\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "\n",
    "'''Data Splitting'''\n",
    "\n",
    "s = 0.8309\n",
    "split = int((s)*X.shape[0])\n",
    "X_train = X[:split,:,:]\n",
    "y_train = y[:split,:]\n",
    "X_test =  X[split:,:,:]\n",
    "y_test =  y[split:,:]\n",
    "\n",
    "sets = [X, X_test, y, y_test]\n",
    "sets_n = ['X_train', 'X_test', 'y_train', 'y_test']\n",
    "\n",
    "for i,im in enumerate(sets):\n",
    "    print(f'{sets_n[i]}:',im.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654755c0-0c5a-4900-b959-616958b1f403",
   "metadata": {},
   "source": [
    "## RNN functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8a99da1-f68b-4ddf-b4d9-d097b4a26694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.callbacks import DeltaYStopper\n",
    "search_space = [Categorical([12,34,60,120], name='batch_size'),\n",
    "                Categorical([10,30], name='epochs'),\n",
    "                Real(1e-8, 1e-3,'log-uniform',name='learning_rate'),\n",
    "                Categorical(['relu', 'tanh','selu','elu'], name='activation'),\n",
    "                Real(0.0, 0.5, name='dropout_rate'),\n",
    "                Categorical(['Adam', 'SGD','rmsprop'], name='optimizer'),\n",
    "                Integer(10, 200, name='num_units'),\n",
    "                Integer(1, 3, name='num_layers'),\n",
    "                Real(0.1,0.9,name='lambda1'), \n",
    "                Real(0.1,0.9,name='lambda2')]\n",
    "\n",
    "# Define the objective function to be minimized\n",
    "@use_named_args(search_space)\n",
    "def Hybrid_model(**params):\n",
    "\n",
    "\n",
    "    batch_size = params['batch_size']\n",
    "    epochs = params['epochs']\n",
    "    learning_rate = params['learning_rate']\n",
    "    activation = params['activation']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    optimizer = params['optimizer']\n",
    "    num_units = params['num_units']\n",
    "    num_layers = params['num_layers']\n",
    "    l1 = params['lambda1']\n",
    "    l2 = params['lambda2']\n",
    "\n",
    "    if optimizer == 'Adam':\n",
    "        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'SGD':\n",
    "        opt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    # Build the model\n",
    "    Hybrid = Sequential()\n",
    "    Hybrid.add(Input(shape=(X_train_kfold.shape[1],X_train_kfold.shape[2]), name='input_layer'))\n",
    "                     \n",
    "    for i in range(num_layers):\n",
    "        if i == num_layers - 1:\n",
    "            Hybrid.add(GRU(units=num_units,\n",
    "                           activation=activation,\n",
    "                           dropout=dropout_rate,\n",
    "                           return_sequences=False,\n",
    "                           name=f'Hidden_layer_{i+1}'))\n",
    "        else:\n",
    "            # Not the last GRU layer\n",
    "            Hybrid.add(GRU(units=num_units,\n",
    "                           activation=activation,\n",
    "                           dropout=dropout_rate,\n",
    "                           return_sequences=True,\n",
    "                           name=f'Hidden_layer_{i+1}'))\n",
    "    Hybrid.add(Dense(1))\n",
    "    Hybrid.compile(loss=rmse, optimizer=opt, metrics=[r_squared])\n",
    "\n",
    "    # Train the model on the training data for this fold\n",
    "    \n",
    "    # Return the validation loss as the objective value to be minimized\n",
    "\n",
    "    # Evaluate the model on the validation data for this fold\n",
    "    Hybrid.fit(X_train_kfold,y_train_kfold,\n",
    "                   batch_size=batch_size,\n",
    "                   epochs=epochs,\n",
    "                   verbose=1)\n",
    "    val_loss, val_r_squared = Hybrid.evaluate(X_val_kfold, y_val_kfold, verbose=1)\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad513f53",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "Iteration No: 1 started. Evaluating function at random point.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     22\u001b[0m delta_stopper \u001b[38;5;241m=\u001b[39m DeltaYStopper(delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgp_minimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHybrid_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mn_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdelta_stopper\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m                \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m clear_output(wait \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaving\u001b[39m\u001b[38;5;124m'\u001b[39m,fold_no)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skopt\\optimizer\\gp.py:259\u001b[0m, in \u001b[0;36mgp_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    255\u001b[0m     base_estimator \u001b[38;5;241m=\u001b[39m cook_estimator(\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGP\u001b[39m\u001b[38;5;124m\"\u001b[39m, space\u001b[38;5;241m=\u001b[39mspace, random_state\u001b[38;5;241m=\u001b[39mrng\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax),\n\u001b[0;32m    257\u001b[0m         noise\u001b[38;5;241m=\u001b[39mnoise)\n\u001b[1;32m--> 259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase_minimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43macq_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macq_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkappa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkappa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macq_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macq_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_random_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_random_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_initial_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_initial_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_point_generator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_point_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_restarts_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_restarts_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_queue_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skopt\\optimizer\\base.py:298\u001b[0m, in \u001b[0;36mbase_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m# Optimize\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_calls):\n\u001b[1;32m--> 298\u001b[0m     next_x \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     next_y \u001b[38;5;241m=\u001b[39m func(next_x)\n\u001b[0;32m    300\u001b[0m     result \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mtell(next_x, next_y)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:367\u001b[0m, in \u001b[0;36mOptimizer.ask\u001b[1;34m(self, n_points, strategy)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"Query point or multiple points at which objective should be evaluated.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03mn_points : int or None, default: None\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    364\u001b[0m \n\u001b[0;32m    365\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_points \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ask\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m supported_strategies \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl_min\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl_max\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(n_points, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m n_points \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:434\u001b[0m, in \u001b[0;36mOptimizer._ask\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_initial_points \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_estimator_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;66;03m# this will not make a copy of `self.rng` and hence keep advancing\u001b[39;00m\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;66;03m# our random state.\u001b[39;00m\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 434\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    436\u001b[0m         \u001b[38;5;66;03m# The samples are evaluated starting form initial_samples[0]\u001b[39;00m\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_samples[\n\u001b[0;32m    438\u001b[0m             \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_initial_points]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skopt\\space\\space.py:900\u001b[0m, in \u001b[0;36mSpace.rvs\u001b[1;34m(self, n_samples, random_state)\u001b[0m\n\u001b[0;32m    897\u001b[0m columns \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdimensions:\n\u001b[1;32m--> 900\u001b[0m     columns\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    902\u001b[0m \u001b[38;5;66;03m# Transpose\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _transpose_list_array(columns)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skopt\\space\\space.py:698\u001b[0m, in \u001b[0;36mCategorical.rvs\u001b[1;34m(self, n_samples, random_state)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minverse_transform([(choices)])\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalize\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchoices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories[c] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m choices]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skopt\\space\\space.py:685\u001b[0m, in \u001b[0;36mCategorical.inverse_transform\u001b[1;34m(self, Xt)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;124;03m\"\"\"Inverse transform samples from the warped space back into the\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;124;03m   original space.\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;66;03m# The concatenation of all transformed dimensions makes Xt to be\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;66;03m# of type float, hence the required cast back to int.\u001b[39;00m\n\u001b[1;32m--> 685\u001b[0m inv_transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCategorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inv_transform, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    687\u001b[0m     inv_transform \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(inv_transform)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skopt\\space\\space.py:168\u001b[0m, in \u001b[0;36mDimension.inverse_transform\u001b[1;34m(self, Xt)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minverse_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, Xt):\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;124;03m\"\"\"Inverse transform samples from the warped space back into the\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m       original space.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skopt\\space\\transformers.py:309\u001b[0m, in \u001b[0;36mPipeline.inverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minverse_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m transformer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformers[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 309\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skopt\\space\\transformers.py:275\u001b[0m, in \u001b[0;36mNormalize.inverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    273\u001b[0m X_orig \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhigh \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_int:\n\u001b[1;32m--> 275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mround(X_orig)\u001b[38;5;241m.\u001b[39mastype(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_orig\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "# Define per-fold score containers\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "from skopt.callbacks import DeltaYStopper\n",
    "r2_per_fold = []\n",
    "loss_per_fold = []\n",
    "rmse_per_fold = []\n",
    "gru_best_score = []\n",
    "\n",
    "#l1 = 0.5\n",
    "#l2 = 0.5\n",
    "kfold = KFold(n_splits=6)\n",
    "fold_no =1\n",
    "parameters_fold = [None]*6\n",
    "#for train_index, val_index in kfold.split(X_train, y_train):\n",
    "for i, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):       \n",
    "    print(i,fold_no)\n",
    "    X_train_kfold, X_val_kfold = X_train[train_index], X_train[val_index]\n",
    "    y_train_kfold, y_val_kfold = y_train[train_index], y_train[val_index]\n",
    "        # Best model for th k-fold\n",
    "    loss = True\n",
    "    delta_stopper = DeltaYStopper(delta=0.1)\n",
    "    result = gp_minimize(func=Hybrid_model,\n",
    "                     dimensions=search_space,\n",
    "                     n_calls=10,\n",
    "                    random_state=42,\n",
    "                    callback=[delta_stopper],\n",
    "                    verbose=True)\n",
    "    clear_output(wait = True)\n",
    "    print('Saving',fold_no)\n",
    "    params0 = {param.name: value for param, value in zip(search_space, result.x)}\n",
    "    parameters_fold[i] = {'fold':fold_no,'parameters':params0}\n",
    "    clear_output(wait = True)\n",
    "    fold_no +=1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f1d031c-e2d6-4727-836a-826a8ad1e6d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GRU_foldcv(params0):\n",
    "    batch_size = params0['batch_size']\n",
    "    epochs = params0['epochs']\n",
    "    learning_rate = params0['learning_rate']\n",
    "    activation = params0['activation']\n",
    "    dropout_rate = params0['dropout_rate']\n",
    "    optimizer = params0['optimizer']\n",
    "    num_units = params0['num_units']\n",
    "    num_layers = params0['num_layers']\n",
    "    l1 = params0['lambda1']\n",
    "    l2 = params0['lambda2']\n",
    "\n",
    "    if optimizer == 'Adam':\n",
    "        #opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        opt = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'SGD':\n",
    "         opt =optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate)\n",
    "         #opt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = tf.keras.optimizers.legacy.RMSprop(learning_rate=learning_rate)\n",
    "        #opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "  \n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):       \n",
    "            X_train_kfold, X_val_kfold = X_train[train_index], X_train[val_index]\n",
    "            y_train_kfold, y_val_kfold = y_train[train_index], y_train[val_index]\n",
    "            mchpt = tf.keras.callbacks.ModelCheckpoint(filepath=f'GRU/gru_new0_fold{i+1}_model{p}.ckpt',\n",
    "                                                       monitor = 'val_loss' ,\n",
    "                                                       save_best_only=True,\n",
    "                                                       mode ='min', \n",
    "                                                       verbose =0)#,save_weights_only=True) restore_best_weights = True\n",
    "            early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', \n",
    "                                                              patience=5, mode ='min'\n",
    "                                                              ,verbose =0) # try patience of 3 or moin\n",
    "\n",
    "            print(f'fitting{i+1}')\n",
    "                        \n",
    "            # Build the model\n",
    "            Hybrid = Sequential()\n",
    "            Hybrid.add(Input(shape=(X_train_kfold.shape[1],X_train_kfold.shape[2]), name='input_layer'))\n",
    "\n",
    "            for i in range(num_layers):\n",
    "                if i == num_layers - 1:\n",
    "                    Hybrid.add(GRU(units=num_units,\n",
    "                                   activation=activation,\n",
    "                                   dropout=dropout_rate,\n",
    "                                   return_sequences=False,\n",
    "                                   name=f'Hidden_layer_{i+1}'))\n",
    "                else:\n",
    "                    # Not the last GRU layer\n",
    "                    Hybrid.add(GRU(units=num_units,\n",
    "                                   activation=activation,\n",
    "                                   dropout=dropout_rate,\n",
    "                                   return_sequences=True,\n",
    "                                   name=f'Hidden_layer_{i+1}'))\n",
    "            Hybrid.add(Dense(1))\n",
    "            Hybrid.compile(loss=rmse, optimizer=opt, metrics=[r_squared])\n",
    "            #Train the model on the entire training dataset\n",
    "            Hybrid.fit(X_train_kfold, y_train_kfold,\n",
    "                                   epochs = params0['batch_size'],\n",
    "                                   batch_size = params0['batch_size'],\n",
    "                                   shuffle = False,\n",
    "                                   callbacks = [early_stopping,mchpt],\n",
    "                                   validation_data=(X_val_kfold, y_val_kfold))\n",
    "    return Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd5babde-6230-4fed-9d99-09c603224bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = K.sum(K.square(y_true - y_pred))\n",
    "    ss_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return 1 - ss_res / (ss_tot + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67694646-b6bf-46be-9160-ec5887e0b694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GRU_foldcv(params0):\n",
    "    batch_size = params0['batch_size']\n",
    "    epochs = params0['epochs']\n",
    "    learning_rate = params0['learning_rate']\n",
    "    activation = params0['activation']\n",
    "    dropout_rate = params0['dropout_rate']\n",
    "    optimizer = params0['optimizer']\n",
    "    num_units = params0['num_units']\n",
    "    num_layers = params0['num_layers']\n",
    "    l1 = params0['lambda1']\n",
    "    l2 = params0['lambda2']\n",
    "\n",
    "    if optimizer == 'Adam':\n",
    "        #opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        opt = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'SGD':\n",
    "         opt =optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate)\n",
    "         #opt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = tf.keras.optimizers.legacy.RMSprop(learning_rate=learning_rate)\n",
    "        #opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "  \n",
    "    \n",
    "    # for i, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):       \n",
    "    #         X_train_kfold, X_val_kfold = X_train[train_index], X_train[val_index]\n",
    "    #         y_train_kfold, y_val_kfold = y_train[train_index], y_train[val_index]\n",
    "            # mchpt = tf.keras.callbacks.ModelCheckpoint(filepath=f'GRU/gru_new0_fold{i+1}_model{p}.ckpt',\n",
    "            #                                            monitor = 'val_loss' ,\n",
    "            #                                            save_best_only=True,\n",
    "            #                                            mode ='min', \n",
    "            #                                            verbose =0)#,save_weights_only=True) restore_best_weights = True\n",
    "            # early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', \n",
    "            #                                                   patience=5, mode ='min'\n",
    "            #                                                   ,verbose =0) # try patience of 3 or moin\n",
    "                        \n",
    "            # Build the model\n",
    "    Hybrid = Sequential()\n",
    "    Hybrid.add(Input(shape=(X_train_kfold.shape[1],X_train_kfold.shape[2]), name='input_layer'))\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        if i == num_layers - 1:\n",
    "            Hybrid.add(GRU(units=num_units,\n",
    "                           activation=activation,\n",
    "                           dropout=dropout_rate,\n",
    "                           return_sequences=False,\n",
    "                           name=f'Hidden_layer_{i+1}'))\n",
    "        else:\n",
    "            # Not the last GRU layer\n",
    "            Hybrid.add(GRU(units=num_units,\n",
    "                           activation=activation,\n",
    "                           dropout=dropout_rate,\n",
    "                           return_sequences=True,\n",
    "                           name=f'Hidden_layer_{i+1}'))\n",
    "    Hybrid.add(Dense(1))\n",
    "    Hybrid.compile(loss=rmse, optimizer=opt, metrics=[rmse])\n",
    "    #Train the model on the entire training dataset\n",
    "    Hybrid.fit(X_train_kfold, y_train_kfold,\n",
    "                           epochs = params0['batch_size'],\n",
    "                           batch_size = params0['batch_size'],\n",
    "                           shuffle = False,\n",
    "                           # callbacks = [early_stopping,mchpt],\n",
    "                           validation_data=(X_val_kfold, y_val_kfold))\n",
    "    return Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "287bac2a-0af1-4faf-a924-4481464fb7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 120,\n",
       " 'epochs': 30,\n",
       " 'learning_rate': 4.061136676690667e-05,\n",
       " 'activation': 'relu',\n",
       " 'dropout_rate': 0.128034161380662,\n",
       " 'optimizer': 'Adam',\n",
       " 'num_units': 145,\n",
       " 'num_layers': 1,\n",
       " 'lambda1': 0.4514692014926161,\n",
       " 'lambda2': 0.261375361868317}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a24139ae-bd92-4d45-805c-dc8e8e6cc088",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "91/91 [==============================] - 8s 53ms/step - loss: 0.3919 - rmse: 0.3909 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 2/120\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.3936 - rmse: 0.3929 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 3/120\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.3950 - rmse: 0.3941 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 4/120\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.3939 - rmse: 0.3932 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 5/120\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.3937 - rmse: 0.3922 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 6/120\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.3911 - rmse: 0.3899 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 7/120\n",
      "91/91 [==============================] - 5s 50ms/step - loss: 0.3932 - rmse: 0.3919 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 8/120\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.3934 - rmse: 0.3925 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 9/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3919 - rmse: 0.3907 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 10/120\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.3943 - rmse: 0.3924 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 11/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3938 - rmse: 0.3936 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 12/120\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.3934 - rmse: 0.3918 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 13/120\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.3929 - rmse: 0.3920 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 14/120\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.3937 - rmse: 0.3931 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 15/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3933 - rmse: 0.3922 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 16/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3923 - rmse: 0.3908 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 17/120\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.3907 - rmse: 0.3896 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 18/120\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.3953 - rmse: 0.3945 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 19/120\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.3914 - rmse: 0.3897 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 20/120\n",
      "91/91 [==============================] - 5s 50ms/step - loss: 0.3937 - rmse: 0.3917 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 21/120\n",
      "91/91 [==============================] - 5s 50ms/step - loss: 0.3927 - rmse: 0.3914 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 22/120\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.3944 - rmse: 0.3926 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 23/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3929 - rmse: 0.3922 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 24/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3927 - rmse: 0.3916 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 25/120\n",
      "91/91 [==============================] - 5s 50ms/step - loss: 0.3950 - rmse: 0.3933 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 26/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3920 - rmse: 0.3911 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 27/120\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.3954 - rmse: 0.3946 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 28/120\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.3931 - rmse: 0.3920 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 29/120\n",
      "91/91 [==============================] - 5s 56ms/step - loss: 0.3939 - rmse: 0.3926 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 30/120\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.3935 - rmse: 0.3927 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 31/120\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.3943 - rmse: 0.3931 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 32/120\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.3939 - rmse: 0.3919 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 33/120\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.3948 - rmse: 0.3938 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 34/120\n",
      "91/91 [==============================] - 5s 60ms/step - loss: 0.3931 - rmse: 0.3922 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 35/120\n",
      "91/91 [==============================] - 5s 58ms/step - loss: 0.3940 - rmse: 0.3924 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 36/120\n",
      "91/91 [==============================] - 5s 50ms/step - loss: 0.3919 - rmse: 0.3900 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 37/120\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.3946 - rmse: 0.3929 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 38/120\n",
      "91/91 [==============================] - 5s 57ms/step - loss: 0.3948 - rmse: 0.3945 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 39/120\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.3927 - rmse: 0.3919 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 40/120\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.3924 - rmse: 0.3915 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 41/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3943 - rmse: 0.3933 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 42/120\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.3927 - rmse: 0.3918 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 43/120\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.3928 - rmse: 0.3913 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 44/120\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.3931 - rmse: 0.3916 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 45/120\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.3906 - rmse: 0.3896 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 46/120\n",
      "91/91 [==============================] - 5s 50ms/step - loss: 0.3921 - rmse: 0.3908 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 47/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3944 - rmse: 0.3934 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 48/120\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3934 - rmse: 0.3914 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 49/120\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.3937 - rmse: 0.3922 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 50/120\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.3922 - rmse: 0.3926 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 51/120\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.3939 - rmse: 0.3925 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 52/120\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.3928 - rmse: 0.3914 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 53/120\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.3932 - rmse: 0.3915 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 54/120\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.3938 - rmse: 0.3931 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 55/120\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.3924 - rmse: 0.3909 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 56/120\n",
      "91/91 [==============================] - 5s 50ms/step - loss: 0.3936 - rmse: 0.3934 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 57/120\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3926 - rmse: 0.3921 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 58/120\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3923 - rmse: 0.3908 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 59/120\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.3936 - rmse: 0.3932 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 60/120\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.3945 - rmse: 0.3931 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 61/120\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.3915 - rmse: 0.3909 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 62/120\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.3930 - rmse: 0.3923 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 63/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3928 - rmse: 0.3916 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 64/120\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3930 - rmse: 0.3914 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 65/120\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3937 - rmse: 0.3925 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 66/120\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3940 - rmse: 0.3925 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 67/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3924 - rmse: 0.3906 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 68/120\n",
      "91/91 [==============================] - 5s 50ms/step - loss: 0.3945 - rmse: 0.3934 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 69/120\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.3933 - rmse: 0.3919 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 70/120\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.3949 - rmse: 0.3937 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 71/120\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.3911 - rmse: 0.3896 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 72/120\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.3937 - rmse: 0.3921 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 73/120\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.3937 - rmse: 0.3928 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 74/120\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.3947 - rmse: 0.3930 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 75/120\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.3947 - rmse: 0.3935 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 76/120\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.3930 - rmse: 0.3905 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 77/120\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.3944 - rmse: 0.3929 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 78/120\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.3944 - rmse: 0.3930 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 79/120\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.3924 - rmse: 0.3928 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 80/120\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.3922 - rmse: 0.3911 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 81/120\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.3932 - rmse: 0.3921 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 82/120\n",
      "91/91 [==============================] - 5s 49ms/step - loss: 0.3944 - rmse: 0.3931 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 83/120\n",
      "91/91 [==============================] - 5s 49ms/step - loss: 0.3959 - rmse: 0.3943 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 84/120\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.3950 - rmse: 0.3934 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 85/120\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.3934 - rmse: 0.3917 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 86/120\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.3926 - rmse: 0.3917 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 87/120\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.3932 - rmse: 0.3912 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 88/120\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.3913 - rmse: 0.3909 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 89/120\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.3919 - rmse: 0.3911 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 90/120\n",
      "91/91 [==============================] - 5s 50ms/step - loss: 0.3945 - rmse: 0.3934 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 91/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3956 - rmse: 0.3941 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 92/120\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3925 - rmse: 0.3915 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 93/120\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.3944 - rmse: 0.3934 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 94/120\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3936 - rmse: 0.3933 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 95/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3934 - rmse: 0.3918 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 96/120\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.3932 - rmse: 0.3916 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 97/120\n",
      "91/91 [==============================] - 5s 50ms/step - loss: 0.3919 - rmse: 0.3905 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 98/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3928 - rmse: 0.3914 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 99/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3932 - rmse: 0.3927 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 100/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3931 - rmse: 0.3916 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 101/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3948 - rmse: 0.3938 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 102/120\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.3937 - rmse: 0.3928 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 103/120\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.3915 - rmse: 0.3902 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 104/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3938 - rmse: 0.3919 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 105/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3921 - rmse: 0.3909 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 106/120\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.3948 - rmse: 0.3933 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 107/120\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3911 - rmse: 0.3896 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 108/120\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.3935 - rmse: 0.3929 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 109/120\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.3932 - rmse: 0.3916 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 110/120\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3914 - rmse: 0.3906 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 111/120\n",
      "91/91 [==============================] - 5s 50ms/step - loss: 0.3948 - rmse: 0.3945 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 112/120\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.3911 - rmse: 0.3905 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 113/120\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.3921 - rmse: 0.3909 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 114/120\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.3936 - rmse: 0.3924 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 115/120\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.3933 - rmse: 0.3920 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 116/120\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.3940 - rmse: 0.3921 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 117/120\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3943 - rmse: 0.3939 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 118/120\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.3935 - rmse: 0.3931 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 119/120\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3918 - rmse: 0.3916 - val_loss: 0.2088 - val_rmse: 0.2052\n",
      "Epoch 120/120\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3935 - rmse: 0.3923 - val_loss: 0.2088 - val_rmse: 0.2052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22252ae7130>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 60\n",
    "epochs = 20\n",
    "learning_rate = 1e-10\n",
    "activation = selu\n",
    "dropout_rate = 0.2\n",
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "num_units = 20\n",
    "l1 = 0.45\n",
    "l2 = 0.26\n",
    "# Build the model\n",
    "Hybrid = Sequential()\n",
    "Hybrid.add(Input(shape=(X_train_kfold.shape[1],X_train_kfold.shape[2]), name='input_layer'))\n",
    "\n",
    "Hybrid.add(GRU(units=num_units,\n",
    "               activation=activation,\n",
    "               dropout=dropout_rate,\n",
    "               return_sequences=False,\n",
    "               name=f'Hidden_layer_{i+1}'))\n",
    "Hybrid.add(Dense(1))\n",
    "Hybrid.compile(loss=rmse, optimizer=opt, metrics=[rmse])\n",
    "#Train the model on the entire training dataset\n",
    "Hybrid.fit(X_train, y_train,\n",
    "                       epochs = params0['batch_size'],\n",
    "                       batch_size = params0['batch_size'],\n",
    "                       shuffle = False,\n",
    "                       # callbacks = [early_stopping,mchpt],\n",
    "                       validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01a8c553-d672-4e9d-9c5f-ee14c88aef6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>activation</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>num_units</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>lambda1</th>\n",
       "      <th>lambda2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.22292</td>\n",
       "      <td>Adam</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>0.21429</td>\n",
       "      <td>0.62071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.12803</td>\n",
       "      <td>Adam</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45147</td>\n",
       "      <td>0.26138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.12803</td>\n",
       "      <td>Adam</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45147</td>\n",
       "      <td>0.26138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.12803</td>\n",
       "      <td>Adam</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45147</td>\n",
       "      <td>0.26138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.12803</td>\n",
       "      <td>Adam</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45147</td>\n",
       "      <td>0.26138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.12803</td>\n",
       "      <td>Adam</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45147</td>\n",
       "      <td>0.26138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      batch_size  epochs  learning_rate activation  dropout_rate optimizer  \\\n",
       "fold                                                                         \n",
       "0             60      10        0.00008       selu       0.22292      Adam   \n",
       "1            120      30        0.00004       relu       0.12803      Adam   \n",
       "2            120      30        0.00004       relu       0.12803      Adam   \n",
       "3            120      30        0.00004       relu       0.12803      Adam   \n",
       "4            120      30        0.00004       relu       0.12803      Adam   \n",
       "5            120      30        0.00004       relu       0.12803      Adam   \n",
       "\n",
       "      num_units  num_layers  lambda1  lambda2  \n",
       "fold                                           \n",
       "0            97           2  0.21429  0.62071  \n",
       "1           145           1  0.45147  0.26138  \n",
       "2           145           1  0.45147  0.26138  \n",
       "3           145           1  0.45147  0.26138  \n",
       "4           145           1  0.45147  0.26138  \n",
       "5           145           1  0.45147  0.26138  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GRUparams = pd.read_csv('GRUparams1.csv', index_col = 'fold')\n",
    "GRUparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a92f46ff-bbf5-4958-9c16-9541272b6c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 2s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    X_train_kfold,X_val_kfold = [X_train]*2\n",
    "    y_train_kfold,y_val_kfold= [y_train]*2\n",
    "    params0 =pd.read_csv('GRUparams1.csv', index_col = 'fold').iloc[i,:].to_dict()\n",
    "    exec(f'G0{i+1} = GRU_foldcv(params0)')\n",
    "    clear_output(wait = True)\n",
    "    pred = eval(f'G0{i+1}.predict(X_test)')\n",
    "    r2_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cab67d07-b1ca-472a-88e6-78b8882332ff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 6\n",
      "(9005, 100, 9)\n",
      "Epoch 1/120\n",
      "76/76 [==============================] - 28s 326ms/step - loss: 0.2293 - r_squared: -85.0461 - val_loss: 0.0477 - val_r_squared: -0.7462\n",
      "Epoch 2/120\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 0.1348 - r_squared: -9.1405 - val_loss: 0.0320 - val_r_squared: -130.5803\n",
      "Epoch 3/120\n",
      "76/76 [==============================] - 22s 296ms/step - loss: 0.1136 - r_squared: -7.1474 - val_loss: 0.0357 - val_r_squared: -194.2290\n",
      "Epoch 4/120\n",
      "76/76 [==============================] - 21s 271ms/step - loss: 0.0982 - r_squared: -8.0342 - val_loss: 0.0455 - val_r_squared: -198.3140\n",
      "Epoch 5/120\n",
      "76/76 [==============================] - 21s 282ms/step - loss: 0.0900 - r_squared: -9.2134 - val_loss: 0.0529 - val_r_squared: -208.5293\n",
      "Epoch 6/120\n",
      "76/76 [==============================] - 22s 283ms/step - loss: 0.0844 - r_squared: -9.2841 - val_loss: 0.0560 - val_r_squared: -237.2890\n",
      "Epoch 7/120\n",
      "76/76 [==============================] - 22s 287ms/step - loss: 0.0808 - r_squared: -5.5418 - val_loss: 0.0583 - val_r_squared: -241.3794\n",
      "Epoch 8/120\n",
      "76/76 [==============================] - 21s 283ms/step - loss: 0.0788 - r_squared: -4.9025 - val_loss: 0.0609 - val_r_squared: -251.9311\n",
      "Epoch 9/120\n",
      "76/76 [==============================] - 21s 280ms/step - loss: 0.0761 - r_squared: -4.5881 - val_loss: 0.0615 - val_r_squared: -218.8446\n",
      "Epoch 10/120\n",
      "76/76 [==============================] - 21s 279ms/step - loss: 0.0747 - r_squared: -4.0625 - val_loss: 0.0617 - val_r_squared: -197.2279\n",
      "Epoch 11/120\n",
      "76/76 [==============================] - 21s 276ms/step - loss: 0.0730 - r_squared: -2.3257 - val_loss: 0.0624 - val_r_squared: -179.7622\n",
      "Epoch 12/120\n",
      "76/76 [==============================] - 21s 278ms/step - loss: 0.0713 - r_squared: -1.4981 - val_loss: 0.0630 - val_r_squared: -169.9412\n",
      "Epoch 13/120\n",
      "76/76 [==============================] - 22s 288ms/step - loss: 0.0704 - r_squared: -2.7375 - val_loss: 0.0634 - val_r_squared: -167.4831\n",
      "Epoch 14/120\n",
      "76/76 [==============================] - 22s 288ms/step - loss: 0.0689 - r_squared: -2.3826 - val_loss: 0.0634 - val_r_squared: -140.9944\n",
      "Epoch 15/120\n",
      "76/76 [==============================] - 22s 292ms/step - loss: 0.0682 - r_squared: -2.1681 - val_loss: 0.0638 - val_r_squared: -142.1956\n",
      "Epoch 16/120\n",
      "76/76 [==============================] - 21s 282ms/step - loss: 0.0677 - r_squared: -2.1196 - val_loss: 0.0649 - val_r_squared: -152.6403\n",
      "Epoch 17/120\n",
      "76/76 [==============================] - 21s 273ms/step - loss: 0.0675 - r_squared: -1.7738 - val_loss: 0.0642 - val_r_squared: -124.4099\n",
      "Epoch 18/120\n",
      "76/76 [==============================] - 21s 275ms/step - loss: 0.0672 - r_squared: -1.6646 - val_loss: 0.0647 - val_r_squared: -124.8603\n",
      "Epoch 19/120\n",
      "76/76 [==============================] - 21s 276ms/step - loss: 0.0657 - r_squared: -3.8465 - val_loss: 0.0644 - val_r_squared: -120.5674\n",
      "Epoch 20/120\n",
      "76/76 [==============================] - 20s 269ms/step - loss: 0.0651 - r_squared: -3.8847 - val_loss: 0.0646 - val_r_squared: -119.4179\n",
      "Epoch 21/120\n",
      "76/76 [==============================] - 20s 269ms/step - loss: 0.0641 - r_squared: -1.4622 - val_loss: 0.0641 - val_r_squared: -107.9710\n",
      "Epoch 22/120\n",
      "76/76 [==============================] - 21s 275ms/step - loss: 0.0646 - r_squared: -0.9127 - val_loss: 0.0644 - val_r_squared: -99.5999\n",
      "Epoch 23/120\n",
      "76/76 [==============================] - 21s 274ms/step - loss: 0.0640 - r_squared: -0.9686 - val_loss: 0.0637 - val_r_squared: -100.7459\n",
      "Epoch 24/120\n",
      "76/76 [==============================] - 21s 273ms/step - loss: 0.0634 - r_squared: -1.8714 - val_loss: 0.0641 - val_r_squared: -92.9786\n",
      "Epoch 25/120\n",
      "76/76 [==============================] - 21s 278ms/step - loss: 0.0633 - r_squared: -2.0299 - val_loss: 0.0638 - val_r_squared: -102.0675\n",
      "Epoch 26/120\n",
      "76/76 [==============================] - 21s 281ms/step - loss: 0.0634 - r_squared: -1.3621 - val_loss: 0.0632 - val_r_squared: -101.0153\n",
      "Epoch 27/120\n",
      "76/76 [==============================] - 21s 274ms/step - loss: 0.0624 - r_squared: -0.5689 - val_loss: 0.0626 - val_r_squared: -92.7806\n",
      "Epoch 28/120\n",
      "76/76 [==============================] - 21s 270ms/step - loss: 0.0626 - r_squared: -0.2467 - val_loss: 0.0631 - val_r_squared: -93.3136\n",
      "Epoch 29/120\n",
      "76/76 [==============================] - 20s 264ms/step - loss: 0.0626 - r_squared: -3.5307 - val_loss: 0.0631 - val_r_squared: -89.7062\n",
      "Epoch 30/120\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 0.0617 - r_squared: -0.8413 - val_loss: 0.0622 - val_r_squared: -93.9793\n",
      "Epoch 31/120\n",
      "76/76 [==============================] - 24s 310ms/step - loss: 0.0618 - r_squared: -3.4366 - val_loss: 0.0623 - val_r_squared: -90.5668\n",
      "Epoch 32/120\n",
      "76/76 [==============================] - 22s 290ms/step - loss: 0.0616 - r_squared: -2.3952 - val_loss: 0.0613 - val_r_squared: -86.9697\n",
      "Epoch 33/120\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 0.0609 - r_squared: -1.7802 - val_loss: 0.0606 - val_r_squared: -86.4351\n",
      "Epoch 34/120\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 0.0614 - r_squared: -1.7091 - val_loss: 0.0610 - val_r_squared: -97.2577\n",
      "Epoch 35/120\n",
      "76/76 [==============================] - 22s 291ms/step - loss: 0.0604 - r_squared: -2.2903 - val_loss: 0.0605 - val_r_squared: -93.4736\n",
      "Epoch 36/120\n",
      "76/76 [==============================] - 21s 283ms/step - loss: 0.0603 - r_squared: -2.0985 - val_loss: 0.0603 - val_r_squared: -91.4509\n",
      "Epoch 37/120\n",
      "76/76 [==============================] - 21s 278ms/step - loss: 0.0609 - r_squared: -1.6308 - val_loss: 0.0605 - val_r_squared: -95.1897\n",
      "Epoch 38/120\n",
      "76/76 [==============================] - 21s 274ms/step - loss: 0.0604 - r_squared: -0.4508 - val_loss: 0.0603 - val_r_squared: -87.9460\n",
      "Epoch 39/120\n",
      "76/76 [==============================] - 21s 279ms/step - loss: 0.0604 - r_squared: -1.1319 - val_loss: 0.0596 - val_r_squared: -96.5502\n",
      "Epoch 40/120\n",
      "76/76 [==============================] - 22s 284ms/step - loss: 0.0599 - r_squared: -1.8922 - val_loss: 0.0595 - val_r_squared: -89.8075\n",
      "Epoch 41/120\n",
      "76/76 [==============================] - 21s 276ms/step - loss: 0.0590 - r_squared: -5.3478 - val_loss: 0.0596 - val_r_squared: -93.1256\n",
      "Epoch 42/120\n",
      "76/76 [==============================] - 21s 276ms/step - loss: 0.0596 - r_squared: -0.6426 - val_loss: 0.0593 - val_r_squared: -97.8149\n",
      "Epoch 43/120\n",
      "76/76 [==============================] - 21s 276ms/step - loss: 0.0596 - r_squared: -0.8558 - val_loss: 0.0599 - val_r_squared: -99.2373\n",
      "Epoch 44/120\n",
      "76/76 [==============================] - 21s 276ms/step - loss: 0.0591 - r_squared: -0.5458 - val_loss: 0.0586 - val_r_squared: -93.6402\n",
      "Epoch 45/120\n",
      "76/76 [==============================] - 21s 275ms/step - loss: 0.0590 - r_squared: -1.4516 - val_loss: 0.0595 - val_r_squared: -97.1986\n",
      "Epoch 46/120\n",
      "76/76 [==============================] - 21s 277ms/step - loss: 0.0590 - r_squared: -0.5490 - val_loss: 0.0593 - val_r_squared: -93.0977\n",
      "Epoch 47/120\n",
      "76/76 [==============================] - 20s 268ms/step - loss: 0.0592 - r_squared: -1.2654 - val_loss: 0.0583 - val_r_squared: -86.7273\n",
      "Epoch 48/120\n",
      "76/76 [==============================] - 20s 260ms/step - loss: 0.0587 - r_squared: -0.3656 - val_loss: 0.0580 - val_r_squared: -91.3435\n",
      "Epoch 49/120\n",
      "76/76 [==============================] - 20s 258ms/step - loss: 0.0586 - r_squared: -1.3740 - val_loss: 0.0586 - val_r_squared: -101.0806\n",
      "Epoch 50/120\n",
      "76/76 [==============================] - 20s 267ms/step - loss: 0.0581 - r_squared: -0.3460 - val_loss: 0.0586 - val_r_squared: -102.5551\n",
      "Epoch 51/120\n",
      "76/76 [==============================] - 20s 258ms/step - loss: 0.0582 - r_squared: -1.9490 - val_loss: 0.0574 - val_r_squared: -91.8952\n",
      "Epoch 52/120\n",
      "76/76 [==============================] - 20s 262ms/step - loss: 0.0585 - r_squared: -0.9457 - val_loss: 0.0587 - val_r_squared: -105.9625\n",
      "Epoch 53/120\n",
      "76/76 [==============================] - 20s 265ms/step - loss: 0.0581 - r_squared: -0.4107 - val_loss: 0.0584 - val_r_squared: -100.6261\n",
      "Epoch 54/120\n",
      "76/76 [==============================] - 20s 262ms/step - loss: 0.0582 - r_squared: -1.0418 - val_loss: 0.0587 - val_r_squared: -97.6565\n",
      "Epoch 55/120\n",
      "76/76 [==============================] - 20s 258ms/step - loss: 0.0580 - r_squared: -0.3141 - val_loss: 0.0580 - val_r_squared: -98.4395\n",
      "Epoch 56/120\n",
      "76/76 [==============================] - 20s 261ms/step - loss: 0.0579 - r_squared: -1.2001 - val_loss: 0.0579 - val_r_squared: -94.8485\n",
      "Epoch 57/120\n",
      "76/76 [==============================] - 20s 262ms/step - loss: 0.0574 - r_squared: -1.1090 - val_loss: 0.0580 - val_r_squared: -96.0669\n",
      "Epoch 58/120\n",
      "76/76 [==============================] - 20s 264ms/step - loss: 0.0582 - r_squared: -1.3636 - val_loss: 0.0580 - val_r_squared: -107.1344\n",
      "Epoch 59/120\n",
      "76/76 [==============================] - 20s 261ms/step - loss: 0.0575 - r_squared: -0.2967 - val_loss: 0.0580 - val_r_squared: -94.1776\n",
      "Epoch 60/120\n",
      "76/76 [==============================] - 20s 260ms/step - loss: 0.0573 - r_squared: -0.1452 - val_loss: 0.0587 - val_r_squared: -90.3059\n",
      "Epoch 61/120\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 0.0574 - r_squared: 0.0055 - val_loss: 0.0586 - val_r_squared: -102.3561\n",
      "Epoch 62/120\n",
      "76/76 [==============================] - 25s 330ms/step - loss: 0.0568 - r_squared: -0.7753 - val_loss: 0.0581 - val_r_squared: -95.1482\n",
      "Epoch 63/120\n",
      "76/76 [==============================] - 22s 286ms/step - loss: 0.0573 - r_squared: -0.3788 - val_loss: 0.0586 - val_r_squared: -101.1953\n",
      "Epoch 64/120\n",
      "76/76 [==============================] - 16s 210ms/step - loss: 0.0573 - r_squared: -0.1516 - val_loss: 0.0581 - val_r_squared: -102.6746\n",
      "Epoch 65/120\n",
      "76/76 [==============================] - 16s 207ms/step - loss: 0.0567 - r_squared: -0.3730 - val_loss: 0.0577 - val_r_squared: -105.4130\n",
      "Epoch 66/120\n",
      "76/76 [==============================] - 16s 214ms/step - loss: 0.0568 - r_squared: -0.1540 - val_loss: 0.0585 - val_r_squared: -101.7635\n",
      "Epoch 67/120\n",
      "76/76 [==============================] - 16s 208ms/step - loss: 0.0569 - r_squared: -1.3320 - val_loss: 0.0590 - val_r_squared: -106.1296\n",
      "Epoch 68/120\n",
      "76/76 [==============================] - 16s 207ms/step - loss: 0.0570 - r_squared: -0.7315 - val_loss: 0.0584 - val_r_squared: -103.6085\n",
      "Epoch 69/120\n",
      "76/76 [==============================] - 16s 206ms/step - loss: 0.0566 - r_squared: -0.7958 - val_loss: 0.0584 - val_r_squared: -117.6517\n",
      "Epoch 70/120\n",
      "56/76 [=====================>........] - ETA: 3s - loss: 0.0639 - r_squared: 0.3285"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Best model for th k-fold\u001b[39;00m\n\u001b[0;32m     16\u001b[0m params0 \u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGRUparams1.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39miloc[i,:]\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m---> 17\u001b[0m \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mG\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m = GRU_foldcv(params0)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m clear_output(wait \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m fold_no \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m<string>:1\u001b[0m\n",
      "Cell \u001b[1;32mIn[13], line 57\u001b[0m, in \u001b[0;36mGRU_foldcv\u001b[1;34m(params0)\u001b[0m\n\u001b[0;32m     55\u001b[0m Hybrid\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mrmse, optimizer\u001b[38;5;241m=\u001b[39mopt, metrics\u001b[38;5;241m=\u001b[39m[r_squared])\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m#Train the model on the entire training dataset\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[43mHybrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_kfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_kfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams0\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams0\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;66;43;03m# callbacks = [early_stopping,mchpt],\u001b[39;49;00m\n\u001b[0;32m     62\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_kfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_kfold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Hybrid\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from skopt.callbacks import DeltaYStopper\n",
    "r2_per_fold = []\n",
    "loss_per_fold = []\n",
    "rmse_per_fold = []\n",
    "gru_best_score = []\n",
    "\n",
    "kfold = KFold(n_splits=6)\n",
    "fold_no = 1\n",
    "parameters_fold = [None]*6\n",
    "for i, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    print(i,fold_no)\n",
    "    X_train_kfold, X_val_kfold = X_train[train_index], X_train[val_index]\n",
    "    y_train_kfold, y_val_kfold = y_train[train_index], y_train[val_index]\n",
    "    print(X_train_kfold.shape)\n",
    "    # Best model for th k-fold\n",
    "    params0 =pd.read_csv('GRUparams1.csv', index_col = 'fold').iloc[i,:].to_dict()\n",
    "    exec(f'G{i+1} = GRU_foldcv(params0)')\n",
    "    clear_output(wait = True)\n",
    "    fold_no +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "672d5820-b85f-474a-b97a-9b4d0314316a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 2s 31ms/step\n",
      "69/69 [==============================] - 1s 20ms/step\n",
      "407/407 [==============================] - 13s 31ms/step\n",
      "407/407 [==============================] - 8s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "weights_range = np.arange(0.05, 1.1, 0.18)\n",
    "y_true = y_test\n",
    "def concatenate_data(X_train, y_train, X_test, y_test):\n",
    "    X = np.concatenate([X_train, X_test], axis=0)\n",
    "    y = np.concatenate([y_train, y_test], axis=0)\n",
    "    return X, y\n",
    "def Unreshape_df(Xs, ys=None):\n",
    "    X = np.concatenate(Xs, axis=0)\n",
    "    if ys is not None:\n",
    "        y = np.concatenate(ys, axis=0)\n",
    "        return X, y\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "        return np.sqrt(np.mean(np.square(y_pred - y_true)))  \n",
    "def calculate_weighted_average(predictions, weights):\n",
    "    return sum([weight*preds for preds, weight in zip(predictions, weights)])\n",
    "\n",
    "def evaluate_ensemble(predictions, weights, y_true):\n",
    "    pred = calculate_weighted_average(predictions, weights)\n",
    "    loss = rmse(y_true,pred)\n",
    "    return r2_score(y_true, pred),loss\n",
    "\n",
    "#for i in range(2):\n",
    " #(f'GRU{i+1}')#\n",
    "best_models = [G01,G02]\n",
    "predictions = []\n",
    "losses = []\n",
    "for model in best_models:\n",
    "    y_pred = model.predict(X_test)\n",
    "    loss = rmse(y_true,y_pred)\n",
    "    losses.append(loss)\n",
    "    predictions.append(y_pred)\n",
    "evaluate_ensemble(predictions, [0.6,0.4], y_true)\n",
    "\n",
    "def Ensemble(X_topredict,best_weights,best_models):\n",
    "    model_predictions = []\n",
    "    for model in best_models:\n",
    "        y_pred = model.predict(X_topredict).T[0]\n",
    "        model_predictions.append(y_pred)\n",
    "\n",
    "    # Combine the model predictions using the optimized weights to obtain the final prediction\n",
    "    final_prediction = calculate_weighted_average(model_predictions, best_weights)\n",
    "    return final_prediction\n",
    "RNNe = Ensemble(X,[0.6,0.4],[G01,G02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cb564bb7-23aa-4c35-8b38-7b63de7df29c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v = scaler.inverse_transform(RNNe.reshape(-1, 1)).T[0]\n",
    "RNNdf = pd.DataFrame().assign(RNNe = v,Time = plgm.iloc[100:-1,-3].values)\n",
    "RNNdf.to_csv('RNNpred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e672a8-81e4-4edb-9def-3e8ec8449767",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [list(parameters_fold[0].keys())[0]] + list(parameters_fold[0][list(parameters_fold[0].keys())[1]].keys()) \n",
    "GRUparams = pd.DataFrame(columns = columns)\n",
    "GRUparams.set_index('fold',inplace = True)\n",
    "for ix in range(6):\n",
    "    GRUparams.loc[ix] = ix+1\n",
    "    for i in list(parameters_fold[ix][list(parameters_fold[0].keys())[1]].keys()):\n",
    "        GRUparams.loc[ix,i] = parameters_fold[ix][list(parameters_fold[0])[1]][i]\n",
    "GRUparams#.to_csv('GRUparams1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "103cce27-0bf6-4618-b1cb-788a6fc93d1f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting1\n",
      "Epoch 1/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.3956 - r_squared: -20.2255INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 52s 652ms/step - loss: 0.3956 - r_squared: -20.2255 - val_loss: 0.3381 - val_r_squared: -625665.0625\n",
      "Epoch 2/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.3040 - r_squared: -6.8599INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 47s 618ms/step - loss: 0.3040 - r_squared: -6.8599 - val_loss: 0.2844 - val_r_squared: -475649.1562\n",
      "Epoch 3/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2662 - r_squared: -3.6612INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 51s 673ms/step - loss: 0.2662 - r_squared: -3.6612 - val_loss: 0.2469 - val_r_squared: -353938.5000\n",
      "Epoch 4/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2263 - r_squared: -2.7473INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 55s 721ms/step - loss: 0.2263 - r_squared: -2.7473 - val_loss: 0.2049 - val_r_squared: -232021.5312\n",
      "Epoch 5/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1814 - r_squared: -1.1320INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 55s 718ms/step - loss: 0.1814 - r_squared: -1.1320 - val_loss: 0.1565 - val_r_squared: -112345.8047\n",
      "Epoch 6/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1360 - r_squared: -1.7047INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 55s 720ms/step - loss: 0.1360 - r_squared: -1.7047 - val_loss: 0.1124 - val_r_squared: -32497.3535\n",
      "Epoch 7/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1085 - r_squared: -0.1934INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 55s 718ms/step - loss: 0.1085 - r_squared: -0.1934 - val_loss: 0.0901 - val_r_squared: -9246.8477\n",
      "Epoch 8/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0993 - r_squared: -0.2681INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 55s 726ms/step - loss: 0.0993 - r_squared: -0.2681 - val_loss: 0.0817 - val_r_squared: -4378.3999\n",
      "Epoch 9/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0962 - r_squared: -0.6016INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 62s 815ms/step - loss: 0.0962 - r_squared: -0.6016 - val_loss: 0.0779 - val_r_squared: -2856.1887\n",
      "Epoch 10/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0930 - r_squared: 0.2942INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 65s 856ms/step - loss: 0.0930 - r_squared: 0.2942 - val_loss: 0.0753 - val_r_squared: -1685.7727\n",
      "Epoch 11/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0912 - r_squared: 0.4003INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 57s 753ms/step - loss: 0.0912 - r_squared: 0.4003 - val_loss: 0.0731 - val_r_squared: -689.7651\n",
      "Epoch 12/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0889 - r_squared: 0.1692INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 51s 668ms/step - loss: 0.0889 - r_squared: 0.1692 - val_loss: 0.0715 - val_r_squared: -226.9647\n",
      "Epoch 13/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0887 - r_squared: 0.3300INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 39s 518ms/step - loss: 0.0887 - r_squared: 0.3300 - val_loss: 0.0705 - val_r_squared: -67.7517\n",
      "Epoch 14/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0878 - r_squared: 0.4256INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 52s 680ms/step - loss: 0.0878 - r_squared: 0.4256 - val_loss: 0.0697 - val_r_squared: -0.7039\n",
      "Epoch 15/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0871 - r_squared: 0.4052INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 51s 678ms/step - loss: 0.0871 - r_squared: 0.4052 - val_loss: 0.0690 - val_r_squared: -40.1836\n",
      "Epoch 16/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0857 - r_squared: 0.5206INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 51s 677ms/step - loss: 0.0857 - r_squared: 0.5206 - val_loss: 0.0681 - val_r_squared: -105.9664\n",
      "Epoch 17/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0857 - r_squared: 0.3607INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 52s 685ms/step - loss: 0.0857 - r_squared: 0.3607 - val_loss: 0.0677 - val_r_squared: -156.1475\n",
      "Epoch 18/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0848 - r_squared: 0.4178INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 55s 722ms/step - loss: 0.0848 - r_squared: 0.4178 - val_loss: 0.0671 - val_r_squared: -522.6163\n",
      "Epoch 19/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0837 - r_squared: -0.1091INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 49s 640ms/step - loss: 0.0837 - r_squared: -0.1091 - val_loss: 0.0667 - val_r_squared: -580.7151\n",
      "Epoch 20/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0831 - r_squared: 0.2406INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 43s 567ms/step - loss: 0.0831 - r_squared: 0.2406 - val_loss: 0.0663 - val_r_squared: -533.6748\n",
      "Epoch 21/120\n",
      "76/76 [==============================] - 30s 401ms/step - loss: 0.0840 - r_squared: 0.5524 - val_loss: 0.0665 - val_r_squared: -562.7433\n",
      "Epoch 22/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0825 - r_squared: 0.5441INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 33s 434ms/step - loss: 0.0825 - r_squared: 0.5441 - val_loss: 0.0660 - val_r_squared: -664.7033\n",
      "Epoch 23/120\n",
      "76/76 [==============================] - 30s 397ms/step - loss: 0.0825 - r_squared: -1.5660 - val_loss: 0.0667 - val_r_squared: -987.3991\n",
      "Epoch 24/120\n",
      "76/76 [==============================] - 34s 447ms/step - loss: 0.0817 - r_squared: -1.2469 - val_loss: 0.0662 - val_r_squared: -967.4443\n",
      "Epoch 25/120\n",
      "76/76 [==============================] - 36s 477ms/step - loss: 0.0801 - r_squared: 0.2985 - val_loss: 0.0663 - val_r_squared: -958.9290\n",
      "Epoch 26/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0802 - r_squared: 0.4262INFO:tensorflow:Assets written to: GRU\\gru_new0_fold1_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 494ms/step - loss: 0.0802 - r_squared: 0.4262 - val_loss: 0.0654 - val_r_squared: -967.9919\n",
      "Epoch 27/120\n",
      "76/76 [==============================] - 34s 447ms/step - loss: 0.0788 - r_squared: 0.1347 - val_loss: 0.0661 - val_r_squared: -1105.9043\n",
      "Epoch 28/120\n",
      "76/76 [==============================] - 35s 460ms/step - loss: 0.0778 - r_squared: -0.4345 - val_loss: 0.0659 - val_r_squared: -1238.1458\n",
      "Epoch 29/120\n",
      "76/76 [==============================] - 35s 462ms/step - loss: 0.0788 - r_squared: 0.3055 - val_loss: 0.0664 - val_r_squared: -1065.9454\n",
      "Epoch 30/120\n",
      "76/76 [==============================] - 34s 448ms/step - loss: 0.0780 - r_squared: 0.5607 - val_loss: 0.0655 - val_r_squared: -1074.3286\n",
      "Epoch 31/120\n",
      "76/76 [==============================] - 34s 446ms/step - loss: 0.0775 - r_squared: 0.3150 - val_loss: 0.0658 - val_r_squared: -1142.2952\n",
      "fitting2\n",
      "Epoch 1/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.3150 - r_squared: -3.8224INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 37s 471ms/step - loss: 0.3150 - r_squared: -3.8224 - val_loss: 0.2513 - val_r_squared: -144830.5625\n",
      "Epoch 2/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1575 - r_squared: -1.5202INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 466ms/step - loss: 0.1575 - r_squared: -1.5202 - val_loss: 0.1047 - val_r_squared: -34.1729\n",
      "Epoch 3/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0973 - r_squared: -0.4753INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 470ms/step - loss: 0.0973 - r_squared: -0.4753 - val_loss: 0.0940 - val_r_squared: -460.0524\n",
      "Epoch 4/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0929 - r_squared: -0.1734INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 32s 425ms/step - loss: 0.0929 - r_squared: -0.1734 - val_loss: 0.0888 - val_r_squared: -693.2324\n",
      "Epoch 5/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0913 - r_squared: 0.5352INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 33s 437ms/step - loss: 0.0913 - r_squared: 0.5352 - val_loss: 0.0856 - val_r_squared: -596.1112\n",
      "Epoch 6/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0888 - r_squared: 0.5959INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 42s 552ms/step - loss: 0.0888 - r_squared: 0.5959 - val_loss: 0.0821 - val_r_squared: -736.4976\n",
      "Epoch 7/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0875 - r_squared: 0.0816INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 42s 552ms/step - loss: 0.0875 - r_squared: 0.0816 - val_loss: 0.0805 - val_r_squared: -551.0381\n",
      "Epoch 8/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0856 - r_squared: 0.6258INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 42s 558ms/step - loss: 0.0856 - r_squared: 0.6258 - val_loss: 0.0780 - val_r_squared: -641.8085\n",
      "Epoch 9/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0846 - r_squared: 0.4568INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 42s 550ms/step - loss: 0.0846 - r_squared: 0.4568 - val_loss: 0.0770 - val_r_squared: -516.1205\n",
      "Epoch 10/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0834 - r_squared: 0.5582INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 42s 555ms/step - loss: 0.0834 - r_squared: 0.5582 - val_loss: 0.0760 - val_r_squared: -427.5800\n",
      "Epoch 11/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0826 - r_squared: 0.6263INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 44s 578ms/step - loss: 0.0826 - r_squared: 0.6263 - val_loss: 0.0739 - val_r_squared: -617.0908\n",
      "Epoch 12/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0820 - r_squared: 0.0429INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 54s 707ms/step - loss: 0.0820 - r_squared: 0.0429 - val_loss: 0.0733 - val_r_squared: -401.9782\n",
      "Epoch 13/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0810 - r_squared: 0.5393INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 49s 647ms/step - loss: 0.0810 - r_squared: 0.5393 - val_loss: 0.0720 - val_r_squared: -474.7002\n",
      "Epoch 14/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0806 - r_squared: 0.0594INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 46s 612ms/step - loss: 0.0806 - r_squared: 0.0594 - val_loss: 0.0712 - val_r_squared: -407.6983\n",
      "Epoch 15/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0793 - r_squared: 0.2357INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 47s 616ms/step - loss: 0.0793 - r_squared: 0.2357 - val_loss: 0.0707 - val_r_squared: -305.5791\n",
      "Epoch 16/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0782 - r_squared: 0.2188INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 504ms/step - loss: 0.0782 - r_squared: 0.2188 - val_loss: 0.0695 - val_r_squared: -376.6484\n",
      "Epoch 17/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0777 - r_squared: 0.4070INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 506ms/step - loss: 0.0777 - r_squared: 0.4070 - val_loss: 0.0694 - val_r_squared: -247.5932\n",
      "Epoch 18/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0774 - r_squared: -0.0568INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 465ms/step - loss: 0.0774 - r_squared: -0.0568 - val_loss: 0.0690 - val_r_squared: -153.2286\n",
      "Epoch 19/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0765 - r_squared: -0.8491INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 33s 436ms/step - loss: 0.0765 - r_squared: -0.8491 - val_loss: 0.0688 - val_r_squared: -115.7118\n",
      "Epoch 20/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0776 - r_squared: 0.6112INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 31s 406ms/step - loss: 0.0776 - r_squared: 0.6112 - val_loss: 0.0683 - val_r_squared: -120.2470\n",
      "Epoch 21/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0754 - r_squared: 0.5796INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 31s 411ms/step - loss: 0.0754 - r_squared: 0.5796 - val_loss: 0.0673 - val_r_squared: -201.8481\n",
      "Epoch 22/120\n",
      "76/76 [==============================] - 29s 383ms/step - loss: 0.0744 - r_squared: 0.5786 - val_loss: 0.0673 - val_r_squared: -79.3249\n",
      "Epoch 23/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0758 - r_squared: 0.4112INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 31s 414ms/step - loss: 0.0758 - r_squared: 0.4112 - val_loss: 0.0664 - val_r_squared: -175.5568\n",
      "Epoch 24/120\n",
      "76/76 [==============================] - 29s 382ms/step - loss: 0.0754 - r_squared: 0.4600 - val_loss: 0.0667 - val_r_squared: -60.0753\n",
      "Epoch 25/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0742 - r_squared: -2.4786INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 31s 415ms/step - loss: 0.0742 - r_squared: -2.4786 - val_loss: 0.0660 - val_r_squared: -97.3812\n",
      "Epoch 26/120\n",
      "76/76 [==============================] - 29s 385ms/step - loss: 0.0733 - r_squared: -1.1255 - val_loss: 0.0660 - val_r_squared: -85.8716\n",
      "Epoch 27/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0724 - r_squared: -0.2054INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 31s 414ms/step - loss: 0.0724 - r_squared: -0.2054 - val_loss: 0.0657 - val_r_squared: -60.9131\n",
      "Epoch 28/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0717 - r_squared: 0.2989INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 32s 421ms/step - loss: 0.0717 - r_squared: 0.2989 - val_loss: 0.0654 - val_r_squared: -59.7548\n",
      "Epoch 29/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0732 - r_squared: 0.6220INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 31s 412ms/step - loss: 0.0732 - r_squared: 0.6220 - val_loss: 0.0651 - val_r_squared: -22.5849\n",
      "Epoch 30/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0719 - r_squared: 0.5596INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 31s 415ms/step - loss: 0.0719 - r_squared: 0.5596 - val_loss: 0.0646 - val_r_squared: -18.4341\n",
      "Epoch 31/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0716 - r_squared: 0.6438INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 480ms/step - loss: 0.0716 - r_squared: 0.6438 - val_loss: 0.0640 - val_r_squared: -34.7377\n",
      "Epoch 32/120\n",
      "76/76 [==============================] - 36s 473ms/step - loss: 0.0709 - r_squared: 0.1320 - val_loss: 0.0640 - val_r_squared: -1.7532\n",
      "Epoch 33/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0702 - r_squared: 0.1362INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 37s 481ms/step - loss: 0.0702 - r_squared: 0.1362 - val_loss: 0.0633 - val_r_squared: -14.3273\n",
      "Epoch 34/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0692 - r_squared: 0.2621INFO:tensorflow:Assets written to: GRU\\gru_new0_fold2_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 466ms/step - loss: 0.0692 - r_squared: 0.2621 - val_loss: 0.0629 - val_r_squared: -33.7177\n",
      "Epoch 35/120\n",
      "76/76 [==============================] - 35s 462ms/step - loss: 0.0694 - r_squared: 0.6552 - val_loss: 0.0631 - val_r_squared: -4.2800\n",
      "Epoch 36/120\n",
      "76/76 [==============================] - 35s 464ms/step - loss: 0.0689 - r_squared: 0.5240 - val_loss: 0.0635 - val_r_squared: 0.2538\n",
      "Epoch 37/120\n",
      "76/76 [==============================] - 34s 443ms/step - loss: 0.0687 - r_squared: 0.2315 - val_loss: 0.0633 - val_r_squared: 0.3194\n",
      "Epoch 38/120\n",
      "76/76 [==============================] - 33s 435ms/step - loss: 0.0686 - r_squared: 0.3679 - val_loss: 0.0636 - val_r_squared: -9.7948\n",
      "Epoch 39/120\n",
      "76/76 [==============================] - 34s 443ms/step - loss: 0.0674 - r_squared: 0.5364 - val_loss: 0.0634 - val_r_squared: -54.0721\n",
      "fitting3\n",
      "Epoch 1/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2633 - r_squared: -3.2329INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 476ms/step - loss: 0.2633 - r_squared: -3.2329 - val_loss: 0.1728 - val_r_squared: -58744.3242\n",
      "Epoch 2/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1244 - r_squared: -0.1881INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 469ms/step - loss: 0.1244 - r_squared: -0.1881 - val_loss: 0.0837 - val_r_squared: -185.2128\n",
      "Epoch 3/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0962 - r_squared: 0.2219INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 470ms/step - loss: 0.0962 - r_squared: 0.2219 - val_loss: 0.0812 - val_r_squared: 0.3449\n",
      "Epoch 4/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0914 - r_squared: 0.2352INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 37s 483ms/step - loss: 0.0914 - r_squared: 0.2352 - val_loss: 0.0799 - val_r_squared: -64.8904\n",
      "Epoch 5/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0885 - r_squared: 0.4667INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 34s 451ms/step - loss: 0.0885 - r_squared: 0.4667 - val_loss: 0.0789 - val_r_squared: -216.7253\n",
      "Epoch 6/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0854 - r_squared: 0.4659INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 34s 448ms/step - loss: 0.0854 - r_squared: 0.4659 - val_loss: 0.0777 - val_r_squared: -197.8899\n",
      "Epoch 7/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0833 - r_squared: 0.3042INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 34s 450ms/step - loss: 0.0833 - r_squared: 0.3042 - val_loss: 0.0768 - val_r_squared: -482.5157\n",
      "Epoch 8/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0824 - r_squared: 0.4174INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 34s 449ms/step - loss: 0.0824 - r_squared: 0.4174 - val_loss: 0.0758 - val_r_squared: -642.5131\n",
      "Epoch 9/120\n",
      "76/76 [==============================] - 31s 411ms/step - loss: 0.0822 - r_squared: 0.4411 - val_loss: 0.0759 - val_r_squared: -1055.3812\n",
      "Epoch 10/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0818 - r_squared: 0.3706INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 34s 455ms/step - loss: 0.0818 - r_squared: 0.3706 - val_loss: 0.0744 - val_r_squared: -824.5239\n",
      "Epoch 11/120\n",
      "76/76 [==============================] - 32s 428ms/step - loss: 0.0819 - r_squared: 0.5309 - val_loss: 0.0749 - val_r_squared: -1467.2922\n",
      "Epoch 12/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0804 - r_squared: 0.5625INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 41s 545ms/step - loss: 0.0804 - r_squared: 0.5625 - val_loss: 0.0739 - val_r_squared: -1453.6527\n",
      "Epoch 13/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0800 - r_squared: 0.6044INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 43s 562ms/step - loss: 0.0800 - r_squared: 0.6044 - val_loss: 0.0735 - val_r_squared: -1565.7396\n",
      "Epoch 14/120\n",
      "76/76 [==============================] - 39s 519ms/step - loss: 0.0792 - r_squared: 0.4048 - val_loss: 0.0739 - val_r_squared: -2111.3386\n",
      "Epoch 15/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0776 - r_squared: 0.3770INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 42s 555ms/step - loss: 0.0776 - r_squared: 0.3770 - val_loss: 0.0729 - val_r_squared: -1933.2172\n",
      "Epoch 16/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0782 - r_squared: 0.5962INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0782 - r_squared: 0.5962 - val_loss: 0.0721 - val_r_squared: -1935.8680\n",
      "Epoch 17/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0775 - r_squared: 0.4394INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 37s 486ms/step - loss: 0.0775 - r_squared: 0.4394 - val_loss: 0.0719 - val_r_squared: -2060.2568\n",
      "Epoch 18/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0760 - r_squared: 0.6553INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 34s 448ms/step - loss: 0.0760 - r_squared: 0.6553 - val_loss: 0.0713 - val_r_squared: -2130.7266\n",
      "Epoch 19/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0765 - r_squared: 0.5391INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 32s 423ms/step - loss: 0.0765 - r_squared: 0.5391 - val_loss: 0.0703 - val_r_squared: -1948.5470\n",
      "Epoch 20/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0767 - r_squared: 0.4687INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 32s 418ms/step - loss: 0.0767 - r_squared: 0.4687 - val_loss: 0.0701 - val_r_squared: -2233.9429\n",
      "Epoch 21/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0755 - r_squared: 0.4574INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 32s 420ms/step - loss: 0.0755 - r_squared: 0.4574 - val_loss: 0.0693 - val_r_squared: -2193.2473\n",
      "Epoch 22/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0748 - r_squared: 0.0821INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 32s 423ms/step - loss: 0.0748 - r_squared: 0.0821 - val_loss: 0.0689 - val_r_squared: -2319.5479\n",
      "Epoch 23/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0741 - r_squared: 0.2993INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 32s 419ms/step - loss: 0.0741 - r_squared: 0.2993 - val_loss: 0.0684 - val_r_squared: -2449.2937\n",
      "Epoch 24/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0738 - r_squared: 0.5266INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 31s 408ms/step - loss: 0.0738 - r_squared: 0.5266 - val_loss: 0.0674 - val_r_squared: -2144.1907\n",
      "Epoch 25/120\n",
      "76/76 [==============================] - 30s 396ms/step - loss: 0.0747 - r_squared: 0.6094 - val_loss: 0.0674 - val_r_squared: -2386.6633\n",
      "Epoch 26/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0735 - r_squared: 0.6073INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 33s 430ms/step - loss: 0.0735 - r_squared: 0.6073 - val_loss: 0.0672 - val_r_squared: -2304.6179\n",
      "Epoch 27/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0734 - r_squared: 0.1744INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 33s 432ms/step - loss: 0.0734 - r_squared: 0.1744 - val_loss: 0.0668 - val_r_squared: -2753.2258\n",
      "Epoch 28/120\n",
      "76/76 [==============================] - 30s 397ms/step - loss: 0.0737 - r_squared: 0.3816 - val_loss: 0.0671 - val_r_squared: -3065.0925\n",
      "Epoch 29/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0707 - r_squared: 0.6007INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 33s 438ms/step - loss: 0.0707 - r_squared: 0.6007 - val_loss: 0.0649 - val_r_squared: -2368.9604\n",
      "Epoch 30/120\n",
      "76/76 [==============================] - 30s 396ms/step - loss: 0.0710 - r_squared: 0.2285 - val_loss: 0.0653 - val_r_squared: -2707.6060\n",
      "Epoch 31/120\n",
      "76/76 [==============================] - 30s 399ms/step - loss: 0.0711 - r_squared: 0.6541 - val_loss: 0.0655 - val_r_squared: -2981.8674\n",
      "Epoch 32/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0711 - r_squared: 0.6714INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 33s 430ms/step - loss: 0.0711 - r_squared: 0.6714 - val_loss: 0.0645 - val_r_squared: -2803.9265\n",
      "Epoch 33/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0717 - r_squared: 0.6606INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 39s 518ms/step - loss: 0.0717 - r_squared: 0.6606 - val_loss: 0.0644 - val_r_squared: -2971.1326\n",
      "Epoch 34/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0718 - r_squared: 0.0332INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0718 - r_squared: 0.0332 - val_loss: 0.0642 - val_r_squared: -3050.0708\n",
      "Epoch 35/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0700 - r_squared: 0.5223INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 469ms/step - loss: 0.0700 - r_squared: 0.5223 - val_loss: 0.0639 - val_r_squared: -3085.9202\n",
      "Epoch 36/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0696 - r_squared: 0.6412INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 464ms/step - loss: 0.0696 - r_squared: 0.6412 - val_loss: 0.0632 - val_r_squared: -2958.0505\n",
      "Epoch 37/120\n",
      "76/76 [==============================] - 32s 427ms/step - loss: 0.0707 - r_squared: 0.6042 - val_loss: 0.0636 - val_r_squared: -3334.7554\n",
      "Epoch 38/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0695 - r_squared: 0.4036INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 463ms/step - loss: 0.0695 - r_squared: 0.4036 - val_loss: 0.0623 - val_r_squared: -2820.0188\n",
      "Epoch 39/120\n",
      "76/76 [==============================] - 31s 407ms/step - loss: 0.0696 - r_squared: 0.2426 - val_loss: 0.0629 - val_r_squared: -3310.5735\n",
      "Epoch 40/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0687 - r_squared: 0.2413INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 33s 438ms/step - loss: 0.0687 - r_squared: 0.2413 - val_loss: 0.0619 - val_r_squared: -2845.6270\n",
      "Epoch 41/120\n",
      "76/76 [==============================] - 30s 402ms/step - loss: 0.0694 - r_squared: 0.0219 - val_loss: 0.0622 - val_r_squared: -3188.9363\n",
      "Epoch 42/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0692 - r_squared: 0.5232INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 33s 436ms/step - loss: 0.0692 - r_squared: 0.5232 - val_loss: 0.0616 - val_r_squared: -2990.9602\n",
      "Epoch 43/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0680 - r_squared: 0.0587INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 33s 435ms/step - loss: 0.0680 - r_squared: 0.0587 - val_loss: 0.0615 - val_r_squared: -3063.7402\n",
      "Epoch 44/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0683 - r_squared: -0.7942INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 34s 445ms/step - loss: 0.0683 - r_squared: -0.7942 - val_loss: 0.0614 - val_r_squared: -3135.1145\n",
      "Epoch 45/120\n",
      "76/76 [==============================] - 31s 403ms/step - loss: 0.0680 - r_squared: 0.3671 - val_loss: 0.0618 - val_r_squared: -3527.9348\n",
      "Epoch 46/120\n",
      "76/76 [==============================] - 31s 406ms/step - loss: 0.0689 - r_squared: -0.1737 - val_loss: 0.0614 - val_r_squared: -3365.5720\n",
      "Epoch 47/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0654 - r_squared: 0.5246INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 33s 432ms/step - loss: 0.0654 - r_squared: 0.5246 - val_loss: 0.0609 - val_r_squared: -3291.8157\n",
      "Epoch 48/120\n",
      "76/76 [==============================] - 30s 397ms/step - loss: 0.0670 - r_squared: 0.6197 - val_loss: 0.0613 - val_r_squared: -3622.3350\n",
      "Epoch 49/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0673 - r_squared: 0.6043INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 33s 434ms/step - loss: 0.0673 - r_squared: 0.6043 - val_loss: 0.0605 - val_r_squared: -3366.2734\n",
      "Epoch 50/120\n",
      "76/76 [==============================] - 30s 398ms/step - loss: 0.0660 - r_squared: 0.4520 - val_loss: 0.0611 - val_r_squared: -3706.2144\n",
      "Epoch 51/120\n",
      "76/76 [==============================] - 29s 381ms/step - loss: 0.0674 - r_squared: 0.2777 - val_loss: 0.0607 - val_r_squared: -3622.3887\n",
      "Epoch 52/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0653 - r_squared: 0.6444INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 33s 430ms/step - loss: 0.0653 - r_squared: 0.6444 - val_loss: 0.0600 - val_r_squared: -3471.3494\n",
      "Epoch 53/120\n",
      "76/76 [==============================] - 29s 384ms/step - loss: 0.0663 - r_squared: 0.3218 - val_loss: 0.0607 - val_r_squared: -3874.2852\n",
      "Epoch 54/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0643 - r_squared: 0.2277INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 37s 492ms/step - loss: 0.0643 - r_squared: 0.2277 - val_loss: 0.0591 - val_r_squared: -3372.2759\n",
      "Epoch 55/120\n",
      "76/76 [==============================] - 38s 500ms/step - loss: 0.0653 - r_squared: -0.4738 - val_loss: 0.0594 - val_r_squared: -3572.4607\n",
      "Epoch 56/120\n",
      "76/76 [==============================] - 35s 454ms/step - loss: 0.0639 - r_squared: -0.1354 - val_loss: 0.0592 - val_r_squared: -3655.1387\n",
      "Epoch 57/120\n",
      "76/76 [==============================] - 36s 469ms/step - loss: 0.0641 - r_squared: 0.6508 - val_loss: 0.0595 - val_r_squared: -3884.6016\n",
      "Epoch 58/120\n",
      "76/76 [==============================] - 38s 496ms/step - loss: 0.0637 - r_squared: 0.5533 - val_loss: 0.0593 - val_r_squared: -3871.1418\n",
      "Epoch 59/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0639 - r_squared: 0.3020INFO:tensorflow:Assets written to: GRU\\gru_new0_fold3_model1.ckpt\\assets\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0639 - r_squared: 0.3020 - val_loss: 0.0580 - val_r_squared: -3409.6226\n",
      "Epoch 60/120\n",
      "76/76 [==============================] - 34s 444ms/step - loss: 0.0631 - r_squared: 0.0156 - val_loss: 0.0587 - val_r_squared: -3806.7495\n",
      "Epoch 61/120\n",
      "76/76 [==============================] - 34s 444ms/step - loss: 0.0637 - r_squared: 0.7331 - val_loss: 0.0588 - val_r_squared: -3929.4492\n",
      "Epoch 62/120\n",
      "76/76 [==============================] - 33s 441ms/step - loss: 0.0638 - r_squared: 0.5193 - val_loss: 0.0581 - val_r_squared: -3706.6748\n",
      "Epoch 63/120\n",
      "76/76 [==============================] - 33s 440ms/step - loss: 0.0641 - r_squared: -0.0209 - val_loss: 0.0586 - val_r_squared: -3998.4038\n",
      "Epoch 64/120\n",
      "76/76 [==============================] - 33s 438ms/step - loss: 0.0648 - r_squared: 0.6748 - val_loss: 0.0584 - val_r_squared: -3989.8550\n",
      "fitting4\n",
      "Epoch 1/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2618 - r_squared: -4.0157INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 440ms/step - loss: 0.2618 - r_squared: -4.0157 - val_loss: 0.1686 - val_r_squared: -27676.9082\n",
      "Epoch 2/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1159 - r_squared: -0.7591INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 33s 430ms/step - loss: 0.1159 - r_squared: -0.7591 - val_loss: 0.0947 - val_r_squared: -4154.4971\n",
      "Epoch 3/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0923 - r_squared: 0.4428INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 33s 432ms/step - loss: 0.0923 - r_squared: 0.4428 - val_loss: 0.0925 - val_r_squared: -5551.2886\n",
      "Epoch 4/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0890 - r_squared: -0.0070INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 34s 441ms/step - loss: 0.0890 - r_squared: -0.0070 - val_loss: 0.0922 - val_r_squared: -6152.2505\n",
      "Epoch 5/120\n",
      "76/76 [==============================] - 35s 455ms/step - loss: 0.0871 - r_squared: 0.4344 - val_loss: 0.0923 - val_r_squared: -6121.4878\n",
      "Epoch 6/120\n",
      "76/76 [==============================] - 34s 453ms/step - loss: 0.0851 - r_squared: 0.3880 - val_loss: 0.0931 - val_r_squared: -5876.7842\n",
      "Epoch 7/120\n",
      "76/76 [==============================] - 34s 450ms/step - loss: 0.0841 - r_squared: 0.4890 - val_loss: 0.0924 - val_r_squared: -5626.1304\n",
      "Epoch 8/120\n",
      "76/76 [==============================] - 35s 460ms/step - loss: 0.0832 - r_squared: -0.6962 - val_loss: 0.0937 - val_r_squared: -5603.4136\n",
      "Epoch 9/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0816 - r_squared: 0.4313INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 495ms/step - loss: 0.0816 - r_squared: 0.4313 - val_loss: 0.0921 - val_r_squared: -5384.1392\n",
      "Epoch 10/120\n",
      "76/76 [==============================] - 41s 545ms/step - loss: 0.0817 - r_squared: 0.4694 - val_loss: 0.0921 - val_r_squared: -5420.1675\n",
      "Epoch 11/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0806 - r_squared: 0.1215INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 42s 551ms/step - loss: 0.0806 - r_squared: 0.1215 - val_loss: 0.0915 - val_r_squared: -4936.8057\n",
      "Epoch 12/120\n",
      "76/76 [==============================] - 35s 463ms/step - loss: 0.0798 - r_squared: 0.3097 - val_loss: 0.0918 - val_r_squared: -4933.5127\n",
      "Epoch 13/120\n",
      "76/76 [==============================] - 35s 460ms/step - loss: 0.0795 - r_squared: 0.2576 - val_loss: 0.0915 - val_r_squared: -4630.9478\n",
      "Epoch 14/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0784 - r_squared: 0.2191INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 37s 490ms/step - loss: 0.0784 - r_squared: 0.2191 - val_loss: 0.0906 - val_r_squared: -4441.3857\n",
      "Epoch 15/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0779 - r_squared: 0.1051INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 466ms/step - loss: 0.0779 - r_squared: 0.1051 - val_loss: 0.0893 - val_r_squared: -4108.9629\n",
      "Epoch 16/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0775 - r_squared: 0.6229INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 473ms/step - loss: 0.0775 - r_squared: 0.6229 - val_loss: 0.0885 - val_r_squared: -3868.6323\n",
      "Epoch 17/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0762 - r_squared: -0.2002INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 39s 511ms/step - loss: 0.0762 - r_squared: -0.2002 - val_loss: 0.0880 - val_r_squared: -3804.5063\n",
      "Epoch 18/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0761 - r_squared: 0.0817INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 39s 512ms/step - loss: 0.0761 - r_squared: 0.0817 - val_loss: 0.0879 - val_r_squared: -3648.3604\n",
      "Epoch 19/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0754 - r_squared: 0.2229INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 39s 508ms/step - loss: 0.0754 - r_squared: 0.2229 - val_loss: 0.0875 - val_r_squared: -3478.5730\n",
      "Epoch 20/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0758 - r_squared: 0.5396INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 39s 511ms/step - loss: 0.0758 - r_squared: 0.5396 - val_loss: 0.0853 - val_r_squared: -3005.1633\n",
      "Epoch 21/120\n",
      "76/76 [==============================] - 36s 475ms/step - loss: 0.0745 - r_squared: 0.3378 - val_loss: 0.0858 - val_r_squared: -3137.4065\n",
      "Epoch 22/120\n",
      "76/76 [==============================] - 36s 475ms/step - loss: 0.0736 - r_squared: 0.3361 - val_loss: 0.0863 - val_r_squared: -3214.9260\n",
      "Epoch 23/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0742 - r_squared: 0.3474INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 506ms/step - loss: 0.0742 - r_squared: 0.3474 - val_loss: 0.0845 - val_r_squared: -2757.3989\n",
      "Epoch 24/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0729 - r_squared: 0.0504INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 505ms/step - loss: 0.0729 - r_squared: 0.0504 - val_loss: 0.0844 - val_r_squared: -2719.1423\n",
      "Epoch 25/120\n",
      "76/76 [==============================] - 36s 473ms/step - loss: 0.0719 - r_squared: 0.5425 - val_loss: 0.0845 - val_r_squared: -2795.9172\n",
      "Epoch 26/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0718 - r_squared: -0.2676INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 502ms/step - loss: 0.0718 - r_squared: -0.2676 - val_loss: 0.0832 - val_r_squared: -2479.5605\n",
      "Epoch 27/120\n",
      "76/76 [==============================] - 36s 471ms/step - loss: 0.0717 - r_squared: 0.4531 - val_loss: 0.0841 - val_r_squared: -2822.5723\n",
      "Epoch 28/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0723 - r_squared: 0.5667INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 42s 554ms/step - loss: 0.0723 - r_squared: 0.5667 - val_loss: 0.0828 - val_r_squared: -2362.1753\n",
      "Epoch 29/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0711 - r_squared: 0.5103INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 48s 633ms/step - loss: 0.0711 - r_squared: 0.5103 - val_loss: 0.0818 - val_r_squared: -2307.9600\n",
      "Epoch 30/120\n",
      "76/76 [==============================] - 39s 514ms/step - loss: 0.0702 - r_squared: 0.4055 - val_loss: 0.0824 - val_r_squared: -2314.2585\n",
      "Epoch 31/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0685 - r_squared: 0.5949INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 41s 537ms/step - loss: 0.0685 - r_squared: 0.5949 - val_loss: 0.0814 - val_r_squared: -2206.4258\n",
      "Epoch 32/120\n",
      "76/76 [==============================] - 39s 508ms/step - loss: 0.0688 - r_squared: -0.0700 - val_loss: 0.0820 - val_r_squared: -2295.8457\n",
      "Epoch 33/120\n",
      "76/76 [==============================] - 37s 486ms/step - loss: 0.0707 - r_squared: 0.4567 - val_loss: 0.0820 - val_r_squared: -2384.7444\n",
      "Epoch 34/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0692 - r_squared: 0.5348INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 39s 513ms/step - loss: 0.0692 - r_squared: 0.5348 - val_loss: 0.0809 - val_r_squared: -2261.7393\n",
      "Epoch 35/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0676 - r_squared: 0.7158INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 39s 511ms/step - loss: 0.0676 - r_squared: 0.7158 - val_loss: 0.0796 - val_r_squared: -1949.8706\n",
      "Epoch 36/120\n",
      "76/76 [==============================] - 35s 456ms/step - loss: 0.0682 - r_squared: 0.4998 - val_loss: 0.0799 - val_r_squared: -2041.0891\n",
      "Epoch 37/120\n",
      "76/76 [==============================] - 34s 448ms/step - loss: 0.0681 - r_squared: 0.6587 - val_loss: 0.0812 - val_r_squared: -2245.6050\n",
      "Epoch 38/120\n",
      "76/76 [==============================] - 34s 442ms/step - loss: 0.0691 - r_squared: 0.5831 - val_loss: 0.0815 - val_r_squared: -2277.2588\n",
      "Epoch 39/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0672 - r_squared: 0.6563INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 472ms/step - loss: 0.0672 - r_squared: 0.6563 - val_loss: 0.0793 - val_r_squared: -2045.8379\n",
      "Epoch 40/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0689 - r_squared: 0.1860INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 31s 405ms/step - loss: 0.0689 - r_squared: 0.1860 - val_loss: 0.0790 - val_r_squared: -1952.0356\n",
      "Epoch 41/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0664 - r_squared: -0.1502INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 471ms/step - loss: 0.0664 - r_squared: -0.1502 - val_loss: 0.0781 - val_r_squared: -1835.4204\n",
      "Epoch 42/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0663 - r_squared: 0.6788INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 471ms/step - loss: 0.0663 - r_squared: 0.6788 - val_loss: 0.0772 - val_r_squared: -1661.9856\n",
      "Epoch 43/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0651 - r_squared: -0.0062INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 471ms/step - loss: 0.0651 - r_squared: -0.0062 - val_loss: 0.0771 - val_r_squared: -1742.2715\n",
      "Epoch 44/120\n",
      "76/76 [==============================] - 33s 441ms/step - loss: 0.0655 - r_squared: 0.1711 - val_loss: 0.0772 - val_r_squared: -1801.2585\n",
      "Epoch 45/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0664 - r_squared: 0.5489INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 474ms/step - loss: 0.0664 - r_squared: 0.5489 - val_loss: 0.0767 - val_r_squared: -1729.2850\n",
      "Epoch 46/120\n",
      "76/76 [==============================] - 34s 443ms/step - loss: 0.0659 - r_squared: 0.2593 - val_loss: 0.0771 - val_r_squared: -1720.8759\n",
      "Epoch 47/120\n",
      "76/76 [==============================] - 39s 514ms/step - loss: 0.0664 - r_squared: 0.4960 - val_loss: 0.0768 - val_r_squared: -1750.3538\n",
      "Epoch 48/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0638 - r_squared: -0.1349INFO:tensorflow:Assets written to: GRU\\gru_new0_fold4_model1.ckpt\\assets\n",
      "76/76 [==============================] - 47s 615ms/step - loss: 0.0638 - r_squared: -0.1349 - val_loss: 0.0755 - val_r_squared: -1555.6346\n",
      "Epoch 49/120\n",
      "76/76 [==============================] - 38s 502ms/step - loss: 0.0625 - r_squared: -0.9824 - val_loss: 0.0775 - val_r_squared: -1746.1475\n",
      "Epoch 50/120\n",
      "76/76 [==============================] - 38s 501ms/step - loss: 0.0642 - r_squared: 0.5152 - val_loss: 0.0757 - val_r_squared: -1581.9607\n",
      "Epoch 51/120\n",
      "76/76 [==============================] - 38s 497ms/step - loss: 0.0649 - r_squared: 0.6640 - val_loss: 0.0777 - val_r_squared: -1765.2615\n",
      "Epoch 52/120\n",
      "76/76 [==============================] - 37s 483ms/step - loss: 0.0645 - r_squared: 0.5850 - val_loss: 0.0761 - val_r_squared: -1560.9536\n",
      "Epoch 53/120\n",
      "76/76 [==============================] - 36s 481ms/step - loss: 0.0633 - r_squared: 0.4154 - val_loss: 0.0766 - val_r_squared: -1589.8660\n",
      "fitting5\n",
      "Epoch 1/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2348 - r_squared: -1.2863INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 42s 524ms/step - loss: 0.2348 - r_squared: -1.2863 - val_loss: 0.1190 - val_r_squared: -1411.6168\n",
      "Epoch 2/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1038 - r_squared: 0.3092INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 39s 519ms/step - loss: 0.1038 - r_squared: 0.3092 - val_loss: 0.0809 - val_r_squared: -3755.1575\n",
      "Epoch 3/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0920 - r_squared: 0.4694INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 40s 522ms/step - loss: 0.0920 - r_squared: 0.4694 - val_loss: 0.0784 - val_r_squared: -2568.8540\n",
      "Epoch 4/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0897 - r_squared: 0.5700INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 39s 518ms/step - loss: 0.0897 - r_squared: 0.5700 - val_loss: 0.0763 - val_r_squared: -2375.6733\n",
      "Epoch 5/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0872 - r_squared: 0.2894INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0872 - r_squared: 0.2894 - val_loss: 0.0749 - val_r_squared: -2256.0740\n",
      "Epoch 6/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0859 - r_squared: 0.6375INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 40s 521ms/step - loss: 0.0859 - r_squared: 0.6375 - val_loss: 0.0745 - val_r_squared: -2210.5940\n",
      "Epoch 7/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0851 - r_squared: 0.6703INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 39s 520ms/step - loss: 0.0851 - r_squared: 0.6703 - val_loss: 0.0723 - val_r_squared: -2401.6870\n",
      "Epoch 8/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0834 - r_squared: 0.6754INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 41s 541ms/step - loss: 0.0834 - r_squared: 0.6754 - val_loss: 0.0714 - val_r_squared: -2251.8242\n",
      "Epoch 9/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0830 - r_squared: 0.0345INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 41s 541ms/step - loss: 0.0830 - r_squared: 0.0345 - val_loss: 0.0706 - val_r_squared: -2243.7136\n",
      "Epoch 10/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0816 - r_squared: 0.5618INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 43s 560ms/step - loss: 0.0816 - r_squared: 0.5618 - val_loss: 0.0696 - val_r_squared: -2236.2209\n",
      "Epoch 11/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0813 - r_squared: 0.6601INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 43s 565ms/step - loss: 0.0813 - r_squared: 0.6601 - val_loss: 0.0690 - val_r_squared: -2180.6353\n",
      "Epoch 12/120\n",
      "76/76 [==============================] - 44s 573ms/step - loss: 0.0787 - r_squared: 0.6875 - val_loss: 0.0691 - val_r_squared: -2005.2666\n",
      "Epoch 13/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0796 - r_squared: 0.3969INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 45s 589ms/step - loss: 0.0796 - r_squared: 0.3969 - val_loss: 0.0686 - val_r_squared: -2051.4009\n",
      "Epoch 14/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0789 - r_squared: 0.6593INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 41s 544ms/step - loss: 0.0789 - r_squared: 0.6593 - val_loss: 0.0682 - val_r_squared: -1925.8007\n",
      "Epoch 15/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0782 - r_squared: 0.3988INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 41s 543ms/step - loss: 0.0782 - r_squared: 0.3988 - val_loss: 0.0679 - val_r_squared: -2009.8005\n",
      "Epoch 16/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0770 - r_squared: 0.5408INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 43s 569ms/step - loss: 0.0770 - r_squared: 0.5408 - val_loss: 0.0676 - val_r_squared: -2024.2347\n",
      "Epoch 17/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0769 - r_squared: 0.5964INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 41s 541ms/step - loss: 0.0769 - r_squared: 0.5964 - val_loss: 0.0666 - val_r_squared: -2222.0227\n",
      "Epoch 18/120\n",
      "76/76 [==============================] - 38s 504ms/step - loss: 0.0772 - r_squared: 0.6556 - val_loss: 0.0667 - val_r_squared: -2115.8118\n",
      "Epoch 19/120\n",
      "76/76 [==============================] - 38s 506ms/step - loss: 0.0761 - r_squared: 0.7241 - val_loss: 0.0673 - val_r_squared: -1913.0507\n",
      "Epoch 20/120\n",
      "76/76 [==============================] - 38s 505ms/step - loss: 0.0758 - r_squared: 0.5293 - val_loss: 0.0666 - val_r_squared: -1985.8359\n",
      "Epoch 21/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0754 - r_squared: 0.5220INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 41s 538ms/step - loss: 0.0754 - r_squared: 0.5220 - val_loss: 0.0666 - val_r_squared: -2063.5742\n",
      "Epoch 22/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0750 - r_squared: 0.5828INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 41s 536ms/step - loss: 0.0750 - r_squared: 0.5828 - val_loss: 0.0666 - val_r_squared: -2057.8999\n",
      "Epoch 23/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0729 - r_squared: 0.5174INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 41s 539ms/step - loss: 0.0729 - r_squared: 0.5174 - val_loss: 0.0664 - val_r_squared: -2002.5026\n",
      "Epoch 24/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0728 - r_squared: 0.6246INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 41s 542ms/step - loss: 0.0728 - r_squared: 0.6246 - val_loss: 0.0658 - val_r_squared: -2069.8105\n",
      "Epoch 25/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0736 - r_squared: 0.6014INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 41s 539ms/step - loss: 0.0736 - r_squared: 0.6014 - val_loss: 0.0655 - val_r_squared: -2158.8276\n",
      "Epoch 26/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0728 - r_squared: 0.0829INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 41s 541ms/step - loss: 0.0728 - r_squared: 0.0829 - val_loss: 0.0651 - val_r_squared: -2176.8892\n",
      "Epoch 27/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0723 - r_squared: 0.6451INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 41s 542ms/step - loss: 0.0723 - r_squared: 0.6451 - val_loss: 0.0645 - val_r_squared: -2222.3430\n",
      "Epoch 28/120\n",
      "76/76 [==============================] - 41s 543ms/step - loss: 0.0711 - r_squared: 0.5165 - val_loss: 0.0646 - val_r_squared: -2166.9297\n",
      "Epoch 29/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0714 - r_squared: 0.6549INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 49s 646ms/step - loss: 0.0714 - r_squared: 0.6549 - val_loss: 0.0642 - val_r_squared: -2222.7327\n",
      "Epoch 30/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0713 - r_squared: 0.1103INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 44s 581ms/step - loss: 0.0713 - r_squared: 0.1103 - val_loss: 0.0640 - val_r_squared: -2253.6267\n",
      "Epoch 31/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0712 - r_squared: 0.7173INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 42s 548ms/step - loss: 0.0712 - r_squared: 0.7173 - val_loss: 0.0636 - val_r_squared: -2348.1724\n",
      "Epoch 32/120\n",
      "76/76 [==============================] - 39s 510ms/step - loss: 0.0694 - r_squared: 0.4783 - val_loss: 0.0636 - val_r_squared: -2108.0466\n",
      "Epoch 33/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0699 - r_squared: 0.4791INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 41s 535ms/step - loss: 0.0699 - r_squared: 0.4791 - val_loss: 0.0634 - val_r_squared: -2108.0176\n",
      "Epoch 34/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0693 - r_squared: 0.6408INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 40s 522ms/step - loss: 0.0693 - r_squared: 0.6408 - val_loss: 0.0632 - val_r_squared: -2199.2031\n",
      "Epoch 35/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0695 - r_squared: 0.3794INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0695 - r_squared: 0.3794 - val_loss: 0.0623 - val_r_squared: -2483.0630\n",
      "Epoch 36/120\n",
      "76/76 [==============================] - 37s 481ms/step - loss: 0.0685 - r_squared: 0.3641 - val_loss: 0.0625 - val_r_squared: -2297.0212\n",
      "Epoch 37/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0692 - r_squared: 0.1962INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 39s 517ms/step - loss: 0.0692 - r_squared: 0.1962 - val_loss: 0.0623 - val_r_squared: -2433.2610\n",
      "Epoch 38/120\n",
      "76/76 [==============================] - 36s 480ms/step - loss: 0.0689 - r_squared: -0.2311 - val_loss: 0.0624 - val_r_squared: -2369.7185\n",
      "Epoch 39/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0671 - r_squared: 0.5136INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 39s 518ms/step - loss: 0.0671 - r_squared: 0.5136 - val_loss: 0.0622 - val_r_squared: -2276.4419\n",
      "Epoch 40/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0673 - r_squared: 0.0039INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 39s 512ms/step - loss: 0.0673 - r_squared: 0.0039 - val_loss: 0.0620 - val_r_squared: -2149.9429\n",
      "Epoch 41/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0690 - r_squared: 0.4580INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 39s 514ms/step - loss: 0.0690 - r_squared: 0.4580 - val_loss: 0.0617 - val_r_squared: -2348.2520\n",
      "Epoch 42/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0667 - r_squared: 0.3372INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 39s 514ms/step - loss: 0.0667 - r_squared: 0.3372 - val_loss: 0.0617 - val_r_squared: -2200.3538\n",
      "Epoch 43/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0669 - r_squared: 0.2828INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 41s 535ms/step - loss: 0.0669 - r_squared: 0.2828 - val_loss: 0.0615 - val_r_squared: -2203.4702\n",
      "Epoch 44/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0673 - r_squared: 0.7413INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 42s 556ms/step - loss: 0.0673 - r_squared: 0.7413 - val_loss: 0.0614 - val_r_squared: -2198.6982\n",
      "Epoch 45/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0673 - r_squared: 0.3736INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 43s 569ms/step - loss: 0.0673 - r_squared: 0.3736 - val_loss: 0.0610 - val_r_squared: -2257.6687\n",
      "Epoch 46/120\n",
      "76/76 [==============================] - 47s 617ms/step - loss: 0.0659 - r_squared: 0.6252 - val_loss: 0.0613 - val_r_squared: -2243.4287\n",
      "Epoch 47/120\n",
      "76/76 [==============================] - 46s 602ms/step - loss: 0.0656 - r_squared: 0.4053 - val_loss: 0.0612 - val_r_squared: -2462.5547\n",
      "Epoch 48/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0654 - r_squared: 0.5123INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 45s 586ms/step - loss: 0.0654 - r_squared: 0.5123 - val_loss: 0.0608 - val_r_squared: -2396.4460\n",
      "Epoch 49/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0645 - r_squared: 0.6580INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 44s 582ms/step - loss: 0.0645 - r_squared: 0.6580 - val_loss: 0.0608 - val_r_squared: -2168.2268\n",
      "Epoch 50/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0647 - r_squared: -0.6740INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 41s 546ms/step - loss: 0.0647 - r_squared: -0.6740 - val_loss: 0.0607 - val_r_squared: -2248.7998\n",
      "Epoch 51/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0638 - r_squared: 0.0107INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 506ms/step - loss: 0.0638 - r_squared: 0.0107 - val_loss: 0.0605 - val_r_squared: -2295.8022\n",
      "Epoch 52/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0650 - r_squared: 0.5580INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 506ms/step - loss: 0.0650 - r_squared: 0.5580 - val_loss: 0.0598 - val_r_squared: -2444.9348\n",
      "Epoch 53/120\n",
      "76/76 [==============================] - 36s 471ms/step - loss: 0.0649 - r_squared: 0.2376 - val_loss: 0.0602 - val_r_squared: -2276.4241\n",
      "Epoch 54/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0644 - r_squared: 0.7295INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 37s 493ms/step - loss: 0.0644 - r_squared: 0.7295 - val_loss: 0.0597 - val_r_squared: -2349.0122\n",
      "Epoch 55/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0643 - r_squared: 0.0577INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 37s 487ms/step - loss: 0.0643 - r_squared: 0.0577 - val_loss: 0.0596 - val_r_squared: -2445.0618\n",
      "Epoch 56/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0640 - r_squared: 0.5725INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 499ms/step - loss: 0.0640 - r_squared: 0.5725 - val_loss: 0.0596 - val_r_squared: -2298.5884\n",
      "Epoch 57/120\n",
      "76/76 [==============================] - 35s 463ms/step - loss: 0.0637 - r_squared: 0.6160 - val_loss: 0.0597 - val_r_squared: -2340.6997\n",
      "Epoch 58/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0628 - r_squared: 0.6128INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 501ms/step - loss: 0.0628 - r_squared: 0.6128 - val_loss: 0.0592 - val_r_squared: -2269.7112\n",
      "Epoch 59/120\n",
      "76/76 [==============================] - 35s 463ms/step - loss: 0.0624 - r_squared: 0.0300 - val_loss: 0.0592 - val_r_squared: -2350.9927\n",
      "Epoch 60/120\n",
      "76/76 [==============================] - 35s 463ms/step - loss: 0.0631 - r_squared: 0.4622 - val_loss: 0.0594 - val_r_squared: -2374.0854\n",
      "Epoch 61/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0622 - r_squared: -0.1837INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 498ms/step - loss: 0.0622 - r_squared: -0.1837 - val_loss: 0.0586 - val_r_squared: -2395.1833\n",
      "Epoch 62/120\n",
      "76/76 [==============================] - 35s 463ms/step - loss: 0.0614 - r_squared: 0.5204 - val_loss: 0.0588 - val_r_squared: -2290.8579\n",
      "Epoch 63/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0645 - r_squared: -0.1641INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 40s 530ms/step - loss: 0.0645 - r_squared: -0.1641 - val_loss: 0.0581 - val_r_squared: -2365.3948\n",
      "Epoch 64/120\n",
      "76/76 [==============================] - 42s 550ms/step - loss: 0.0625 - r_squared: 0.6800 - val_loss: 0.0586 - val_r_squared: -2220.1409\n",
      "Epoch 65/120\n",
      "76/76 [==============================] - 40s 533ms/step - loss: 0.0599 - r_squared: 0.2123 - val_loss: 0.0585 - val_r_squared: -2240.7310\n",
      "Epoch 66/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0630 - r_squared: 0.6119INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 39s 519ms/step - loss: 0.0630 - r_squared: 0.6119 - val_loss: 0.0581 - val_r_squared: -2274.0608\n",
      "Epoch 67/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0615 - r_squared: 0.5004INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 497ms/step - loss: 0.0615 - r_squared: 0.5004 - val_loss: 0.0580 - val_r_squared: -2231.6733\n",
      "Epoch 68/120\n",
      "76/76 [==============================] - 36s 476ms/step - loss: 0.0606 - r_squared: 0.2753 - val_loss: 0.0582 - val_r_squared: -2188.4211\n",
      "Epoch 69/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0607 - r_squared: 0.5277INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 499ms/step - loss: 0.0607 - r_squared: 0.5277 - val_loss: 0.0579 - val_r_squared: -2161.5640\n",
      "Epoch 70/120\n",
      "76/76 [==============================] - 34s 452ms/step - loss: 0.0628 - r_squared: 0.6816 - val_loss: 0.0581 - val_r_squared: -2061.6238\n",
      "Epoch 71/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0615 - r_squared: 0.2483INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 37s 488ms/step - loss: 0.0615 - r_squared: 0.2483 - val_loss: 0.0578 - val_r_squared: -2078.5459\n",
      "Epoch 72/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0611 - r_squared: 0.1397INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 37s 491ms/step - loss: 0.0611 - r_squared: 0.1397 - val_loss: 0.0575 - val_r_squared: -2437.2646\n",
      "Epoch 73/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0604 - r_squared: 0.3228INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 37s 492ms/step - loss: 0.0604 - r_squared: 0.3228 - val_loss: 0.0574 - val_r_squared: -2240.5920\n",
      "Epoch 74/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0595 - r_squared: 0.1478INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 479ms/step - loss: 0.0595 - r_squared: 0.1478 - val_loss: 0.0572 - val_r_squared: -2146.0664\n",
      "Epoch 75/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0579 - r_squared: 0.2438INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 480ms/step - loss: 0.0579 - r_squared: 0.2438 - val_loss: 0.0565 - val_r_squared: -2337.6099\n",
      "Epoch 76/120\n",
      "76/76 [==============================] - 33s 434ms/step - loss: 0.0587 - r_squared: 0.2189 - val_loss: 0.0570 - val_r_squared: -2076.7546\n",
      "Epoch 77/120\n",
      "76/76 [==============================] - 33s 435ms/step - loss: 0.0607 - r_squared: 0.4664 - val_loss: 0.0569 - val_r_squared: -2204.3953\n",
      "Epoch 78/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0590 - r_squared: 0.6605INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 467ms/step - loss: 0.0590 - r_squared: 0.6605 - val_loss: 0.0565 - val_r_squared: -2227.7898\n",
      "Epoch 79/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0584 - r_squared: 0.1821INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 469ms/step - loss: 0.0584 - r_squared: 0.1821 - val_loss: 0.0561 - val_r_squared: -2228.7043\n",
      "Epoch 80/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0589 - r_squared: 0.4137INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 476ms/step - loss: 0.0589 - r_squared: 0.4137 - val_loss: 0.0561 - val_r_squared: -2098.1873\n",
      "Epoch 81/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0582 - r_squared: 0.5869INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 501ms/step - loss: 0.0582 - r_squared: 0.5869 - val_loss: 0.0559 - val_r_squared: -2113.6851\n",
      "Epoch 82/120\n",
      "76/76 [==============================] - 34s 444ms/step - loss: 0.0606 - r_squared: 0.5825 - val_loss: 0.0561 - val_r_squared: -1977.8999\n",
      "Epoch 83/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0578 - r_squared: 0.4840INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 43s 571ms/step - loss: 0.0578 - r_squared: 0.4840 - val_loss: 0.0555 - val_r_squared: -2016.3630\n",
      "Epoch 84/120\n",
      "76/76 [==============================] - 41s 541ms/step - loss: 0.0589 - r_squared: 0.6627 - val_loss: 0.0560 - val_r_squared: -2081.4316\n",
      "Epoch 85/120\n",
      "76/76 [==============================] - 36s 468ms/step - loss: 0.0593 - r_squared: 0.5013 - val_loss: 0.0557 - val_r_squared: -2089.8525\n",
      "Epoch 86/120\n",
      "76/76 [==============================] - 35s 462ms/step - loss: 0.0595 - r_squared: 0.4315 - val_loss: 0.0558 - val_r_squared: -2072.2356\n",
      "Epoch 87/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0572 - r_squared: 0.0757INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 496ms/step - loss: 0.0572 - r_squared: 0.0757 - val_loss: 0.0554 - val_r_squared: -2187.8135\n",
      "Epoch 88/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0578 - r_squared: 0.5567INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 37s 485ms/step - loss: 0.0578 - r_squared: 0.5567 - val_loss: 0.0551 - val_r_squared: -2022.5396\n",
      "Epoch 89/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0583 - r_squared: 0.5372INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 472ms/step - loss: 0.0583 - r_squared: 0.5372 - val_loss: 0.0548 - val_r_squared: -2055.6755\n",
      "Epoch 90/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0576 - r_squared: 0.4370INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 480ms/step - loss: 0.0576 - r_squared: 0.4370 - val_loss: 0.0545 - val_r_squared: -1968.2434\n",
      "Epoch 91/120\n",
      "76/76 [==============================] - 33s 440ms/step - loss: 0.0592 - r_squared: 0.2929 - val_loss: 0.0549 - val_r_squared: -2099.6616\n",
      "Epoch 92/120\n",
      "76/76 [==============================] - 33s 440ms/step - loss: 0.0584 - r_squared: 0.5875 - val_loss: 0.0548 - val_r_squared: -2049.2048\n",
      "Epoch 93/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0579 - r_squared: 0.6581INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 475ms/step - loss: 0.0579 - r_squared: 0.6581 - val_loss: 0.0539 - val_r_squared: -2015.6957\n",
      "Epoch 94/120\n",
      "76/76 [==============================] - 33s 437ms/step - loss: 0.0567 - r_squared: 0.5988 - val_loss: 0.0545 - val_r_squared: -1939.3572\n",
      "Epoch 95/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0571 - r_squared: 0.5723INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 472ms/step - loss: 0.0571 - r_squared: 0.5723 - val_loss: 0.0539 - val_r_squared: -2070.9521\n",
      "Epoch 96/120\n",
      "76/76 [==============================] - 33s 439ms/step - loss: 0.0555 - r_squared: 0.4963 - val_loss: 0.0540 - val_r_squared: -1905.0953\n",
      "Epoch 97/120\n",
      "76/76 [==============================] - 34s 444ms/step - loss: 0.0595 - r_squared: 0.7620 - val_loss: 0.0541 - val_r_squared: -1979.5096\n",
      "Epoch 98/120\n",
      "76/76 [==============================] - 34s 442ms/step - loss: 0.0568 - r_squared: 0.6151 - val_loss: 0.0543 - val_r_squared: -2012.7737\n",
      "Epoch 99/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0562 - r_squared: 0.4889INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 475ms/step - loss: 0.0562 - r_squared: 0.4889 - val_loss: 0.0533 - val_r_squared: -2168.8049\n",
      "Epoch 100/120\n",
      "76/76 [==============================] - 33s 441ms/step - loss: 0.0561 - r_squared: 0.6806 - val_loss: 0.0544 - val_r_squared: -1889.6702\n",
      "Epoch 101/120\n",
      "76/76 [==============================] - 33s 440ms/step - loss: 0.0568 - r_squared: 0.1806 - val_loss: 0.0540 - val_r_squared: -1988.5513\n",
      "Epoch 102/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0558 - r_squared: 0.5854INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 41s 538ms/step - loss: 0.0558 - r_squared: 0.5854 - val_loss: 0.0531 - val_r_squared: -1907.8564\n",
      "Epoch 103/120\n",
      "76/76 [==============================] - 41s 537ms/step - loss: 0.0583 - r_squared: 0.7831 - val_loss: 0.0536 - val_r_squared: -1779.4646\n",
      "Epoch 104/120\n",
      "76/76 [==============================] - 38s 502ms/step - loss: 0.0578 - r_squared: -0.1111 - val_loss: 0.0535 - val_r_squared: -1908.2140\n",
      "Epoch 105/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0565 - r_squared: 0.3200INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 39s 510ms/step - loss: 0.0565 - r_squared: 0.3200 - val_loss: 0.0530 - val_r_squared: -2017.4836\n",
      "Epoch 106/120\n",
      "76/76 [==============================] - 36s 474ms/step - loss: 0.0577 - r_squared: 0.3629 - val_loss: 0.0530 - val_r_squared: -2059.4404\n",
      "Epoch 107/120\n",
      "76/76 [==============================] - 35s 466ms/step - loss: 0.0559 - r_squared: 0.5212 - val_loss: 0.0533 - val_r_squared: -2011.3091\n",
      "Epoch 108/120\n",
      "76/76 [==============================] - 34s 450ms/step - loss: 0.0550 - r_squared: 0.5744 - val_loss: 0.0531 - val_r_squared: -1908.3990\n",
      "Epoch 109/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0540 - r_squared: 0.2330INFO:tensorflow:Assets written to: GRU\\gru_new0_fold5_model1.ckpt\\assets\n",
      "76/76 [==============================] - 37s 484ms/step - loss: 0.0540 - r_squared: 0.2330 - val_loss: 0.0522 - val_r_squared: -1931.9933\n",
      "Epoch 110/120\n",
      "76/76 [==============================] - 34s 451ms/step - loss: 0.0527 - r_squared: 0.2307 - val_loss: 0.0525 - val_r_squared: -1916.5902\n",
      "Epoch 111/120\n",
      "76/76 [==============================] - 34s 450ms/step - loss: 0.0540 - r_squared: 0.7172 - val_loss: 0.0525 - val_r_squared: -1811.2426\n",
      "Epoch 112/120\n",
      "76/76 [==============================] - 33s 435ms/step - loss: 0.0562 - r_squared: 0.6486 - val_loss: 0.0526 - val_r_squared: -1833.5764\n",
      "Epoch 113/120\n",
      "76/76 [==============================] - 33s 435ms/step - loss: 0.0551 - r_squared: 0.6185 - val_loss: 0.0524 - val_r_squared: -1760.6447\n",
      "Epoch 114/120\n",
      "76/76 [==============================] - 33s 436ms/step - loss: 0.0551 - r_squared: -0.1111 - val_loss: 0.0523 - val_r_squared: -1681.8730\n",
      "fitting6\n",
      "Epoch 1/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2735 - r_squared: -5.4458INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 37s 467ms/step - loss: 0.2735 - r_squared: -5.4458 - val_loss: 0.1473 - val_r_squared: -2166.0168\n",
      "Epoch 2/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1128 - r_squared: -5.9170INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 463ms/step - loss: 0.1128 - r_squared: -5.9170 - val_loss: 0.1001 - val_r_squared: -931.2175\n",
      "Epoch 3/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0967 - r_squared: -2.3326INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 465ms/step - loss: 0.0967 - r_squared: -2.3326 - val_loss: 0.0903 - val_r_squared: -551.3369\n",
      "Epoch 4/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0930 - r_squared: -1.3839INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 34s 455ms/step - loss: 0.0930 - r_squared: -1.3839 - val_loss: 0.0850 - val_r_squared: -384.4045\n",
      "Epoch 5/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0900 - r_squared: -7.4645INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 457ms/step - loss: 0.0900 - r_squared: -7.4645 - val_loss: 0.0806 - val_r_squared: -261.2769\n",
      "Epoch 6/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0882 - r_squared: -4.3148INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 458ms/step - loss: 0.0882 - r_squared: -4.3148 - val_loss: 0.0776 - val_r_squared: -141.7872\n",
      "Epoch 7/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0863 - r_squared: -1.9123INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 465ms/step - loss: 0.0863 - r_squared: -1.9123 - val_loss: 0.0760 - val_r_squared: -129.4975\n",
      "Epoch 8/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0857 - r_squared: -1.1340INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 45s 591ms/step - loss: 0.0857 - r_squared: -1.1340 - val_loss: 0.0736 - val_r_squared: -93.4386\n",
      "Epoch 9/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0853 - r_squared: -1.0524INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 42s 552ms/step - loss: 0.0853 - r_squared: -1.0524 - val_loss: 0.0714 - val_r_squared: -66.5854\n",
      "Epoch 10/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0844 - r_squared: -9.4614INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 37s 487ms/step - loss: 0.0844 - r_squared: -9.4614 - val_loss: 0.0698 - val_r_squared: -44.2840\n",
      "Epoch 11/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0832 - r_squared: -1.8061INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 37s 485ms/step - loss: 0.0832 - r_squared: -1.8061 - val_loss: 0.0684 - val_r_squared: -33.4487\n",
      "Epoch 12/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0836 - r_squared: -5.7686INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 476ms/step - loss: 0.0836 - r_squared: -5.7686 - val_loss: 0.0679 - val_r_squared: -33.4945\n",
      "Epoch 13/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0836 - r_squared: -4.6852INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 467ms/step - loss: 0.0836 - r_squared: -4.6852 - val_loss: 0.0668 - val_r_squared: -16.9652\n",
      "Epoch 14/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0842 - r_squared: -3.7284INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 461ms/step - loss: 0.0842 - r_squared: -3.7284 - val_loss: 0.0658 - val_r_squared: -12.0039\n",
      "Epoch 15/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0832 - r_squared: -1.8388INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 461ms/step - loss: 0.0832 - r_squared: -1.8388 - val_loss: 0.0648 - val_r_squared: 0.0090\n",
      "Epoch 16/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0835 - r_squared: -2.3768INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 460ms/step - loss: 0.0835 - r_squared: -2.3768 - val_loss: 0.0642 - val_r_squared: 0.6928\n",
      "Epoch 17/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0838 - r_squared: -0.3443INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 462ms/step - loss: 0.0838 - r_squared: -0.3443 - val_loss: 0.0635 - val_r_squared: -0.4360\n",
      "Epoch 18/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0836 - r_squared: -1.7053INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 467ms/step - loss: 0.0836 - r_squared: -1.7053 - val_loss: 0.0634 - val_r_squared: -4.2845\n",
      "Epoch 19/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0837 - r_squared: -5.9136INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 466ms/step - loss: 0.0837 - r_squared: -5.9136 - val_loss: 0.0632 - val_r_squared: -20.0960\n",
      "Epoch 20/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0828 - r_squared: -17.4006INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 467ms/step - loss: 0.0828 - r_squared: -17.4006 - val_loss: 0.0628 - val_r_squared: -63.6070\n",
      "Epoch 21/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0829 - r_squared: -4.0800INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 464ms/step - loss: 0.0829 - r_squared: -4.0800 - val_loss: 0.0623 - val_r_squared: -93.9070\n",
      "Epoch 22/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0818 - r_squared: -5.1691INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 464ms/step - loss: 0.0818 - r_squared: -5.1691 - val_loss: 0.0620 - val_r_squared: -77.7773\n",
      "Epoch 23/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0820 - r_squared: -2.2524INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 465ms/step - loss: 0.0820 - r_squared: -2.2524 - val_loss: 0.0619 - val_r_squared: -101.2711\n",
      "Epoch 24/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0811 - r_squared: -11.4546INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 34s 452ms/step - loss: 0.0811 - r_squared: -11.4546 - val_loss: 0.0612 - val_r_squared: -88.8304\n",
      "Epoch 25/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0794 - r_squared: -3.8746INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 462ms/step - loss: 0.0794 - r_squared: -3.8746 - val_loss: 0.0610 - val_r_squared: -118.9054\n",
      "Epoch 26/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0796 - r_squared: -3.2724INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 470ms/step - loss: 0.0796 - r_squared: -3.2724 - val_loss: 0.0610 - val_r_squared: -176.0889\n",
      "Epoch 27/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0780 - r_squared: -3.2851INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 43s 574ms/step - loss: 0.0780 - r_squared: -3.2851 - val_loss: 0.0603 - val_r_squared: -136.6489\n",
      "Epoch 28/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0780 - r_squared: -9.7585INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 44s 574ms/step - loss: 0.0780 - r_squared: -9.7585 - val_loss: 0.0598 - val_r_squared: -155.5186\n",
      "Epoch 29/120\n",
      "76/76 [==============================] - 36s 480ms/step - loss: 0.0785 - r_squared: -4.3737 - val_loss: 0.0599 - val_r_squared: -170.4588\n",
      "Epoch 30/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0775 - r_squared: -7.1249INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 500ms/step - loss: 0.0775 - r_squared: -7.1249 - val_loss: 0.0595 - val_r_squared: -167.1106\n",
      "Epoch 31/120\n",
      "76/76 [==============================] - 35s 463ms/step - loss: 0.0760 - r_squared: -7.1322 - val_loss: 0.0597 - val_r_squared: -206.4534\n",
      "Epoch 32/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0749 - r_squared: -2.7655INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 471ms/step - loss: 0.0749 - r_squared: -2.7655 - val_loss: 0.0593 - val_r_squared: -191.7055\n",
      "Epoch 33/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0743 - r_squared: -10.8374INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 469ms/step - loss: 0.0743 - r_squared: -10.8374 - val_loss: 0.0587 - val_r_squared: -200.9771\n",
      "Epoch 34/120\n",
      "76/76 [==============================] - 33s 434ms/step - loss: 0.0766 - r_squared: -2.9701 - val_loss: 0.0589 - val_r_squared: -226.4955\n",
      "Epoch 35/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0750 - r_squared: -3.1412INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 465ms/step - loss: 0.0750 - r_squared: -3.1412 - val_loss: 0.0585 - val_r_squared: -232.1803\n",
      "Epoch 36/120\n",
      "76/76 [==============================] - 33s 431ms/step - loss: 0.0770 - r_squared: -2.1127 - val_loss: 0.0589 - val_r_squared: -291.9180\n",
      "Epoch 37/120\n",
      "76/76 [==============================] - 33s 433ms/step - loss: 0.0749 - r_squared: -4.5506 - val_loss: 0.0586 - val_r_squared: -255.2243\n",
      "Epoch 38/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0742 - r_squared: -4.2698INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 470ms/step - loss: 0.0742 - r_squared: -4.2698 - val_loss: 0.0585 - val_r_squared: -335.8272\n",
      "Epoch 39/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0732 - r_squared: -2.1274INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 463ms/step - loss: 0.0732 - r_squared: -2.1274 - val_loss: 0.0579 - val_r_squared: -263.2446\n",
      "Epoch 40/120\n",
      "76/76 [==============================] - 33s 433ms/step - loss: 0.0732 - r_squared: -5.2466 - val_loss: 0.0579 - val_r_squared: -316.1179\n",
      "Epoch 41/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0738 - r_squared: -4.8114INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 466ms/step - loss: 0.0738 - r_squared: -4.8114 - val_loss: 0.0575 - val_r_squared: -305.6683\n",
      "Epoch 42/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0729 - r_squared: -2.9835INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 462ms/step - loss: 0.0729 - r_squared: -2.9835 - val_loss: 0.0575 - val_r_squared: -306.8387\n",
      "Epoch 43/120\n",
      "76/76 [==============================] - 33s 431ms/step - loss: 0.0715 - r_squared: -2.5128 - val_loss: 0.0579 - val_r_squared: -351.9955\n",
      "Epoch 44/120\n",
      "76/76 [==============================] - 33s 436ms/step - loss: 0.0729 - r_squared: -8.1604 - val_loss: 0.0578 - val_r_squared: -421.2201\n",
      "Epoch 45/120\n",
      "76/76 [==============================] - 33s 436ms/step - loss: 0.0719 - r_squared: -0.5768 - val_loss: 0.0575 - val_r_squared: -466.7318\n",
      "Epoch 46/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0721 - r_squared: -3.1245INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 39s 511ms/step - loss: 0.0721 - r_squared: -3.1245 - val_loss: 0.0575 - val_r_squared: -505.3415\n",
      "Epoch 47/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0706 - r_squared: -8.7720INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 44s 580ms/step - loss: 0.0706 - r_squared: -8.7720 - val_loss: 0.0564 - val_r_squared: -422.7384\n",
      "Epoch 48/120\n",
      "76/76 [==============================] - 38s 499ms/step - loss: 0.0707 - r_squared: -11.1526 - val_loss: 0.0566 - val_r_squared: -408.4381\n",
      "Epoch 49/120\n",
      "76/76 [==============================] - 36s 471ms/step - loss: 0.0714 - r_squared: -1.1114 - val_loss: 0.0567 - val_r_squared: -443.0506\n",
      "Epoch 50/120\n",
      "76/76 [==============================] - 36s 473ms/step - loss: 0.0701 - r_squared: -6.5587 - val_loss: 0.0566 - val_r_squared: -459.9052\n",
      "Epoch 51/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0701 - r_squared: -6.9585INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 503ms/step - loss: 0.0701 - r_squared: -6.9585 - val_loss: 0.0563 - val_r_squared: -463.5335\n",
      "Epoch 52/120\n",
      "76/76 [==============================] - 34s 446ms/step - loss: 0.0702 - r_squared: -6.4056 - val_loss: 0.0568 - val_r_squared: -484.4333\n",
      "Epoch 53/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0701 - r_squared: -3.2388INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 472ms/step - loss: 0.0701 - r_squared: -3.2388 - val_loss: 0.0559 - val_r_squared: -483.6037\n",
      "Epoch 54/120\n",
      "76/76 [==============================] - 33s 439ms/step - loss: 0.0688 - r_squared: -7.3656 - val_loss: 0.0564 - val_r_squared: -507.8103\n",
      "Epoch 55/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0691 - r_squared: -1.4544INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 475ms/step - loss: 0.0691 - r_squared: -1.4544 - val_loss: 0.0557 - val_r_squared: -530.3629\n",
      "Epoch 56/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0695 - r_squared: -13.8410INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 474ms/step - loss: 0.0695 - r_squared: -13.8410 - val_loss: 0.0556 - val_r_squared: -578.8350\n",
      "Epoch 57/120\n",
      "76/76 [==============================] - 33s 439ms/step - loss: 0.0689 - r_squared: -8.9088 - val_loss: 0.0559 - val_r_squared: -554.6323\n",
      "Epoch 58/120\n",
      "76/76 [==============================] - 34s 442ms/step - loss: 0.0694 - r_squared: -3.2328 - val_loss: 0.0559 - val_r_squared: -603.5353\n",
      "Epoch 59/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0690 - r_squared: -4.9637INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 37s 483ms/step - loss: 0.0690 - r_squared: -4.9637 - val_loss: 0.0556 - val_r_squared: -612.2057\n",
      "Epoch 60/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0684 - r_squared: -3.1366INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 471ms/step - loss: 0.0684 - r_squared: -3.1366 - val_loss: 0.0554 - val_r_squared: -602.7819\n",
      "Epoch 61/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0682 - r_squared: -2.5689INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 35s 467ms/step - loss: 0.0682 - r_squared: -2.5689 - val_loss: 0.0551 - val_r_squared: -616.8014\n",
      "Epoch 62/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0668 - r_squared: -13.0969INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 36s 471ms/step - loss: 0.0668 - r_squared: -13.0969 - val_loss: 0.0545 - val_r_squared: -594.1777\n",
      "Epoch 63/120\n",
      "76/76 [==============================] - 33s 436ms/step - loss: 0.0666 - r_squared: -2.3822 - val_loss: 0.0546 - val_r_squared: -656.2725\n",
      "Epoch 64/120\n",
      "76/76 [==============================] - 34s 441ms/step - loss: 0.0673 - r_squared: -3.8007 - val_loss: 0.0546 - val_r_squared: -680.4547\n",
      "Epoch 65/120\n",
      "76/76 [==============================] - 34s 442ms/step - loss: 0.0667 - r_squared: -2.0390 - val_loss: 0.0549 - val_r_squared: -697.0626\n",
      "Epoch 66/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0666 - r_squared: -8.6765INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 44s 586ms/step - loss: 0.0666 - r_squared: -8.6765 - val_loss: 0.0538 - val_r_squared: -623.2159\n",
      "Epoch 67/120\n",
      "76/76 [==============================] - 41s 542ms/step - loss: 0.0676 - r_squared: -6.9311 - val_loss: 0.0541 - val_r_squared: -616.6733\n",
      "Epoch 68/120\n",
      "76/76 [==============================] - 36s 474ms/step - loss: 0.0676 - r_squared: -15.2631 - val_loss: 0.0542 - val_r_squared: -706.6281\n",
      "Epoch 69/120\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0656 - r_squared: -3.4056INFO:tensorflow:Assets written to: GRU\\gru_new0_fold6_model1.ckpt\\assets\n",
      "76/76 [==============================] - 38s 501ms/step - loss: 0.0656 - r_squared: -3.4056 - val_loss: 0.0534 - val_r_squared: -647.2500\n",
      "Epoch 70/120\n",
      "76/76 [==============================] - 35s 463ms/step - loss: 0.0671 - r_squared: -15.6389 - val_loss: 0.0539 - val_r_squared: -691.7815\n",
      "Epoch 71/120\n",
      "76/76 [==============================] - 34s 441ms/step - loss: 0.0647 - r_squared: -5.9010 - val_loss: 0.0538 - val_r_squared: -745.8752\n",
      "Epoch 72/120\n",
      "76/76 [==============================] - 33s 435ms/step - loss: 0.0659 - r_squared: -2.8899 - val_loss: 0.0538 - val_r_squared: -753.4162\n",
      "Epoch 73/120\n",
      "76/76 [==============================] - 33s 433ms/step - loss: 0.0664 - r_squared: -10.1776 - val_loss: 0.0537 - val_r_squared: -840.6545\n",
      "Epoch 74/120\n",
      "76/76 [==============================] - 33s 437ms/step - loss: 0.0656 - r_squared: -18.5621 - val_loss: 0.0538 - val_r_squared: -770.7570\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "def rmse(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "    \n",
    "r2 = []\n",
    "loss = []\n",
    "dic ={\"r_squared\": r_squared,\"rmse\":rmse}\n",
    "\n",
    "for p in GRUparams.index:\n",
    "    if p == 0 or p == 1:\n",
    "        params0 = GRUparams.iloc[p,:].to_dict()\n",
    "        G = GRU_foldcv(params0)\n",
    "        clear_output(wait = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19aac84f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:1, r2:0.9275436180152187, rmse: 0.07009861618280411\n",
      "Fold:2, r2:0.9250859682903441, rmse: 0.07110964506864548\n",
      "Fold:3, r2:0.9171828774660425, rmse: 0.07621310651302338\n",
      "Fold:4, r2:0.8911486092055572, rmse: 0.08431190252304077\n",
      "Fold:5, r2:0.9163851028982557, rmse: 0.07371922582387924\n",
      "Fold:6, r2:0.9162267008656125, rmse: 0.07669807225465775\n",
      "Fold:1, r2:0.9275436180152187, rmse: 0.07009861618280411\n",
      "Fold:2, r2:0.9250859682903441, rmse: 0.07110964506864548\n",
      "Fold:3, r2:0.9171828774660425, rmse: 0.07621310651302338\n",
      "Fold:4, r2:0.8911486092055572, rmse: 0.08431190252304077\n",
      "Fold:5, r2:0.9163851028982557, rmse: 0.07371922582387924\n",
      "Fold:6, r2:0.9162267008656125, rmse: 0.07669807225465775\n"
     ]
    }
   ],
   "source": [
    "#import load_model\n",
    "\n",
    "r2 = []\n",
    "loss = []\n",
    "dic ={\"r_squared\": r_squared,\"rmse\":rmse}\n",
    "best_models = []\n",
    "for p in GRUparams.index:\n",
    "    for i in list(range(0,7))[1:]:\n",
    "        if p == 0 or p == 1:\n",
    "            params = GRUparams.iloc[p,:].to_dict()\n",
    "            exec(f'model{p}_{i} = load_model(f\"GRU/gru_new0_fold{i}_model{p}.ckpt\",dic)')\n",
    "            exec(f'model{p}_{i}.compile(loss = rmse,\\\n",
    "                                     optimizer=keras.optimizers.Adam(learning_rate= params[\"learning_rate\"]),\\\n",
    "                                    metrics = [r_squared])')\n",
    "            print(f'fold{p}_{i}')\n",
    "            exec(f'pred = model{p}_{i}.predict(X_test)')\n",
    "            exec(f'best_models.append(model{p}_{i})')\n",
    "            r = r2_score(pred,y_test);r2.append(r)\n",
    "            loss.append(rmse(pred,y_test))\n",
    "    clear_output()\n",
    "for p in GRUparams.index:\n",
    "    for i in list(range(0,7))[1:]:\n",
    "        if p == 0 or p == 1:\n",
    "            print(f'Fold:{i}, r2:{r2[i]}, rmse: {loss[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d1f1fd76-1c85-468a-9667-4353308b56c5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. sol[W/m2]: 0.00202740915119648\n",
      "2. HypTemp: 0.001991651253774762\n",
      "3. EpiV: 0.0014319387264549732\n",
      "4. St: 0.0013611712493002415\n",
      "5. u2*[m/s]: 0.0008866788703016937\n",
      "6. EpiTemp: 0.0007368387305177748\n",
      "7. HypV: 0.0007323896861635149\n",
      "8. Therm: 0.0007055659079924226\n",
      "9. PLoad: 0.0004093690076842904\n"
     ]
    }
   ],
   "source": [
    "def get_input_gradients(model, x):\n",
    "    # Convert the input to a tensor\n",
    "    x = np.reshape(x, (x.shape[0], 100, 9))\n",
    "    x_tensor = tf.convert_to_tensor(x)\n",
    "    \n",
    "    # Compute the gradients of the output with respect to the input\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x_tensor)\n",
    "        output = model(x_tensor)\n",
    "        \n",
    "    gradients = tape.gradient(output, x_tensor)\n",
    "    \n",
    "    # Convert the gradients to a numpy array\n",
    "    gradients = tf.keras.backend.get_value(gradients)\n",
    "    \n",
    "    # Compute the absolute sum of the gradients for each feature\n",
    "    feature_importances = np.sum(np.abs(gradients), axis=0)\n",
    "    \n",
    "    return feature_importances\n",
    "\n",
    "# Compute the feature importances for a sample input\n",
    "x_sample = X_train[0:1, :, :]\n",
    "feature_importances = get_input_gradients(G1, x_sample)\n",
    "\n",
    "# Print the feature importances\n",
    "#print(\"Feature importances:\", feature_importances)\n",
    "\n",
    "# Compute the average feature importances across all timesteps\n",
    "average_importances = np.mean(feature_importances, axis=0)\n",
    "\n",
    "# Rank the features based on their importance\n",
    "sorted_indices = np.argsort(average_importances)[::-1]\n",
    "\n",
    "# Print the feature rankings\n",
    "names = list(df.columns)[:-1]\n",
    "for i in range(len(sorted_indices)):\n",
    "    print(f\"{i+1}. {names[sorted_indices[i]]}: {average_importances[sorted_indices[i]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f82b1434-7c37-459f-a17c-8212c08cb816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. PLoad: 0.0059575545601546764\n",
      "2. sol[W/m2]: 0.004594739060848951\n",
      "3. St: 0.003902588738128543\n",
      "4. EpiTemp: 0.003902561031281948\n",
      "5. HypTemp: 0.003791992086917162\n",
      "6. EpiV: 0.003101204987615347\n",
      "7. Therm: 0.0019482164643704891\n",
      "8. HypV: 0.0013939919881522655\n",
      "9. u2*[m/s]: 0.0013693601358681917\n"
     ]
    }
   ],
   "source": [
    "def get_input_gradients(model, x):\n",
    "    # Convert the input to a tensor\n",
    "    x = np.reshape(x, (x.shape[0], 100, 9))\n",
    "    x_tensor = tf.convert_to_tensor(x)\n",
    "    \n",
    "    # Compute the gradients of the output with respect to the input\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x_tensor)\n",
    "        output = model(x_tensor)\n",
    "        \n",
    "    gradients = tape.gradient(output, x_tensor)\n",
    "    \n",
    "    # Convert the gradients to a numpy array\n",
    "    gradients = tf.keras.backend.get_value(gradients)\n",
    "    \n",
    "    # Compute the absolute sum of the gradients for each feature\n",
    "    feature_importances = np.sum(np.abs(gradients), axis=0)\n",
    "    \n",
    "    return feature_importances\n",
    "\n",
    "# Compute the feature importances for a sample input\n",
    "x_sample = X_train[0:1, :, :]\n",
    "feature_importances = get_input_gradients(G2, x_sample)\n",
    "\n",
    "# Print the feature importances\n",
    "#print(\"Feature importances:\", feature_importances)\n",
    "\n",
    "# Compute the average feature importances across all timesteps\n",
    "average_importances = np.mean(feature_importances, axis=0)\n",
    "\n",
    "# Rank the features based on their importance\n",
    "sorted_indices = np.argsort(average_importances)[::-1]\n",
    "\n",
    "# Print the feature rankings\n",
    "names = list(df.columns)[:-1]\n",
    "for i in range(len(sorted_indices)):\n",
    "    print(f\"{i+1}. {names[sorted_indices[i]]}: {average_importances[sorted_indices[i]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792dccc7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.13%\n",
      "0.06462617963552475\n",
      "[0.16843971631205673, 0.13652482269503546, 0.04078014184397163, 0.10460992907801421, 0.008865248226950355, 0.04078014184397163, 0.04078014184397163, 0.10460992907801421, 0.008865248226950355, 0.10460992907801421, 0.0726950354609929, 0.16843971631205673]\n",
      "False\n",
      "78175,8\n",
      "#### Best ####\n",
      "94.87%\n",
      "0.06591259688138962\n",
      "[0.015151515151515152, 0.06969696969696969, 0.06969696969696969, 0.015151515151515152, 0.015151515151515152, 0.015151515151515152, 0.015151515151515152, 0.015151515151515152, 0.2333333333333333, 0.1787878787878788, 0.2333333333333333, 0.12424242424242422]\n"
     ]
    }
   ],
   "source": [
    "weights_range = np.arange(0.05, 1.1, 0.18)\n",
    "y_true = y_test\n",
    "def concatenate_data(X_train, y_train, X_test, y_test):\n",
    "    X = np.concatenate([X_train, X_test], axis=0)\n",
    "    y = np.concatenate([y_train, y_test], axis=0)\n",
    "    return X, y\n",
    "def Unreshape_df(Xs, ys=None):\n",
    "    X = np.concatenate(Xs, axis=0)\n",
    "    if ys is not None:\n",
    "        y = np.concatenate(ys, axis=0)\n",
    "        return X, y\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "        return np.sqrt(np.mean(np.square(y_pred - y_true)))  \n",
    "def calculate_weighted_average(predictions, weights):\n",
    "    return sum([weight*preds for preds, weight in zip(predictions, weights)])\n",
    "\n",
    "def evaluate_ensemble(predictions, weights, y_true):\n",
    "    pred = calculate_weighted_average(predictions, weights)\n",
    "    loss = rmse(y_true,pred)\n",
    "    return r2_score(y_true, pred),loss\n",
    "\n",
    "#for i in range(2):\n",
    " #(f'GRU{i+1}')#\n",
    "best_models = best_modelss\n",
    "predictions = []\n",
    "losses = []\n",
    "for model in best_models:\n",
    "    y_pred = model.predict(X_test)\n",
    "    loss = rmse(y_true,y_pred)\n",
    "    losses.append(loss)\n",
    "    predictions.append(y_pred)\n",
    "iterations = len(weights_range) ** len(best_models)\n",
    "best_weights = None\n",
    "best_score = None\n",
    "best_loss = None\n",
    "track= {}\n",
    "si = 0\n",
    "bi = 0\n",
    "found = lambda si,bi,best_score: si> 0 and bi> 0 and si > bi*10000#or (best_score is not None and best_score > 0.91)\n",
    "for wx0, w1 in enumerate(weights_range):\n",
    "    if found(si,bi,best_score):\n",
    "        wx0 = weights_range[len(weights_range)-1]\n",
    "        break\n",
    "    for wx1, w2 in enumerate(weights_range):\n",
    "        if found(si,bi,best_score):\n",
    "            wx1 = weights_range[len(weights_range)-1]\n",
    "            break\n",
    "        for wx2, w3 in enumerate(weights_range):\n",
    "            if found(si,bi,best_score):\n",
    "                wx2 = weights_range[len(weights_range)-1]\n",
    "                break\n",
    "            for wx3, w4 in enumerate(weights_range):\n",
    "                if found(si,bi,best_score):\n",
    "                    wx3 = weights_range[len(weights_range)-1]\n",
    "                    break\n",
    "                for wx5, w6 in enumerate(weights_range):\n",
    "                    if found(si,bi,best_score):\n",
    "                        wx5 = weights_range[len(weights_range)-1]\n",
    "                        break\n",
    "                    for wx6, w7 in enumerate(weights_range):\n",
    "                        if found(si,bi,best_score):\n",
    "                            wx6 = weights_range[len(weights_range)-1]\n",
    "                            break\n",
    "                        for wx7, w8 in enumerate(weights_range):\n",
    "                            if found(si,bi,best_score):\n",
    "                                wx7 = weights_range[len(weights_range)-1]\n",
    "                                break\n",
    "                            for wx8, w9 in enumerate(weights_range):\n",
    "                                if found(si,bi,best_score):\n",
    "                                    wx8 = weights_range[len(weights_range)-1]\n",
    "                                    break\n",
    "                                for wx9, w10 in enumerate(weights_range):\n",
    "                                    if found(si,bi,best_score):\n",
    "                                        wx9 = weights_range[len(weights_range)-1]\n",
    "                                        break\n",
    "                                    for wx10, w11 in enumerate(weights_range):\n",
    "                                        if found(si,bi,best_score):\n",
    "                                            wx10= weights_range[len(weights_range)-1]\n",
    "                                            break\n",
    "                                        for wx12, w13 in enumerate(weights_range):\n",
    "                                            if found(si,bi,best_score):\n",
    "                                                wx12 = weights_range[len(weights_range)-1]\n",
    "                                                break\n",
    "                                            if found(si,bi,best_score):\n",
    "                                                wx6 = weights_range[len(weights_range)-1]\n",
    "                                                clear_output(wait = True)\n",
    "                                                print(f'{best_score:.2%}')\n",
    "                                                print(f'{found(si,bi,best_score)}')\n",
    "                                                print(f'{si},{bi}')\n",
    "                                                break\n",
    "                                            else:\n",
    "                                                w = [random.choice(weights_range) for _ in range(12)]\n",
    "                                                weights = [nu/sum(w) for nu in w]\n",
    "                                                score = evaluate_ensemble(predictions, weights, y_true)[0]\n",
    "                                                s_loss = evaluate_ensemble(predictions, weights, y_true)[1]\n",
    "                                                si += 1\n",
    "                                                cond = (best_score is None or best_loss is None) or\\\n",
    "                                                       (np.round(score,4) > np.round(best_score,4) and np.round(s_loss, 2) < 0.12)\n",
    "                                                if cond:\n",
    "                                                    bi += 1\n",
    "                                                    best_score = score       \n",
    "                                                    best_weights = weights\n",
    "                                                    best_loss = s_loss\n",
    "                                                    track[bi] = {'Titer':si,'score':best_score , 'loss':best_loss}\n",
    "                                                    clear_output(wait = True)\n",
    "                                                    print(f'{best_loss}')\n",
    "                                                    print(f'{best_score:.2%}')\n",
    "                                                    print(f'{best_weights}')\n",
    "                                                    print(f'{found(si,bi,best_score)}')\n",
    "                                                    print(f'recent')\n",
    "\n",
    "                                                if si> 0 and bi> 0:\n",
    "                                                    clear_output(wait = True)\n",
    "                                                    print(f'{score:.2%}')\n",
    "                                                    print(f'{loss}')\n",
    "                                                    print(f'{weights}')\n",
    "                                                    print(f'{found(si,bi,best_score)}')\n",
    "                                                    print(f'{si},{bi}')\n",
    "                                                    print(f'#### Best ####')\n",
    "                                                    print(f'{best_score:.2%}')\n",
    "                                                    print(f'{best_loss}')\n",
    "                                                    print(f'{best_weights}')\n",
    "\n",
    "\n",
    "                                            if found(si,bi,best_score):\n",
    "                                                break\n",
    "                                        if found(si,bi,best_score):\n",
    "                                            break\n",
    "                                    if found(si,bi,best_score):\n",
    "                                        break\n",
    "                                if found(si,bi,best_score):\n",
    "                                    break\n",
    "                            if found(si,bi,best_score):\n",
    "                                break\n",
    "                        if found(si,bi,best_score):\n",
    "                            break\n",
    "                    if found(si,bi,best_score):\n",
    "                        break\n",
    "                if found(si,bi,best_score):\n",
    "                    break\n",
    "            if found(si,bi,best_score):\n",
    "                break\n",
    "        if found(si,bi,best_score):        \n",
    "            track[bi] = {'Titer':si,'score':best_score , 'loss':best_loss}\n",
    "            print(best_weights, best_score)\n",
    "            break\n",
    "                #if i == 0:\n",
    "                #    best_weights1 = best_weights\n",
    "                #else:\n",
    "                #    best_weights2 = best_weights\n",
    "                #best_w = [best_weights1,best_weights2]         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d53b4a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9486571402934604 0.3862703\n"
     ]
    }
   ],
   "source": [
    "def Ensemble(X_topredict,best_weights,best_models):\n",
    "    model_predictions = []\n",
    "    for model in best_models:\n",
    "        y_pred = model.predict(X_topredict).T[0]\n",
    "        model_predictions.append(y_pred)\n",
    "\n",
    "    # Combine the model predictions using the optimized weights to obtain the final prediction\n",
    "    final_prediction = calculate_weighted_average(model_predictions, best_weights)\n",
    "    return final_prediction\n",
    "\n",
    "pred = list(Ensemble(X_test,best_weights,best_modelss))\n",
    "clear_output()\n",
    "print(r2_score(y_true, pred),rmse(y_true,pred))\n",
    "#for i in range(2):\n",
    "#    print(f'GRU{i+1}')\n",
    "#    best_models = best_modelss[i]\n",
    "#    best_weights = best_w[i]\n",
    "#    exec(f'Ens_pred{i} = list(Ensemble(X_test,best_weights,best_models))')\n",
    "#    print(r2_score(y_true, eval(f'Ens_pred{i}')),rmse(y_true,eval(f'Ens_pred{i}')))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffee14cd-86ad-45f1-86c9-e2db9986f310",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. EpiV: 0.00637510218705329\n",
      "2. HypV: 0.0019807248936464066\n",
      "3. u2*[m/s]: 0.0016990603085086313\n",
      "4. sol[W/m2]: 0.0012254551064136187\n",
      "5. PLoad[g]: 0.0006424423296362717\n",
      "6. Qout[m3/d]: 0.00042358138383406953\n",
      "7. EpiTemp: 0.00019585197359176667\n",
      "8. St: 0.00015248619823943595\n",
      "9. HypTemp: 0.00013704249371558852\n"
     ]
    }
   ],
   "source": [
    "def get_input_gradients(model, x):\n",
    "    # Convert the input to a tensor\n",
    "    x = np.reshape(x, (x.shape[0], 100, 9))\n",
    "    x_tensor = tf.convert_to_tensor(x)\n",
    "    \n",
    "    # Compute the gradients of the output with respect to the input\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x_tensor)\n",
    "        output = model(x_tensor)\n",
    "        \n",
    "    gradients = tape.gradient(output, x_tensor)\n",
    "    \n",
    "    # Convert the gradients to a numpy array\n",
    "    gradients = tf.keras.backend.get_value(gradients)\n",
    "    \n",
    "    # Compute the absolute sum of the gradients for each feature\n",
    "    feature_importances = np.sum(np.abs(gradients), axis=0)\n",
    "    \n",
    "    return feature_importances\n",
    "\n",
    "# Compute the feature importances for a sample input\n",
    "x_sample = X_train[0:1, :, :]\n",
    "feature_importances = get_input_gradients(model0_1, x_sample)\n",
    "\n",
    "# Print the feature importances\n",
    "#print(\"Feature importances:\", feature_importances)\n",
    "\n",
    "# Compute the average feature importances across all timesteps\n",
    "average_importances = np.mean(feature_importances, axis=0)\n",
    "\n",
    "# Rank the features based on their importance\n",
    "sorted_indices = np.argsort(average_importances)[::-1]\n",
    "\n",
    "# Print the feature rankings\n",
    "names = list(df.columns)[:-1]\n",
    "for i in range(len(sorted_indices)):\n",
    "    print(f\"{i+1}. {names[sorted_indices[i]]}: {average_importances[sorted_indices[i]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "043a679e-4904-49be-b090-255fce153d73",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 12s 37ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 12s 34ms/step\n",
      "338/338 [==============================] - 11s 34ms/step\n",
      "338/338 [==============================] - 11s 34ms/step\n",
      "338/338 [==============================] - 7s 20ms/step\n",
      "338/338 [==============================] - 6s 19ms/step\n",
      "338/338 [==============================] - 7s 19ms/step\n",
      "338/338 [==============================] - 6s 19ms/step\n",
      "338/338 [==============================] - 7s 19ms/step\n",
      "338/338 [==============================] - 7s 19ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 12s 34ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 13s 39ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 40ms/step\n",
      "338/338 [==============================] - 14s 40ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 11s 33ms/step\n",
      "338/338 [==============================] - 7s 19ms/step\n",
      "338/338 [==============================] - 7s 19ms/step\n",
      "338/338 [==============================] - 7s 20ms/step\n",
      "338/338 [==============================] - 7s 20ms/step\n",
      "338/338 [==============================] - 7s 20ms/step\n",
      "338/338 [==============================] - 7s 20ms/step\n",
      "338/338 [==============================] - 11s 33ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 7s 20ms/step\n",
      "338/338 [==============================] - 7s 20ms/step\n",
      "338/338 [==============================] - 7s 20ms/step\n",
      "338/338 [==============================] - 7s 20ms/step\n",
      "338/338 [==============================] - 7s 20ms/step\n",
      "338/338 [==============================] - 7s 20ms/step\n",
      "338/338 [==============================] - 13s 39ms/step\n",
      "338/338 [==============================] - 17s 51ms/step\n",
      "338/338 [==============================] - 17s 49ms/step\n",
      "338/338 [==============================] - 16s 48ms/step\n",
      "338/338 [==============================] - 16s 47ms/step\n",
      "338/338 [==============================] - 17s 51ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 8s 25ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 8s 25ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 14s 40ms/step\n",
      "338/338 [==============================] - 14s 40ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 8s 25ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 8s 25ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 8s 25ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 43ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 8s 25ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 8s 25ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 8s 25ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 18s 52ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 17s 50ms/step\n",
      "338/338 [==============================] - 17s 50ms/step\n",
      "338/338 [==============================] - 18s 52ms/step\n",
      "338/338 [==============================] - 18s 52ms/step\n",
      "338/338 [==============================] - 18s 52ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 11s 31ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 14s 43ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 8s 25ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 8s 25ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 43ms/step\n",
      "338/338 [==============================] - 18s 52ms/step\n",
      "338/338 [==============================] - 17s 49ms/step\n",
      "338/338 [==============================] - 18s 52ms/step\n",
      "338/338 [==============================] - 18s 52ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 11s 33ms/step\n",
      "338/338 [==============================] - 11s 33ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 14s 43ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 14s 43ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 8s 25ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 14s 43ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 43ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 16s 48ms/step\n",
      "338/338 [==============================] - 17s 50ms/step\n",
      "338/338 [==============================] - 17s 51ms/step\n",
      "338/338 [==============================] - 17s 51ms/step\n",
      "338/338 [==============================] - 10s 31ms/step\n",
      "338/338 [==============================] - 11s 31ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 11s 31ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 43ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 14s 43ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 14s 43ms/step\n",
      "338/338 [==============================] - 14s 43ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 43ms/step\n",
      "338/338 [==============================] - 14s 43ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 14s 43ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 14s 43ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 16s 47ms/step\n",
      "338/338 [==============================] - 17s 51ms/step\n",
      "338/338 [==============================] - 17s 51ms/step\n",
      "338/338 [==============================] - 10s 31ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 17s 50ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 14s 43ms/step\n",
      "338/338 [==============================] - 14s 43ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 18s 53ms/step\n",
      "338/338 [==============================] - 18s 53ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 11s 33ms/step\n",
      "338/338 [==============================] - 11s 33ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 11s 33ms/step\n",
      "338/338 [==============================] - 11s 33ms/step\n",
      "338/338 [==============================] - 18s 52ms/step\n",
      "338/338 [==============================] - 16s 47ms/step\n",
      "338/338 [==============================] - 16s 47ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 16s 47ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 16s 47ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 18s 53ms/step\n",
      "338/338 [==============================] - 18s 53ms/step\n",
      "338/338 [==============================] - 18s 53ms/step\n",
      "338/338 [==============================] - 11s 33ms/step\n",
      "338/338 [==============================] - 11s 31ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 11s 31ms/step\n",
      "338/338 [==============================] - 11s 33ms/step\n",
      "338/338 [==============================] - 11s 31ms/step\n",
      "338/338 [==============================] - 16s 48ms/step\n",
      "338/338 [==============================] - 16s 47ms/step\n",
      "338/338 [==============================] - 16s 47ms/step\n",
      "338/338 [==============================] - 16s 47ms/step\n",
      "338/338 [==============================] - 16s 47ms/step\n",
      "338/338 [==============================] - 16s 47ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 13s 39ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 16s 47ms/step\n",
      "338/338 [==============================] - 16s 47ms/step\n",
      "338/338 [==============================] - 10s 30ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 10s 31ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 13s 37ms/step\n",
      "338/338 [==============================] - 14s 40ms/step\n",
      "338/338 [==============================] - 14s 40ms/step\n",
      "338/338 [==============================] - 13s 37ms/step\n",
      "338/338 [==============================] - 17s 50ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 15s 43ms/step\n",
      "338/338 [==============================] - 18s 53ms/step\n",
      "338/338 [==============================] - 18s 52ms/step\n",
      "338/338 [==============================] - 18s 52ms/step\n",
      "338/338 [==============================] - 16s 48ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 10s 30ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 16s 47ms/step\n",
      "338/338 [==============================] - 18s 52ms/step\n",
      "338/338 [==============================] - 19s 55ms/step\n",
      "338/338 [==============================] - 20s 59ms/step\n",
      "338/338 [==============================] - 20s 59ms/step\n",
      "338/338 [==============================] - 19s 57ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 11s 34ms/step\n",
      "338/338 [==============================] - 12s 34ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 11s 32ms/step\n",
      "338/338 [==============================] - 10s 30ms/step\n",
      "338/338 [==============================] - 17s 50ms/step\n",
      "338/338 [==============================] - 17s 50ms/step\n",
      "338/338 [==============================] - 17s 49ms/step\n",
      "338/338 [==============================] - 16s 49ms/step\n",
      "338/338 [==============================] - 16s 48ms/step\n",
      "338/338 [==============================] - 17s 49ms/step\n",
      "338/338 [==============================] - 10s 30ms/step\n",
      "338/338 [==============================] - 10s 30ms/step\n",
      "338/338 [==============================] - 10s 30ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 16s 47ms/step\n",
      "338/338 [==============================] - 16s 47ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 40ms/step\n",
      "338/338 [==============================] - 14s 40ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 8s 25ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 25ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 14s 40ms/step\n",
      "338/338 [==============================] - 14s 40ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 40ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 13s 39ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 17s 50ms/step\n",
      "338/338 [==============================] - 15s 46ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 8s 25ms/step\n",
      "338/338 [==============================] - 13s 40ms/step\n",
      "338/338 [==============================] - 13s 39ms/step\n",
      "338/338 [==============================] - 13s 39ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 39ms/step\n",
      "338/338 [==============================] - 13s 37ms/step\n",
      "338/338 [==============================] - 8s 22ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 7s 21ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 12s 37ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 7s 21ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 12s 37ms/step\n",
      "338/338 [==============================] - 12s 37ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 8s 22ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 22ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 7s 21ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 8s 22ms/step\n",
      "338/338 [==============================] - 8s 22ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 15s 45ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 13s 40ms/step\n",
      "338/338 [==============================] - 13s 39ms/step\n",
      "338/338 [==============================] - 13s 39ms/step\n",
      "338/338 [==============================] - 12s 37ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 13s 37ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 7s 21ms/step\n",
      "338/338 [==============================] - 7s 21ms/step\n",
      "338/338 [==============================] - 7s 21ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 8s 22ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 12s 37ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 7s 21ms/step\n",
      "338/338 [==============================] - 7s 21ms/step\n",
      "338/338 [==============================] - 7s 21ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 8s 22ms/step\n",
      "338/338 [==============================] - 7s 21ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 12s 37ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 7s 21ms/step\n",
      "338/338 [==============================] - 7s 21ms/step\n",
      "338/338 [==============================] - 7s 21ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 13s 37ms/step\n",
      "338/338 [==============================] - 13s 40ms/step\n",
      "338/338 [==============================] - 12s 36ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 7s 21ms/step\n",
      "338/338 [==============================] - 7s 21ms/step\n",
      "338/338 [==============================] - 7s 21ms/step\n",
      "338/338 [==============================] - 7s 21ms/step\n",
      "338/338 [==============================] - 7s 21ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 12s 34ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 12s 34ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 12s 35ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 7s 22ms/step\n",
      "338/338 [==============================] - 7s 21ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 15s 44ms/step\n",
      "338/338 [==============================] - 16s 49ms/step\n",
      "338/338 [==============================] - 20s 58ms/step\n",
      "338/338 [==============================] - 19s 57ms/step\n",
      "338/338 [==============================] - 16s 48ms/step\n",
      "338/338 [==============================] - 14s 42ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 8s 25ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 14s 40ms/step\n",
      "338/338 [==============================] - 13s 39ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 39ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 39ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 39ms/step\n",
      "338/338 [==============================] - 9s 25ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 37ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 37ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 37ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 12s 37ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 8s 22ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 22ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 12s 37ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 16s 46ms/step\n",
      "338/338 [==============================] - 16s 48ms/step\n",
      "338/338 [==============================] - 16s 48ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 10s 31ms/step\n",
      "338/338 [==============================] - 10s 29ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 9s 27ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 14s 40ms/step\n",
      "338/338 [==============================] - 14s 41ms/step\n",
      "338/338 [==============================] - 13s 39ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 13s 37ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 37ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 12s 37ms/step\n",
      "338/338 [==============================] - 13s 37ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 13s 40ms/step\n",
      "338/338 [==============================] - 13s 39ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 37ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 22ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 37ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 37ms/step\n",
      "338/338 [==============================] - 13s 39ms/step\n",
      "338/338 [==============================] - 13s 37ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 37ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 13s 37ms/step\n",
      "338/338 [==============================] - 13s 38ms/step\n",
      "338/338 [==============================] - 12s 37ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 8s 24ms/step\n",
      "338/338 [==============================] - 8s 23ms/step\n",
      "338/338 [==============================] - 9s 26ms/step\n",
      "338/338 [==============================] - 10s 28ms/step\n",
      "338/338 [==============================] - 9s 28ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_1188\\1884792773.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  feature_importances = feature_importances / np.sum(feature_importances)\n"
     ]
    }
   ],
   "source": [
    "def compute_feature_importances(models, weights, X, y):\n",
    "    \"\"\"\n",
    "    Computes the feature importances using the representation erasure approach for an ensemble of models.\n",
    "    \n",
    "    Args:\n",
    "        models (list): A list of TensorFlow or Keras models.\n",
    "        weights (list): A list of floating point numbers representing the weights of each model in the ensemble.\n",
    "        X (numpy.ndarray): An array of shape (n_samples, n_features) containing the input data.\n",
    "        y (numpy.ndarray): An array of shape (n_samples,) containing the target labels.\n",
    "    \n",
    "    Returns:\n",
    "        A numpy.ndarray of shape (n_features,) containing the feature importances.\n",
    "    \"\"\"\n",
    "    baseline_score = 0\n",
    "    for i, model in enumerate(models):\n",
    "        pred = model.predict(X)\n",
    "        score = rmse(y,pred)\n",
    "        baseline_score += score * weights[i]\n",
    "    baseline_score /= np.sum(weights)\n",
    "\n",
    "    feature_importances = np.zeros(X.shape[1])\n",
    "    clear_output()\n",
    "    for i in range(X.shape[1]):\n",
    "        X_test = np.copy(X)\n",
    "        X_test[:, i] = 0\n",
    "        score = 0\n",
    "        for j, model in enumerate(models):\n",
    "            pred = model.predict(X)\n",
    "            score +=  rmse(y,pred) * weights[j]\n",
    "        feature_importances[i] = np.abs(baseline_score - score)\n",
    "\n",
    "    # Normalize the feature importances\n",
    "    feature_importances = feature_importances / np.sum(feature_importances)\n",
    "    \n",
    "    return feature_importances\n",
    "best_weights = [0.015151515151515152, 0.06969696969696969, 0.06969696969696969, 0.015151515151515152, \n",
    "                0.015151515151515152, 0.015151515151515152, 0.015151515151515152, 0.015151515151515152, \n",
    "                0.2333333333333333, 0.1787878787878788, 0.2333333333333333, 0.12424242424242422]\n",
    "fi = compute_feature_importances(best_models, best_weights, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8246c6c9-ffb0-4215-b63b-b274dca17b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights = [0.6,0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f451d3a-f928-489f-b0b3-d054ce448055",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407/407 [==============================] - 11s 28ms/step\n",
      "407/407 [==============================] - 7s 18ms/step\n",
      "407/407 [==============================] - 12s 29ms/step\n",
      "407/407 [==============================] - 7s 18ms/step\n",
      "407/407 [==============================] - 12s 28ms/step\n",
      "407/407 [==============================] - 7s 18ms/step\n",
      "407/407 [==============================] - 12s 30ms/step\n",
      "407/407 [==============================] - 8s 19ms/step\n",
      "407/407 [==============================] - 13s 31ms/step\n",
      "407/407 [==============================] - 8s 19ms/step\n",
      "407/407 [==============================] - 13s 31ms/step\n",
      "407/407 [==============================] - 8s 19ms/step\n",
      "407/407 [==============================] - 13s 31ms/step\n",
      "407/407 [==============================] - 8s 20ms/step\n",
      "407/407 [==============================] - 13s 32ms/step\n",
      "407/407 [==============================] - 8s 20ms/step\n",
      "407/407 [==============================] - 13s 32ms/step\n",
      "407/407 [==============================] - 8s 20ms/step\n",
      "407/407 [==============================] - 13s 33ms/step\n",
      "407/407 [==============================] - 8s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "feature_importances = np.zeros(X.shape[2])\n",
    "clear_output()\n",
    "best_weights = [0.6,0.4]\n",
    "best_models = [G01,G02]\n",
    "\n",
    "baseline_score = 0\n",
    "for i, model in enumerate(best_models):\n",
    "    pred = model.predict(X)\n",
    "    score = np.array(rmse(y,pred))\n",
    "    baseline_score += score * best_weights[i]\n",
    "baseline_score /= np.sum(best_weights)\n",
    "for i in range(X.shape[2]):\n",
    "    X_test = np.copy(X)\n",
    "    X_test[:,:,i] = 0\n",
    "    score = 0\n",
    "    for j, model in enumerate(best_models):\n",
    "        pred = model.predict(X_test)\n",
    "        score +=  rmse(y,pred) * best_weights[j]\n",
    "    feature_importances[i] = np.abs(baseline_score - score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6ec4d79-2759-4333-a8ce-d91ccd63587b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. EpiV : 0.06586500257253647\n",
      "2. HypTemp : 0.01964396983385086\n",
      "3. sol[W/m2] : 0.004967309534549713\n",
      "4. EpiTemp : 0.004275105893611908\n",
      "5. HypV : 0.004015378654003143\n",
      "6. u2*[m/s] : 0.003573976457118988\n",
      "7. St : 0.0027765557169914246\n",
      "8. Qout : 0.001466907560825348\n",
      "9. PLoad : 2.3506581783294678e-05\n"
     ]
    }
   ],
   "source": [
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "names = list(df.columns)[:-1]\n",
    "nm = {}\n",
    "for i in range(len(sorted_indices)):\n",
    "    nm[names[sorted_indices[i]]] = {'value':feature_importances[sorted_indices[i]],\n",
    "                                    'N':int(i+1)}\n",
    "    print(f\"{i+1}. {names[sorted_indices[i]]} : {feature_importances[sorted_indices[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f054a54d-f9d5-4d53-8428-34c9f4389788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.513\n",
      "0.172\n",
      "0.131\n",
      "0.097\n",
      "0.035\n",
      "0.022\n",
      "0.021\n",
      "0.009\n",
      "0.001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FI = pd.DataFrame(nm).T\n",
    "FI['value'] = np.round(FI['value'] / np.sum(FI['value']),3)\n",
    "[print(x) for x in FI.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6db9c385-d3c7-4e93-87ed-f17514432c4c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_input_gradients' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m dic_importance \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ix, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(best_models):\n\u001b[1;32m----> 3\u001b[0m     feature_importances \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_gradients\u001b[49m(model, x_sample)\n\u001b[0;32m      4\u001b[0m     average_importances \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(feature_importances, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      5\u001b[0m     sorted_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(average_importances)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_input_gradients' is not defined"
     ]
    }
   ],
   "source": [
    "dic_importance = {}\n",
    "for ix, model in enumerate(best_models):\n",
    "    feature_importances = get_input_gradients(model, x_sample)\n",
    "    average_importances = np.mean(feature_importances, axis=0)\n",
    "    sorted_indices = np.argsort(average_importances)[::-1]\n",
    "    feature_rankings = {names[sorted_indices[i]]: average_importances[sorted_indices[i]] for i in range(len(sorted_indices))}\n",
    "    dic_importance[ix] = {'importance':feature_rankings}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "69d66a26-cb59-43bf-9014-e4fd578182b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {}\n",
    "for _, sub_dict in dic_importance.items():\n",
    "    for key, value in subdict['importance'].items():\n",
    "        if key not in result:\n",
    "            result[key] = []\n",
    "        result[key].append(value)\n",
    "\n",
    "for key in result:\n",
    "    result[key] =sum(w * v for w, v in zip(best_weights, result[key])) / sum(best_weights)\n",
    "result = dict(sorted(result.items(), key=lambda item: item[1],reverse=True))\n",
    "result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e41777",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = scaler.inverse_transform(Ensemble(X_test,best_weights,best_modelss).reshape(-1, 1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6474908f-56a5-4a87-8a9a-d2e7c97aa0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ad8aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 2s 49ms/step\n",
      "38/38 [==============================] - 1s 29ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = scaler.inverse_transform(Ensemble(X,best_weights,best_models).reshape(-1, 1)).T\n",
    "with open('RNNpred.pkl', 'wb') as f:\n",
    "    pickle.dump(pred[0], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d14cc5c7-e4f1-48ab-a49e-aecc26b31155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 3s 88ms/step\n",
      "38/38 [==============================] - 2s 62ms/step\n"
     ]
    }
   ],
   "source": [
    "p =Ensemble(X,best_weights,best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "49941b8b-bd1c-4cfc-8a0f-ff6bf73e1dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1205,)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06725d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [Ens_pred0]\n",
    "predictions.append(Ens_pred1)\n",
    "#losses = [rmse(y_true,Ens_pred0),rmse(y_true,Ens_pred1)]\n",
    "iterations = len(weights_range) ** 2\n",
    "best_weights = None\n",
    "best_score = None\n",
    "best_loss = None\n",
    "track= {}\n",
    "si = 0\n",
    "bi = 0\n",
    "found = lambda si,bi,best_score: si> 0 and bi> 0 and si > bi*10000 or best_score is not None and best_score > 0.91\n",
    "    \n",
    "for wx0, w1 in enumerate(weights_range):    \n",
    "    if found(si,bi,best_score):\n",
    "        wx0 = weights_range[len(weights_range)-1]\n",
    "        break\n",
    "    for wx1, w2 in enumerate(weights_range):\n",
    "        if found(si,bi,best_score):\n",
    "            wx1 = weights_range[len(weights_range)-1]\n",
    "            clear_output(wait = True)\n",
    "            print(f'{best_score:.2%}')\n",
    "            print(f'{found(si,bi,best_score)}')\n",
    "            print(f'{si},{bi}')\n",
    "            break\n",
    "        else:\n",
    "            w = [random.choice(weights_range) for _ in range(2)]\n",
    "            weights = [nu/sum(w) for nu in w]\n",
    "            score = evaluate_ensemble(predictions, weights, y_true)[0]\n",
    "            s_loss = evaluate_ensemble(predictions, weights, y_true)[1]\n",
    "            si += 1\n",
    "            cond = (best_score is None or best_loss is None) or (np.round(score,4) > np.round(best_score,4) and np.round(s_loss, 2) < 0.12)\n",
    "            if cond:\n",
    "                bi += 1\n",
    "                best_score = score       \n",
    "                best_weights = weights\n",
    "                if best_loss is None:\n",
    "                    best_loss = s_loss\n",
    "                track[bi] = {'Titer':si,'score':best_score , 'loss':best_loss}\n",
    "                if (np.round(s_loss,2) < np.round(best_loss, 2)):\n",
    "                    if not cond:\n",
    "                        bi += 1\n",
    "                    best_score = score \n",
    "                    best_loss = s_loss\n",
    "                    best_weights = weights        \n",
    "                    track[bi] = {'Titer':si,'score':best_score , 'loss':best_loss}\n",
    "                clear_output(wait = True)\n",
    "                print(f'{best_loss}')\n",
    "                print(f'{best_score:.2%}')\n",
    "                print(f'{best_weights}')\n",
    "                print(f'{found(si,bi,best_score)}')\n",
    "                print(f'recent')\n",
    "\n",
    "            if si> 0 and bi> 0:\n",
    "                clear_output(wait = True)\n",
    "                print(f'{score:.2%}')\n",
    "                print(f'{loss}')\n",
    "                print(f'{weights}')\n",
    "                print(f'{found(si,bi,best_score)}')\n",
    "                print(f'{si},{bi}')\n",
    "                print(f'#### Best ####')\n",
    "                print(f'{best_score:.2%}')\n",
    "                print(f'{best_loss}')\n",
    "                print(f'{best_weights}')\n",
    "\n",
    "            if found(si,bi,best_score):\n",
    "                wx1 = weights_range[len(weights_range)-1]\n",
    "                break\n",
    "    if found(si,bi,best_score):\n",
    "        track[bi] = {'Titer':si,'score':best_score , 'loss':best_loss}\n",
    "        print(best_weights, best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac17aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = []\n",
    "loss = []\n",
    "for i in list(range(0,7))[1:]:\n",
    "    exec(f'model{i} = load_model(\"GRU/gru_{i}_fold.ckpt\",dic)')\n",
    "    print(f'fold{i}')\n",
    "    exec(f'pred{i} = model{i}.predict(X_test)')\n",
    "    r = eval(f'r2_score(pred{i},y_test)');r2.append(r)\n",
    "    eval(f'loss.append(rmse(pred{i},y_test))')\n",
    "clear_output()\n",
    "for i in list(range(0,6)):\n",
    "    print(i)\n",
    "    print(f'Fold:{i+1}, r2:{r2[i]}, rmse: {loss[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4216df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {'r_squared':r_squared,'rmse':rmse}\n",
    "\n",
    "r2 = []\n",
    "loss = []\n",
    "for i in list(range(0,7))[1:]:\n",
    "    exec(f'model{i} = load_model(\"GRU/gru_{i}_fold.ckpt\",dic)')\n",
    "    print(f'fold{i}')\n",
    "    exec(f'pred{i} = model{i}.predict(X_test)')\n",
    "    r = eval(f'r2_score(pred{i},y_test)');r2.append(r)\n",
    "    eval(f'loss.append(rmse(pred{i},y_test))')\n",
    "clear_output()\n",
    "for i in list(range(0,6)):\n",
    "    print(i)\n",
    "    print(f'Fold:{i+1}, r2:{r2[i]}, rmse: {loss[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06787b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r2_score(y,Ens_pred),rmse(y,Ens_pred)\n",
    "r2_score(y_true,Ens_pred),rmse(y_true,Ens_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea8a2bc-8ba6-4f99-888c-d1b94c7ec696",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa3264f-65ad-4b51-839f-bce457318499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.callbacks import DeltaYStopper\n",
    "n_splits = 6\n",
    "kfold = KFold(n_splits=n_splits, shuffle=False)\n",
    "\n",
    "# Define the search space\n",
    "search_space = [Categorical([10,20,40], name='batch_size'),\n",
    "                Categorical([10], name='epochs'),\n",
    "                Real(1e-6, 1e-3 ,'log-uniform',name='learning_rate'),\n",
    "                Categorical(['relu', 'tanh','selu','elu'], name='activation'),\n",
    "                Real(0.0, 0.5, name='dropout_rate'),\n",
    "                Categorical(['Adam', 'SGD','rmsprop'], name='optimizer'),\n",
    "                Integer(50, 200, name='num_units'),\n",
    "                Integer(1, 3, name='num_layers'),\n",
    "                Real(0.1,0.9,name='lambda1'), \n",
    "                Real(0.1,0.9,name='lambda2')]\n",
    "\n",
    "@use_named_args(search_space)\n",
    "def objective(**params):\n",
    "    # Unpack the hyperparameters\n",
    "    batch_size = params['batch_size']\n",
    "    epochs = params['epochs']\n",
    "    learning_rate = params['learning_rate']\n",
    "    activation = params['activation']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    optimizer = params['optimizer']\n",
    "    num_units = params['num_units']\n",
    "    num_layers = params['num_layers']\n",
    "    \n",
    "    if optimizer == 'Adam':\n",
    "        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'SGD':\n",
    "        opt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    \n",
    "    GRUmodel = Sequential()\n",
    "    GRUmodel.add(Input(shape=(X_train.shape[1],X_train.shape[2]), name='input_layer'))\n",
    "    for i in range(num_layers):\n",
    "        if i == num_layers - 1:\n",
    "            GRUmodel.add(GRU(units=num_units,\n",
    "                           activation=activation,\n",
    "                           dropout=dropout_rate,\n",
    "                           return_sequences=False,\n",
    "                           name=f'Hidden_layer_{i+1}'))\n",
    "        else:\n",
    "            # Not the last GRU layer\n",
    "            GRUmodel.add(GRU(units=num_units,\n",
    "                           activation=activation,\n",
    "                           dropout=dropout_rate,\n",
    "                           return_sequences=True,\n",
    "                           name=f'Hidden_layer_{i+1}'))\n",
    "    GRUmodel.add(Dense(1))\n",
    "    GRUmodel.compile(loss=rmse, optimizer= opt, metrics=[r_squared])\n",
    "    \n",
    "    print(f)\n",
    "    # Train the model on the training data for this fold\n",
    "    history = GRUmodel.fit(X_train_kfold, y_train_kfold,\n",
    "                           batch_size=batch_size,\n",
    "                           epochs=epochs)\n",
    "    \n",
    "    # Evaluate the model on the validation data for this fold\n",
    "    val_loss, val_r_squared = GRUmodel.evaluate(X_val_kfold, y_val_kfold)\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "delta_stopper = DeltaYStopper(delta=0.1)\n",
    "for train_index, val_index in kfold.split(X_train, y_train):\n",
    "    X_train_kfold, X_val_kfold = X_train[train_index], X_train[val_index]\n",
    "    y_train_kfold, y_val_kfold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    # Perform Bayesian optimization on the training data for this fold\n",
    "    res_gp = gp_minimize(objective,\n",
    "                         search_space,\n",
    "                         n_calls=15,\n",
    "                         callback=[delta_stopper],\n",
    "                         random_state=42)\n",
    "    \n",
    "    # Print the best score and best parameters for this fold\n",
    "    print(f\"Best score for fold {i}: {res_gp.fun} using {res_gp.x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9882efd4-c99e-42a7-92d8-b51326fca56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "r2_score(y_test,pred),rmse(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727dff0e-4a5b-4b12-8882-b775cbc181a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Unreshape_df(Xs, ys=None):\n",
    "    X = np.concatenate(Xs, axis=0)\n",
    "    if ys is not None:\n",
    "        y = np.concatenate(ys, axis=0)\n",
    "        return X, y\n",
    "    else:\n",
    "        return X\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd441c5-5ddc-4c74-8baa-bac4e77785f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(unpred.reshape(-1, 1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a54ee7-59bb-47ed-bfce-e16967d87581",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpred = Unreshape_df(Xs = pred)\n",
    "predicted = scaler.inverse_transform(unpred.reshape(-1, 1)).T[0]\n",
    "observed =  scaler.inverse_transform(Unreshape_df(Xs = y_test).reshape(-1, 1)).T[0]\n",
    "r2_score(predicted,observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70b8ab-b610-462e-9e5a-81cc45bf60ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "optimizers = {'Adam':Adam, 'SGD':SGD, 'RMSprop':RMSprop, 'Adamax':Adamax, 'Nadam':Nadam}\n",
    "activation_functions = ['selu','elu',LeakyReLU,'relu'] #()\n",
    "\n",
    "# Define the hyperparameter search space:\n",
    "pbounds = {'dropout_rate': (0.1, 0.5),\n",
    "           'num_neurons': (8,256),\n",
    "           'learning_rate': (-7, -2),\n",
    "           'activation_function': (0, len(activation_functions)-1),\n",
    "           'optimizer': (0, len(optimizers)-1),\n",
    "           'num_hidden_layers': (1, 3),\n",
    "           'epochs': (5,100),\n",
    "           'batch_size': (8, 128)\n",
    "          }\n",
    "\n",
    "# Define a function to train and evaluate the RNN model for a given set of hyperparameters:\n",
    "def rnn_evaluate(epochs,batch_size,dropout_rate, num_neurons, learning_rate, activation_function, optimizer, num_hidden_layers):\n",
    "    \n",
    "    # Convert integer indices back to hyperparameters:\n",
    "    activation_function = activation_functions[int(activation_function)]\n",
    "    optimizer = list(optimizers.values())[int(optimizer)](learning_rate=learning_rate)\n",
    "    \n",
    "    # Perform 6-fold cross validation:\n",
    "    num_folds = 6\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "    acc_list = []\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "    for train_idx, test_idx in kf.split(X_train,y_train):\n",
    "        X_Train, X_val = X[train_idx], X[test_idx]\n",
    "        y_Train, y_val = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Define and train the RNN model:\n",
    "        model = Sequential()\n",
    "        model.add(SimpleRNN(units=int(num_neurons), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        for i in range(int(num_hidden_layers)-1):\n",
    "            model.add(Dense(num_neurons, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss=rmse, optimizer=optimizer, metrics=[r_squared])\n",
    "        #model.fit(X_Train, y_Train, epochs=10, batch_size=32,validation_data=(X_val, y_val), verbose=0)\n",
    "        #model.fit(X_Train, y_Train, epochs=int(epochs), batch_size= int(batch_size),validation_data=(X_val, y_val),verbose=0)\n",
    "        history = model.fit(X_Train, y_Train, epochs=int(epochs), batch_size=int(batch_size),\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=0, callbacks=[es])   \n",
    "        \n",
    "        y_pred = scaler.inverse_transform(model.predict(X_test))\n",
    "        mse_v = mean_squared_error(scaler.inverse_transform(y_test), y_pred)\n",
    "        rmse_v = np.sqrt(mse_v)\n",
    "        acc_list.append(rmse_v)\n",
    "    \n",
    "    # Return the mean accuracy over all folds as the performance metric of interest:\n",
    "    return sum(acc_list) / len(acc_list)\n",
    "\n",
    "# Perform Bayesian optimization to find the optimal hyperparameters:\n",
    "optimizer = BayesianOptimization(f=rnn_evaluate, pbounds=pbounds, random_state=42)\n",
    "optimizer.maximize(init_points=5, n_iter=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a04b8-25ef-4cf3-8835-4bfc5ca228cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimizer.max)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c152de-42f5-479b-b0b4-44b6196aa474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, LSTM\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the optimal hyperparameters found by the optimizer\n",
    "\n",
    "def RNN_PLGM(X,param_dict,s = 0.8309):\n",
    "    # Convert integer indices back to hyperparameters:\n",
    "    optimizers = {'Adam':Adam, 'SGD':SGD, 'RMSprop':RMSprop, 'Adamax':Adamax, 'Nadam':Nadam}\n",
    "    activation_functions = ['selu','elu',LeakyReLU,'relu'] \n",
    "    \n",
    "    my_dict= param_dict    \n",
    "    batch_size = int(my_dict['batch_size'])\n",
    "    dropout_rate = my_dict['dropout_rate']\n",
    "    epochs = int(my_dict['epochs'])\n",
    "    learning_rate = my_dict['learning_rate']\n",
    "    num_hidden_layers = my_dict['num_hidden_layers']\n",
    "    num_neurons = my_dict['num_neurons']\n",
    "    activation_function = activation_functions[int(my_dict['activation_function'])]\n",
    "    optimizer = list(optimizers.values())[int(my_dict['optimizer'])](learning_rate=learning_rate)\n",
    "    \n",
    "    split = int((s)*X.shape[0])\n",
    "    X_train = X[:split,:,:]\n",
    "    y_train = y[:split,:]\n",
    "    X_test =  X[split:,:,:]\n",
    "    y_test =  y[split:,:]\n",
    "    \n",
    "    # Define and train the RNN model:\n",
    "    RNN = Sequential()\n",
    "    RNN.add(SimpleRNN(units=int(num_neurons), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    for i in range(int(num_hidden_layers)-1):\n",
    "        RNN.add(Dense(num_neurons, activation=activation_function))\n",
    "        RNN.add(Dropout(dropout_rate))\n",
    "    RNN.add(Dense(1, activation='sigmoid'))\n",
    "    RNN.compile(loss=rmse, optimizer=optimizer, metrics=[r_squared])\n",
    "    # Train the model with your data\n",
    "    RNN.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "    # Make predictions on new data\n",
    "    y_pred = RNN.predict(X_test)\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78466c47-e8be-4fab-94bb-cc52e6bcbbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimal hyperparameters obtained from optimization\n",
    "# Define and compile the RNN model with optimal hyperparameters\n",
    "param_dict = my_dict \n",
    "batch_size = int(my_dict['batch_size'])\n",
    "dropout_rate = my_dict['dropout_rate']\n",
    "epochs = int(my_dict['epochs'])\n",
    "learning_rate = my_dict['learning_rate']\n",
    "num_hidden_layers = my_dict['num_hidden_layers']\n",
    "num_neurons = my_dict['num_neurons']\n",
    "activation_function = activation_functions[int(my_dict['activation_function'])]\n",
    "optimizer = list(optimizers.values())[int(my_dict['optimizer'])](learning_rate=learning_rate)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=int(num_neurons), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "for i in range(int(num_hidden_layers)-1):\n",
    "    model.add(Dense(num_neurons, activation=activation_function))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss=rmse, optimizer=optimizer, metrics=[r_squared])\n",
    "\n",
    "# Train the model on the entire training set with optimal hyperparameters\n",
    "history = model.fit(X_train, y_train, epochs=int(epochs), batch_size=int(batch_size),\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = scaler.inverse_transform(model.predict(X_test))\n",
    "mse_v = mean_squared_error(scaler.inverse_transform((y_test), y_pred)\n",
    "rmse_v = np.sqrt(mse_v)\n",
    "r2 = r2_score(scaler.inverse_transform((y_test), y_pred)\n",
    "print('RMSE:', rmse_v,'R2',r2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b106e299-33db-4e8a-9a2c-5cedff83a729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = scaler.inverse_transform(model.predict(X_test))\n",
    "#X_train: (13006, 100, 15)\n",
    "#X_test: (2200, 100, 15)\n",
    "#y_train: (13006, 1)\n",
    "#y_test: (2200, 1)\n",
    "p =y_test.reshape(2200, 1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.inverse_transform(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
